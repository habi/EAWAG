{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fiddle with the EAWAG scans\n",
    "Look at the orientation and see if we can do some cropping based on landmarks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "import os\n",
    "import glob\n",
    "import pandas\n",
    "import imageio\n",
    "import numpy\n",
    "import scipy\n",
    "import k3d\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib_scalebar.scalebar import ScaleBar\n",
    "import seaborn\n",
    "import dask\n",
    "import dask_image.imread\n",
    "import skimage\n",
    "from tqdm.auto import tqdm, trange\n",
    "import math\n",
    "from numcodecs import Blosc\n",
    "from skimage.segmentation import random_walker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set dask temporary folder\n",
    "# Do this before creating a client: https://stackoverflow.com/a/62804525/323100\n",
    "import tempfile\n",
    "if 'Linux' in platform.system():\n",
    "    # Check if me mounted the FastSSD, otherwise go to standard tmp file\n",
    "    if os.path.exists(os.path.join(os.sep, 'media', 'habi', 'Fast_SSD')):\n",
    "        tmp = os.path.join(os.sep, 'media', 'habi', 'Fast_SSD', 'tmp')\n",
    "    else:\n",
    "        tmp = tempfile.gettempdir()\n",
    "elif 'Darwin' in platform.system():\n",
    "    tmp = tempfile.gettempdir()\n",
    "else:\n",
    "    if 'anaklin' in platform.node():\n",
    "        tmp = os.path.join('F:\\\\tmp')\n",
    "    else:\n",
    "        tmp = os.path.join('D:\\\\tmp')\n",
    "dask.config.set({'temporary_directory': tmp})\n",
    "print('Dask temporary files go to %s' % dask.config.get('temporary_directory'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "client = Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('You can seee what DASK is doing at \"http://localhost:%s/status\"' % client.scheduler_info()['services']['dashboard'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Ignore warnings in the notebook\n",
    "# import warnings\n",
    "# warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seaborn theme\n",
    "seaborn.set_theme(context='notebook', style='ticks')\n",
    "# Set up figure defaults\n",
    "plt.rc('image', cmap='gray', interpolation='nearest')  # Display all images in b&w and with 'nearest' interpolation\n",
    "plt.rcParams['figure.figsize'] = (16, 9)  # Size up figures a bit\n",
    "plt.rcParams['figure.dpi'] = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup scale bar defaults\n",
    "plt.rcParams['scalebar.location'] = 'lower right'\n",
    "plt.rcParams['scalebar.frameon'] = False\n",
    "plt.rcParams['scalebar.color'] = 'white'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display all plots identically\n",
    "lines = 3\n",
    "# And then do something like\n",
    "# plt.subplot(lines, int(numpy.ceil(len(Data) / float(lines))), c + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different locations if running either on Linux or Windows\n",
    "FastSSD = True\n",
    "overthere = False  # Load the data directly from the iee-research_storage drive\n",
    "nanoct = True  # Load the data directly from the 2214\n",
    "# to speed things up significantly\n",
    "if 'Linux' in platform.system():\n",
    "    if FastSSD:\n",
    "        BasePath = os.path.join(os.sep, 'media', 'habi', 'Fast_SSD')\n",
    "    elif overthere:\n",
    "        BasePath = os.path.join(os.sep, 'home', 'habi', 'research-storage-iee')\n",
    "    elif nanoct:\n",
    "        BasePath = os.path.join(os.path.sep, 'home', 'habi', '2214')\n",
    "    else:\n",
    "        BasePath = os.path.join(os.sep, 'home', 'habi', '1272')\n",
    "elif 'Darwin' in platform.system():\n",
    "    FastSSD = False\n",
    "    BasePath = os.path.join('/Users/habi/Dev/EAWAG/Data')\n",
    "elif 'Windows' in platform.system():\n",
    "    if FastSSD:\n",
    "        BasePath = os.path.join('F:\\\\')\n",
    "    else:\n",
    "        if 'anaklin' in platform.node():\n",
    "            BasePath = os.path.join('S:\\\\')\n",
    "        else:\n",
    "            BasePath = os.path.join('D:\\\\Results')\n",
    "if not overthere:\n",
    "    Root = os.path.join(BasePath, 'EAWAG')\n",
    "else:\n",
    "    Root = BasePath\n",
    "# if overthere:\n",
    "#         Root = os.path.join('I:\\\\microCTupload')\n",
    "print('We are loading all the data from %s' % Root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pixelsize(logfile):\n",
    "    \"\"\"Get the pixel size from the scan log file\"\"\"\n",
    "    with open(logfile, 'r') as f:\n",
    "        for line in f:\n",
    "            if 'Image Pixel' in line and 'Scaled' not in line:\n",
    "                pixelsize = float(line.split('=')[1])\n",
    "    return(pixelsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_git_hash():\n",
    "    '''\n",
    "    Get the current git hash from the repository.\n",
    "    Based on http://stackoverflow.com/a/949391/323100 and\n",
    "    http://stackoverflow.com/a/18283905/323100\n",
    "    '''\n",
    "    from subprocess import Popen, PIPE\n",
    "    import os\n",
    "    gitprocess = Popen(['git',\n",
    "                        '--git-dir',\n",
    "                        os.path.join(os.getcwd(), '.git'),\n",
    "                        'rev-parse',\n",
    "                        '--short',\n",
    "                        '--verify',\n",
    "                        'HEAD'],\n",
    "                       stdout=PIPE)\n",
    "    (output, _) = gitprocess.communicate()\n",
    "    return output.strip().decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Make directory for output\n",
    "# OutPutDir = os.path.join(os.getcwd(), 'Output', get_git_hash())\n",
    "# print('We are saving all the output to %s' % OutPutDir)\n",
    "# os.makedirs(OutPutDir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make us a dataframe for saving all that we need\n",
    "Data = pandas.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get *all* log files, unsorted but fast\n",
    "Data['LogFile'] = [os.path.join(root, name)\n",
    "                   for root, dirs, files in os.walk(Root)\n",
    "                   for name in files\n",
    "                   if name.endswith((\".log\"))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('We found %s log files in %s' % (len(Data), Root))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limit *all* the data to only the 'head' scans\n",
    "Data = Data[Data['LogFile'].str.contains('head')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('We have %s log files with \"head\" in their name in %s' % ((len(Data)), Root))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate folder name\n",
    "Data['Folder'] = [os.path.dirname(f) for f in Data['LogFile']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get rid of all non-rec logfiles\n",
    "for c, row in Data.iterrows():\n",
    "    if 'rec' not in row.Folder:\n",
    "        Data.drop([c], inplace=True)\n",
    "    elif 'SubScan' in row.Folder:\n",
    "        Data.drop([c], inplace=True)\n",
    "    elif 'rectmp.log' in row.LogFile:\n",
    "        Data.drop([c], inplace=True)\n",
    "# Reset dataframe to something that we would get if we only would have loaded the 'rec' files\n",
    "Data = Data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate us some meaningful colums\n",
    "Data['Fish'] = [l[len(Root) + 1:].split(os.sep)[0] for l in Data['LogFile']]\n",
    "Data['Scan'] = ['_'.join(l[len(Root) + 1:].split(os.sep)[1:-1]) for l in Data['LogFile']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the file names of the reconstructions\n",
    "Data['Reconstructions'] = [[os.path.join(root, name)\n",
    "                            for root, dirs, files in os.walk(f)\n",
    "                            for name in files\n",
    "                            if 'rec0' in name and name.endswith((\".png\"))] for f in Data['Folder']]\n",
    "# Count how many files we have\n",
    "Data['Number of reconstructions'] = [len(r) for r in Data.Reconstructions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop samples which have either not been reconstructed yet or of which we deleted the reconstructions with\n",
    "# `find . -name \"*rec*.png\" -type f -mtime +333 -delete`\n",
    "# Based on https://stackoverflow.com/a/13851602\n",
    "# for c, row in Data.iterrows():\n",
    "#     if not row['Number of reconstructions']:\n",
    "#         print('%s contains no PNG files, we might be currently reconstructing it' % row.Folder)\n",
    "Data = Data[Data['Number of reconstructions'] > 0]\n",
    "Data.reset_index(drop=True, inplace=True)\n",
    "print('We have %s folders with reconstructions in %s' % ((len(Data)), Root))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get parameters we need from the log files\n",
    "Data['Voxelsize'] = [get_pixelsize(log) for log in Data['LogFile']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load all reconstructions into DASK arrays\n",
    "Reconstructions = [None] * len(Data)\n",
    "for c, row in tqdm(Data.iterrows(),\n",
    "                   desc='Load reconstructions',\n",
    "                   total=len(Data)):\n",
    "    Reconstructions[c] = dask_image.imread.imread(os.path.join(row['Folder'],\n",
    "                                                               '*rec*.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check if something went wrong\n",
    "# for file in Data['OutputNameRec']:\n",
    "#     print(file)\n",
    "#     dask.array.from_zarr(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How big are the datasets?\n",
    "Data['Size'] = [rec.shape for rec in Reconstructions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The three cardinal directions\n",
    "# Names adapted to fishes: https://en.wikipedia.org/wiki/Fish_anatomy#Body\n",
    "directions = ['Anteroposterior',\n",
    "              'Lateral',\n",
    "              'Dorsoventral']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in previously generated MIPs or calculate them\n",
    "for d, direction in enumerate(directions):\n",
    "    Data['MIP_' + direction] = ''\n",
    "for c, row in tqdm(Data.iterrows(), desc='Calculating MIPs', total=len(Data)):\n",
    "    for d, direction in tqdm(enumerate(directions),\n",
    "                             desc='%s/%s: %s' % (row['Fish'], row['Scan'], direction),\n",
    "                             leave=False,\n",
    "                             total=len(directions)):\n",
    "        outfilepath = os.path.join(os.path.dirname(row['Folder']),\n",
    "                                   '%s.%s.MIP.%s.png' % (row['Fish'], row['Scan'], direction))\n",
    "        if os.path.exists(outfilepath):\n",
    "            Data.at[c, 'MIP_' + direction] = dask_image.imread.imread(outfilepath).squeeze()\n",
    "        else:\n",
    "            # Generate MIP\n",
    "            Data.at[c, 'MIP_' + direction] = Reconstructions[c].max(axis=d).compute().squeeze()\n",
    "            # Save it out\n",
    "            imageio.imwrite(outfilepath, Data.at[c, 'MIP_' + direction].astype('uint8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect views\n",
    "for c, row in tqdm(Data.iterrows(),\n",
    "                   desc='Saving overview of MIP images',\n",
    "                   total=len(Data)):\n",
    "    outfilepath = os.path.join(os.path.dirname(row['Folder']),\n",
    "                               '%s.%s.MIPs.png' % (row['Fish'], row['Scan']))\n",
    "    if not os.path.exists(outfilepath):    \n",
    "        print('%s/%s: %s' % (c, len(Data), os.path.join(row.Fish, row.Scan)))\n",
    "        for d, direction in tqdm(enumerate(directions),\n",
    "                                 desc='%s/%s' % (row['Fish'], row['Scan']),\n",
    "                                 leave=False,\n",
    "                                 total=len(directions)):\n",
    "            plt.subplot(1, 3, d + 1)\n",
    "            plt.imshow(row['MIP_' + direction])\n",
    "            plt.gca().add_artist(ScaleBar(row['Voxelsize'],\n",
    "                                          'um'))\n",
    "            plt.title('%s MIP' % direction)\n",
    "            plt.axis('off')\n",
    "            plt.title('%s\\n%s MIP' % (os.path.join(row['Fish'], row['Scan']), direction))\n",
    "            plt.savefig(outfilepath,\n",
    "                        transparent=True,\n",
    "                        bbox_inches='tight')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The functions below were copied from Hearts-Melly/SubMyocardAnalysis.ipynb, in which we also look at the orientation of things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def threshold(image, verbose=False):\n",
    "    # Calculate threshold of image where image is non-zero\n",
    "    threshold = skimage.filters.threshold_otsu(image[image > 0])\n",
    "    if verbose:\n",
    "        plt.subplot(121)\n",
    "        plt.imshow(image)\n",
    "        plt.imshow(dask.array.ma.masked_equal(image > threshold, 0),\n",
    "                   alpha=0.618,\n",
    "                   cmap='viridis_r')\n",
    "        plt.subplot(122)\n",
    "        plt.semilogy(histogram(image), label='Log-Histogram')\n",
    "        plt.axvline(threshold, label='Otsu threshold: %s' % threshold)\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    return(threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the histogram of an image\n",
    "# We can safely assume to only use 8bit images\n",
    "def histogram(img):\n",
    "    histogram, bins = dask.array.histogram(dask.array.array(img),\n",
    "                                           bins=2**8,\n",
    "                                           range=[0, 2**8])\n",
    "    return(histogram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure out the otolith position on each of the directional views."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smoother(curve, frac=0.1):\n",
    "    ''' Smooth a curve '''\n",
    "    from statsmodels.nonparametric.smoothers_lowess import lowess\n",
    "    smoothed = lowess(curve, range(len(curve)), return_sorted=False, frac=frac)\n",
    "    return(smoothed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_minimum(curve, verbose=False):\n",
    "    '''\n",
    "    Function to detect get a 'border' based on the gray value along the image.\n",
    "    We do this by detecting the minimum of the derivative of the smoothed grayvalue curve\n",
    "    Based on https://stackoverflow.com/a/28541805/323100 and some manual tweaking\n",
    "    '''\n",
    "    smoothed = smoother(curve)\n",
    "    minimal_diff = numpy.argmin(numpy.diff(smoothed))\n",
    "    if verbose:\n",
    "        plt.plot(curve, alpha=0.6, label='Input curve')\n",
    "        plt.plot(smoothed, label='LOWESS smoothed')\n",
    "        plt.axvline(minimal_diff, c='r', label='Border')\n",
    "        plt.legend()\n",
    "    return(minimal_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_maximum(curve, verbose=False):\n",
    "    '''\n",
    "    Function to detect get a 'border' based on the gray value along the image.\n",
    "    We do this by detecting the minimum of the derivative of the smoothed grayvalue curve\n",
    "    Based on https://stackoverflow.com/a/28541805/323100 and some manual tweaking\n",
    "    '''\n",
    "    smoothed = smoother(curve)\n",
    "    maximal_diff = numpy.argmax(numpy.diff(smoothed))\n",
    "    if verbose:\n",
    "        plt.plot(curve, alpha=0.6, label='Input curve')\n",
    "        plt.plot(smoothed, label='LOWESS smoothed')\n",
    "        plt.axvline(maximal_diff, c='r', label='Border')\n",
    "        plt.legend()\n",
    "    return(maximal_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_peak(curve, start=None, stop=None, frac=0.25, height=0.25, verbose=False):\n",
    "    ''' Find a peak in the smoothed curve '''\n",
    "    # Peak finding from https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.find_peaks.html\n",
    "    # Mask a bit at the start and a bit start and end of curve, if desired\n",
    "    mask = dask.array.zeros_like(curve)\n",
    "    if start:\n",
    "        mask[:start] = 1\n",
    "    if stop:\n",
    "        mask[stop:] = 1\n",
    "    if start or stop:\n",
    "        original_curve = curve\n",
    "        smoothed = smoother(dask.array.ma.filled(dask.array.ma.masked_where(mask, curve), fill_value=0),\n",
    "                            frac=frac)\n",
    "    else:\n",
    "        smoothed = smoother(curve, frac=frac)\n",
    "    if verbose:\n",
    "        print('The input curve has a length of %s' % len(curve))\n",
    "        if start:\n",
    "            print('We discard the %s values from the start' % start)\n",
    "        if stop:\n",
    "            print('We discard the values from %s to the end' % stop)            \n",
    "        print('The input to the smoother has a length of %s' % (len(mask) - dask.array.count_nonzero(mask)).compute())\n",
    "    peak, _ = scipy.signal.find_peaks(smoothed, width=100)    \n",
    "    peak_value = dask.array.asarray(smoothed[peak])\n",
    "    # Peak width from https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.peak_widths.html\n",
    "    results_width = scipy.signal.peak_widths(smoothed, peak, rel_height=height)\n",
    "    if len(peak) > 1:\n",
    "        # Return only 'higher' peak if we have several\n",
    "        peak = dask.array.asarray(peak[dask.array.argmax(peak_value)])\n",
    "        results_width = [item[dask.array.argmax(peak_value)] for item in results_width]\n",
    "        peak_value = int(peak_value[dask.array.argmax(peak_value)])\n",
    "    # Get actual width\n",
    "    width = results_width[0]\n",
    "    # if start:\n",
    "    #     peak = peak + start\n",
    "    #     results_width = [v[0]+start for v in results_width]\n",
    "    if verbose:\n",
    "        if start or stop:\n",
    "            plt.plot(original_curve, alpha=0.618, label='Original')\n",
    "        plt.plot(dask.array.ma.masked_where(mask, curve), label='Input')\n",
    "        plt.plot(smoothed, label='Smoothed (frac=%s)' % frac)\n",
    "        plt.plot(peak,\n",
    "                 smoothed[peak],\n",
    "                 'x',\n",
    "                 color='C2',\n",
    "                 label='Peak@%s' % int(peak))\n",
    "        plt.hlines(*results_width[1:],\n",
    "                   color=\"C3\",\n",
    "                   label='Peak width at %d%%: %s' % (100*height, int(width)))\n",
    "        plt.legend()\n",
    "        # plt.xlim([0,len(curve)])\n",
    "        plt.show()\n",
    "\n",
    "    return(int(peak), int(peak_value), int(width))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put gray values of one fish in lists, so we can fiddle with them\n",
    "# gv =  [None] * len(directions) \n",
    "# gh =  [None] * len(directions) \n",
    "# for c, direction in enumerate(directions):\n",
    "#     gv[c] = Reconstructions[whichone].max(axis=c).sum(axis=0)\n",
    "#     gh[c] = Reconstructions[whichone].max(axis=c).sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One peak for testing\n",
    "# get_peak(gv[1], start=100, stop=1111, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rescale_linear(array, new_min, new_max):\n",
    "    \"\"\"Rescale an arrary linearly. From https://stackoverflow.com/a/50011743/323100\"\"\"\n",
    "    minimum, maximum = numpy.min(array), numpy.max(array)\n",
    "    m = (new_max - new_min) / (maximum - minimum)\n",
    "    b = new_min - m * minimum\n",
    "    return m * array + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_grayvalues(image, plane, which):\n",
    "    ''' get grayvalue along horizontal or vertical image plane '''\n",
    "    grayvalues = []\n",
    "    if plane == 'horizontal':\n",
    "        ax = 0\n",
    "    elif plane == 'vertical':\n",
    "        ax = 1\n",
    "    else:\n",
    "        print('No plane given, specify either \"plane=horizontal\" or \"plane=vertical\"')\n",
    "        print('Returning EMPTY grayvalues')        \n",
    "        return(grayvalues)\n",
    "    if which == 'sum':\n",
    "        grayvalues = image.sum(axis=ax)\n",
    "    elif which == 'max':\n",
    "        grayvalues = image.max(axis=ax)\n",
    "    else:\n",
    "        print('No method given, specify either \"which=max\" or \"which=sum\"')\n",
    "        print('Returning EMPTY grayvalues')\n",
    "        return(grayvalues)\n",
    "    return(grayvalues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dir in directions:\n",
    "    print(dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "planes = ['horizontal', 'vertical']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate empty columns to fill in the values we calculate below\n",
    "Data['Otolith_Sphere_Diameter_fish'] = '' # Merged peak position\n",
    "for c, direction in enumerate(directions):\n",
    "    Data['Discard_' + direction + '_fish'] = '' # General discard directions\n",
    "    Data['Otolith_Peak_' + direction + '_fish'] = '' # Merged peak position\n",
    "    Data['Otolith_Width_' + direction + '_fish'] = '' # Merged peak position\n",
    "    for plane in planes:\n",
    "        Data['Discard_' + direction + '_image_' + plane] = '' # Copy the general discard values to the ones we need for each image\n",
    "        Data['Otolith_Peak_'  + direction + '_' + plane] = ''\n",
    "        Data['Otolith_Width_' + direction + '_' + plane] = ''\n",
    "        Data['Grayvalues_'    + direction + '_' + plane] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def otolither_region(whichfish, discard_front=None, bottom=1000, showregion=True, verbose=False):\n",
    "    '''\n",
    "    Modeled after algorithm for finding the enamel/dentin border for the tooth project (https://github.com/habi/zmk-tooth-cohort/blob/master/ToothAnalysis.ipynb), we\n",
    "    look for a change and bump in the gray values along different axis of the fishes.\n",
    "    This works out nicely to detect the approximate region of the otoliths.\n",
    "    '''\n",
    "    if verbose:\n",
    "        print('We try to find the otholith for the %s scan of fish %s' % (Data.Scan[whichfish], Data.Fish[whichfish]))\n",
    "    \n",
    "    # Discard some regions of the images for finding the otoliths\n",
    "    # frontally: contains the teeth\n",
    "    # back: can contain labels or dorsal fins\n",
    "    Data.at[whichfish, 'Discard_Anteroposterior_fish'] = [round(Data.MIP_Lateral[whichfish].shape[0] / 5),\n",
    "                                                          round(Data.MIP_Lateral[whichfish].shape[0] - Data.MIP_Lateral[whichfish].shape[0] / 5)]\n",
    "    # laterally otoliths are in the middle of the fish    \n",
    "    Data.at[whichfish, 'Discard_Lateral_fish'] = [round(Data.MIP_Dorsoventral[whichfish].shape[1] / 8),\n",
    "                                                 round(Data.MIP_Dorsoventral[whichfish].shape[1] - Data.MIP_Dorsoventral[whichfish].shape[1] / 8)]\n",
    "    # bottom: a lot of the fish, no otolith\n",
    "    # top: often empty\n",
    "    Data.at[whichfish, 'Discard_Dorsoventral_fish'] = [round(Data.MIP_Anteroposterior[whichfish].shape[1] / 3),\n",
    "                                                      round(Data.MIP_Anteroposterior[whichfish].shape[1] - Data.MIP_Anteroposterior[whichfish].shape[1] / 10)]\n",
    "    \n",
    "    # Copy the discard values to different planar directions for every image.\n",
    "    # This is *very* hacky, but makes it all run in one single loop\n",
    "    Data.at[whichfish, 'Discard_Anteroposterior_image_horizontal'] = Data['Discard_Dorsoventral_fish'][whichfish]\n",
    "    Data.at[whichfish, 'Discard_Anteroposterior_image_vertical']   = Data['Discard_Lateral_fish'][whichfish]\n",
    "    Data.at[whichfish, 'Discard_Lateral_image_horizontal']         = Data['Discard_Dorsoventral_fish'][whichfish]\n",
    "    Data.at[whichfish, 'Discard_Lateral_image_vertical']           = Data['Discard_Anteroposterior_fish'][whichfish]\n",
    "    Data.at[whichfish, 'Discard_Dorsoventral_image_horizontal']    = Data['Discard_Lateral_fish'][whichfish]\n",
    "    Data.at[whichfish, 'Discard_Dorsoventral_image_vertical']      = Data['Discard_Anteroposterior_fish'][whichfish]    \n",
    "\n",
    "    if verbose:\n",
    "        print('We discard the ventral %s and the dorsal %s slices of the fish' % (Data['Discard_Dorsoventral_fish'][whichfish][0], Data['Discard_Dorsoventral_fish'][whichfish][1]))\n",
    "        print('We discard the anterior %s and the posterior %s slices of the fish' % (Data['Discard_Anteroposterior_fish'][whichfish][0], Data['Discard_Anteroposterior_fish'][whichfish][1]))\n",
    "        print('We discard the fish laterally between slices %s and %s' % (Data['Discard_Lateral_fish'][whichfish][0], Data['Discard_Lateral_fish'][whichfish][1]))\n",
    "\n",
    "    for direction in directions:\n",
    "        for plane in planes:\n",
    "            if verbose:\n",
    "                print('Calculating Grayvalues_' + direction + '_' + plane)\n",
    "            if ('Dorso' not in direction) and ('hor' in plane):\n",
    "                method = 'max'\n",
    "            else:\n",
    "                method = 'sum'\n",
    "            Data.at[whichfish, 'Grayvalues_' + direction + '_' + plane] = get_grayvalues(Data['MIP_' + direction][whichfish],\n",
    "                                                                                        plane=plane,\n",
    "                                                                                        which=method)            \n",
    "                                                                                        # which='sum')            \n",
    "            # Our peak finder function returns peak position, peak value and peak width\n",
    "            # We only need position and width and don't save the value (for now)\n",
    "            if verbose:\n",
    "                print(80*'-')\n",
    "                print(direction, plane)\n",
    "                print('GV length', len(Data['Grayvalues_' + direction + '_' + plane][whichfish]))\n",
    "                print('For %s/%s we want to discard %s' % (direction, plane, Data.at[whichfish, 'Discard_' + direction + '_image_' + plane]))\n",
    "                print('MIP shape', Data['MIP_' + direction][whichfish].shape)\n",
    "\n",
    "                print(80*'-')\n",
    "            height = 0.25\n",
    "            frac = 0.1\n",
    "            if 'vert' in plane:\n",
    "                height=0.333\n",
    "            # elif ('Ant' in direction) and ('hor' in plane):\n",
    "            #     frac=0.5\n",
    "            #     height=0.25                \n",
    "            # elif ('Lat' in direction) and ('vert' in plane):\n",
    "            #     height=0.2             \n",
    "            elif ('Dors' not in direction) and ('hor' in plane):\n",
    "                frac=0.5\n",
    "                height=0.333\n",
    "            # elif ('Ante' not in direction) and ('vert' in plane):\n",
    "            #     height=0.2\n",
    "            #     frac = 0.1                \n",
    "            elif ('Dors' in direction) and ('hor' in plane):\n",
    "                height=0.5                \n",
    "            # else:\n",
    "            #     pass\n",
    "            peak, _, width = get_peak(Data['Grayvalues_' + direction + '_' + plane][whichfish],\n",
    "                                      start=Data['Discard_'+ direction + '_image_' + plane][whichfish][0],\n",
    "                                      stop=Data['Discard_'+ direction + '_image_' + plane][whichfish][1],\n",
    "                                      frac=frac,\n",
    "                                      height=height,\n",
    "                                      verbose=verbose)\n",
    "            Data.at[whichfish, 'Otolith_Peak_' + direction + '_' + plane] = peak\n",
    "            Data.at[whichfish, 'Otolith_Width_' + direction + '_' + plane] = width\n",
    "            \n",
    "    # Calculate means found values, to be applied as cutout region\n",
    "    Data.at[whichfish,'Otolith_Peak_Anteroposterior_fish'] = int(round(numpy.mean((Data['Otolith_Peak_Dorsoventral_vertical'][whichfish],\n",
    "                                                                                  Data['Otolith_Peak_Lateral_vertical'][whichfish]))))\n",
    "    Data.at[whichfish,'Otolith_Width_Anteroposterior_fish'] = int(round(numpy.mean((Data['Otolith_Width_Dorsoventral_vertical'][whichfish],\n",
    "                                                                                  Data['Otolith_Width_Lateral_vertical'][whichfish]))))\n",
    "    Data.at[whichfish,'Otolith_Peak_Lateral_fish']         = int(round(numpy.mean((Data['Otolith_Peak_Dorsoventral_horizontal'][whichfish],\n",
    "                                                                                  Data['Otolith_Peak_Anteroposterior_vertical'][whichfish]))))\n",
    "    Data.at[whichfish,'Otolith_Width_Lateral_fish']         = int(round(numpy.mean((Data['Otolith_Width_Dorsoventral_horizontal'][whichfish],\n",
    "                                                                                  Data['Otolith_Width_Anteroposterior_vertical'][whichfish]))))\n",
    "    Data.at[whichfish,'Otolith_Peak_Dorsoventral_fish']    = int(round(numpy.mean((Data['Otolith_Peak_Anteroposterior_horizontal'][whichfish],\n",
    "                                                                                  Data['Otolith_Peak_Lateral_horizontal'][whichfish]))))            \n",
    "    Data.at[whichfish,'Otolith_Width_Dorsoventral_fish']    = int(round(numpy.mean((Data['Otolith_Width_Anteroposterior_horizontal'][whichfish],\n",
    "                                                                                  Data['Otolith_Width_Lateral_horizontal'][whichfish]))))                \n",
    "    Data.at[whichfish,'Otolith_Sphere_Diameter_fish'] = int(round(numpy.mean((Data['Otolith_Width_Anteroposterior_fish'][whichfish],\n",
    "                                                                             Data['Otolith_Width_Lateral_fish'][whichfish],\n",
    "                                                                             Data['Otolith_Width_Dorsoventral_fish'][whichfish]))))\n",
    "    # Safety-check of values\n",
    "    # peak-width/2 should not be negative or peak+width/2 should not be larger than image width\n",
    "    # \"I'm looking at you, 11729/head!\"\n",
    "    for direction in directions:\n",
    "        if Data['Otolith_Peak_' + direction + '_fish'][whichfish] - round(Data['Otolith_Width_' +  direction + '_fish'][whichfish] / 2) < 0:\n",
    "            print('%s peak of %s shifted by %s to make sure that we are not out of the fish' % (direction,\n",
    "                                                                                                os.path.join(Data['Fish'][whichfish], Data['Scan'][whichfish]),\n",
    "                                                                                                Data['Otolith_Peak_' + direction + '_fish'][whichfish] - round(Data['Otolith_Width_' +  direction + '_fish'][whichfish] / 2)))            \n",
    "            Data.at[whichfish, 'Otolith_Peak_' + direction + '_fish'] -= Data['Otolith_Peak_' + direction + '_fish'][whichfish] - round(Data['Otolith_Width_' +  direction + '_fish'][whichfish] / 2)\n",
    "    return()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate otolith region for one fish\n",
    "otolither_region(whichone, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_otolith_position(whichfish):\n",
    "    # Display everything\n",
    "    # Based on https://matplotlib.org/tutorials/intermediate/gridspec.html\n",
    "    fig = plt.figure(constrained_layout=True)\n",
    "    gs = fig.add_gridspec(2, 3)\n",
    "    for c, direction in enumerate(directions):\n",
    "        mip = fig.add_subplot(gs[0, c])\n",
    "        # Show image\n",
    "        plt.imshow(Data['MIP_' + direction][whichfish])\n",
    "    \n",
    "        # # Show discarded regions\n",
    "        # if not 'Dorsoventral' in direction:\n",
    "        #     plt.fill_betweenx(range(Data['MIP_' + direction][whichfish].shape[0]),\n",
    "        #                       0, Data['Discard_Dorsoventral_fish'][whichfish][0],\n",
    "        #                       alpha=0.309, label='discarded up to %s' % Data['Discard_Dorsoventral_fish'][whichfish][0],\n",
    "        #                       color=seaborn.color_palette()[2])\n",
    "        #     plt.fill_betweenx(range(Data['MIP_' + direction][whichfish].shape[0]),\n",
    "        #                       Data['Discard_Dorsoventral_fish'][whichfish][1], Data['MIP_' + direction][whichfish].shape[1] - 1,\n",
    "        #                       alpha=0.309, label='discarded from %s' % Data['Discard_Dorsoventral_fish'][whichfish][1],\n",
    "        #                       color=seaborn.color_palette()[2])            \n",
    "        # else:\n",
    "        #     plt.fill_betweenx(range(Data['MIP_' + direction][whichfish].shape[0]),\n",
    "        #                       0, Data['Discard_Lateral_fish'][whichfish][0],\n",
    "        #                       alpha=0.309, label='discarded up to %s' % Data['Discard_Lateral_fish'][whichfish][0],\n",
    "        #                       color=seaborn.color_palette()[1])\n",
    "        #     plt.fill_betweenx(range(Data['MIP_' + direction][whichfish].shape[0]),\n",
    "        #                       Data['Discard_Lateral_fish'][whichfish][1], Data['MIP_' + direction][whichfish].shape[1] - 1,\n",
    "        #                       alpha=0.309, label='discarded from %s' % Data['Discard_Lateral_fish'][whichfish][1],\n",
    "        #                       color=seaborn.color_palette()[1])\n",
    "        # if 'Ante' not in direction:\n",
    "        #     plt.fill_between(range(Data['MIP_' + direction][whichfish].shape[1]),\n",
    "        #                      0,\n",
    "        #                      Data['Discard_Anteroposterior_fish'][whichfish][0],\n",
    "        #                      alpha=0.309, label='discarded from %s' % Data['Discard_Anteroposterior_fish'][whichfish][0],\n",
    "        #                      color=seaborn.color_palette()[0])\n",
    "        #     plt.fill_between(range(Data['MIP_' + direction][whichfish].shape[1]),\n",
    "        #                      Data['Discard_Anteroposterior_fish'][whichfish][1],\n",
    "        #                      Data['MIP_' + direction][whichfish].shape[0] - 1,\n",
    "        #                      alpha=0.309, label='discarded from %s' % Data['Discard_Anteroposterior_fish'][whichfish][1],\n",
    "        #                      color=seaborn.color_palette()[0])\n",
    "        # else:\n",
    "        #     plt.fill_between(range(Data['MIP_' + direction][whichfish].shape[1]),\n",
    "        #                      0,\n",
    "        #                      Data['Discard_Lateral_fish'][whichfish][0],\n",
    "        #                      alpha=0.309, label='discarded from %s' % Data['Discard_Lateral_fish'][whichfish][0],\n",
    "        #                      color=seaborn.color_palette()[1])\n",
    "        #     plt.fill_between(range(Data['MIP_' + direction][whichfish].shape[1]),\n",
    "        #                      Data['Discard_Lateral_fish'][whichfish][1],\n",
    "        #                      Data['MIP_' + direction][whichfish].shape[0] - 1,\n",
    "        #                      alpha=0.309, label='discarded from %s' % Data['Discard_Lateral_fish'][whichfish][1],\n",
    "        #                      color=seaborn.color_palette()[1])\n",
    "        \n",
    "        # *Very* verbose way of drawing the region we look at\n",
    "        # But since we have all the values, we can easily put them where we want\n",
    "        # Rectangle(xy, width, height)\n",
    "        plt.gca().add_patch(matplotlib.patches.Rectangle((Data['Discard_' + direction + '_image_horizontal'][whichfish][0],\n",
    "                                                          Data['Discard_' + direction + '_image_vertical'][whichfish][0]),\n",
    "                                                         Data['MIP_' + direction][whichfish].shape[1] - (Data['MIP_' + direction][whichfish].shape[1] - Data['Discard_' + direction + '_image_horizontal'][whichfish][1]) - Data['Discard_' + direction + '_image_horizontal'][whichfish][0],\n",
    "                                                         Data['MIP_' + direction][whichfish].shape[0] - (Data['MIP_' + direction][whichfish].shape[0] - Data['Discard_' + direction + '_image_vertical'][whichfish][1]) - Data['Discard_' + direction + '_image_vertical'][whichfish][0],\n",
    "                                                         edgecolor=seaborn.color_palette()[c],\n",
    "                                                         facecolor='none',\n",
    "                                                         label='Region for detection'))\n",
    "        \n",
    "        # Plot gray values onto the image\n",
    "        # plt.plot(rescale_linear(Data['Grayvalues_' + direction + '_horizontal'][whichfish], 0, Data['MIP_' + direction][whichfish].shape[0] - 1),\n",
    "        #          label='horizontal', color='gray')\n",
    "        # Plot *only* the values we're interested in, i.e. discard start and end\n",
    "        # plt.plot(range(Data['Discard_'+ direction + '_image_horizontal'][whichfish][0],len(Data['Grayvalues_' + direction + '_horizontal'][whichfish][:Data['Discard_'+ direction + '_image_horizontal'][whichfish][1]])),\n",
    "        #          rescale_linear(Data['Grayvalues_' + direction + '_horizontal'][whichfish][Data['Discard_'+ direction + '_image_horizontal'][whichfish][0]:Data['Discard_'+ direction + '_image_horizontal'][whichfish][1]],\n",
    "        #                         Data['Discard_' + direction + '_image_vertical'][whichfish][0],\n",
    "        #                         Data['Discard_' + direction + '_image_vertical'][whichfish][1]),\n",
    "        #          # label='horizontal',\n",
    "        #          color='lightgray', alpha=0.618)        \n",
    "        plt.plot(range(Data['Discard_'+ direction + '_image_horizontal'][whichfish][0],len(Data['Grayvalues_' + direction + '_horizontal'][whichfish][:Data['Discard_'+ direction + '_image_horizontal'][whichfish][1]])),\n",
    "                 rescale_linear(smoother(Data['Grayvalues_' + direction + '_horizontal'][whichfish][Data['Discard_'+ direction + '_image_horizontal'][whichfish][0]:Data['Discard_'+ direction + '_image_horizontal'][whichfish][1]]),\n",
    "                                Data['Discard_' + direction + '_image_vertical'][whichfish][0],\n",
    "                                Data['Discard_' + direction + '_image_vertical'][whichfish][1]),\n",
    "                 # label='horizontal',\n",
    "                 color='white', alpha=0.618)        \n",
    "        # plt.plot(rescale_linear(Data['Grayvalues_' + direction + '_vertical'][whichfish][Data['Discard_'+ direction + '_image_vertical'][whichfish][0]:Data['Discard_'+ direction + '_image_vertical'][whichfish][1]],\n",
    "        #                         Data['Discard_' + direction + '_image_horizontal'][whichfish][0],\n",
    "        #                         Data['Discard_' + direction + '_image_horizontal'][whichfish][1]),\n",
    "        #          range(Data['Discard_'+ direction + '_image_vertical'][whichfish][0],len(Data['Grayvalues_' + direction + '_vertical'][whichfish][:Data['Discard_'+ direction + '_image_vertical'][whichfish][1]])),\n",
    "        #          # label='vertical',\n",
    "        #          color='lightgray', alpha=0.618)        \n",
    "        plt.plot(rescale_linear(smoother(Data['Grayvalues_' + direction + '_vertical'][whichfish][Data['Discard_'+ direction + '_image_vertical'][whichfish][0]:Data['Discard_'+ direction + '_image_vertical'][whichfish][1]]),\n",
    "                                Data['Discard_' + direction + '_image_horizontal'][whichfish][0],\n",
    "                                Data['Discard_' + direction + '_image_horizontal'][whichfish][1]),\n",
    "                 range(Data['Discard_'+ direction + '_image_vertical'][whichfish][0],len(Data['Grayvalues_' + direction + '_vertical'][whichfish][:Data['Discard_'+ direction + '_image_vertical'][whichfish][1]])),\n",
    "                 # label='vertical',\n",
    "                 color='white', alpha=0.618)        \n",
    "        \n",
    "        # Show peaks from this direction\n",
    "        plt.axhline(Data['Otolith_Peak_' + direction + '_vertical'][whichfish],\n",
    "                    label='vertical %s @ %s' % (direction, Data['Otolith_Peak_' + direction + '_vertical'][whichfish]),\n",
    "                    color=seaborn.color_palette()[c],\n",
    "                    alpha=0.618)\n",
    "        plt.axvline(Data['Otolith_Peak_' + direction + '_horizontal'][whichfish],\n",
    "                    label='horizontal %s @ %s' % (direction, Data['Otolith_Peak_' + direction + '_horizontal'][whichfish]),\n",
    "                    color=seaborn.color_palette()[c],\n",
    "                    alpha=0.618)\n",
    "\n",
    "        # Show peaks from other directions and chosen peak\n",
    "        if 'Ante' in direction:\n",
    "            plt.axhline(Data['Otolith_Peak_Dorsoventral_horizontal'][whichfish],\n",
    "                        color=seaborn.color_palette()[2],\n",
    "                        alpha=0.618)         \n",
    "            plt.axvline(Data['Otolith_Peak_Lateral_horizontal'][whichfish],\n",
    "                        color=seaborn.color_palette()[1],\n",
    "                        alpha=0.618)\n",
    "            plt.axhline(Data['Otolith_Peak_Lateral_fish'][whichfish],\n",
    "                        label='Mean LT @ %s' % Data['Otolith_Peak_Lateral_fish'][whichfish],\n",
    "                        color='white')            \n",
    "            plt.axvline(Data['Otolith_Peak_Dorsoventral_fish'][whichfish],\n",
    "                        label='Mean DV @ %s' % Data['Otolith_Peak_Dorsoventral_fish'][whichfish],\n",
    "                        color='white')\n",
    "        elif 'Late' in direction:\n",
    "            plt.axhline(Data['Otolith_Peak_Dorsoventral_vertical'][whichfish],\n",
    "                        color=seaborn.color_palette()[2],\n",
    "                        alpha=0.618)\n",
    "            plt.axvline(Data['Otolith_Peak_Anteroposterior_horizontal'][whichfish],\n",
    "                        color=seaborn.color_palette()[0],\n",
    "                        alpha=0.618)\n",
    "            plt.axhline(Data['Otolith_Peak_Anteroposterior_fish'][whichfish],\n",
    "                        label='Mean AP @ %s' % Data['Otolith_Peak_Anteroposterior_fish'][whichfish],\n",
    "                        color='white')            \n",
    "            plt.axvline(Data['Otolith_Peak_Dorsoventral_fish'][whichfish],\n",
    "                        label='Mean DV @ %s' % Data['Otolith_Peak_Dorsoventral_fish'][whichfish],\n",
    "                        color='white')\n",
    "        else:\n",
    "            plt.axhline(Data['Otolith_Peak_Lateral_vertical'][whichfish],\n",
    "                        color=seaborn.color_palette()[1],\n",
    "                        alpha=0.618)\n",
    "            plt.axvline(Data['Otolith_Peak_Anteroposterior_vertical'][whichfish],\n",
    "                        color=seaborn.color_palette()[0],\n",
    "                        alpha=0.618)            \n",
    "            plt.axhline(Data['Otolith_Peak_Anteroposterior_fish'][whichfish],\n",
    "                        label='Mean AP @ %s' % Data['Otolith_Peak_Anteroposterior_fish'][whichfish],\n",
    "                        color='white')\n",
    "            plt.axvline(Data['Otolith_Peak_Lateral_fish'][whichfish],\n",
    "                        label='Mean LT @ %s' % Data['Otolith_Peak_Lateral_fish'][whichfish],\n",
    "                        color='white')            \n",
    "                \n",
    "        \n",
    "        # *Very* verbose way of drawing the otolith region on top\n",
    "        # But since we have all the values, we can easily put them where we want\n",
    "        if 'Ante' in direction:\n",
    "            ol = 'Otolith region (%sx%s)' % (Data['Otolith_Width_Dorsoventral_fish'][whichfish], Data['Otolith_Width_Lateral_fish'][whichfish])\n",
    "        elif 'Later' in direction:\n",
    "            ol = 'Otolith region (%sx%s)' % (Data['Otolith_Width_Dorsoventral_fish'][whichfish], Data['Otolith_Width_Anteroposterior_fish'][whichfish])\n",
    "        else:\n",
    "            ol = 'Otolith region (%sx%s)' % (Data['Otolith_Width_Lateral_fish'][whichfish], Data['Otolith_Width_Anteroposterior_fish'][whichfish])\n",
    "            \n",
    "        plt.gca().add_patch(matplotlib.patches.Rectangle((Data['Otolith_Peak_' + direction + '_horizontal'][whichfish] - round(Data['Otolith_Width_' + direction + '_horizontal'][whichfish] / 2),\n",
    "                                                          Data['Otolith_Peak_' + direction + '_vertical'][whichfish] - round(Data['Otolith_Width_' + direction + '_vertical'][whichfish] / 2)),\n",
    "                                                         Data['Otolith_Width_' + direction + '_horizontal'][whichfish],\n",
    "                                                         Data['Otolith_Width_' + direction + '_vertical'][whichfish],\n",
    "                                                         color=seaborn.color_palette()[c],\n",
    "                                                         alpha=0.618,\n",
    "                                                         label=ol))\n",
    "    \n",
    "        # plt.legend(loc='lower left')\n",
    "        \n",
    "        plt.gca().add_artist(ScaleBar(Data['Voxelsize'][whichfish], 'um', color=seaborn.color_palette()[c]))\n",
    "        plt.title('%s MIP\\nwith a size of %s x %s px' % (direction, Data['MIP_' + direction][whichfish].shape[0], Data['MIP_' + direction][whichfish].shape[1]))\n",
    "        mip = fig.add_subplot(gs[1, c])\n",
    "        OtolithRegion = Reconstructions[whichfish][Data['Otolith_Peak_Anteroposterior_fish'][whichfish] - round(Data['Otolith_Width_Anteroposterior_fish'][whichfish] / 2):\n",
    "                                                  Data['Otolith_Peak_Anteroposterior_fish'][whichfish] + round(Data['Otolith_Width_Anteroposterior_fish'][whichfish] / 2),\n",
    "                                                  Data['Otolith_Peak_Lateral_fish'][whichfish] - round(Data['Otolith_Width_Lateral_fish'][whichfish] / 2):\n",
    "                                                  Data['Otolith_Peak_Lateral_fish'][whichfish] + round(Data['Otolith_Width_Lateral_fish'][whichfish] / 2),\n",
    "                                                  Data['Otolith_Peak_Dorsoventral_fish'][whichfish] - round(Data['Otolith_Width_Dorsoventral_fish'][whichfish] / 2):\n",
    "                                                  Data['Otolith_Peak_Dorsoventral_fish'][whichfish] + round(Data['Otolith_Width_Dorsoventral_fish'][whichfish] / 2)\n",
    "                                                 ].compute()\n",
    "        plt.imshow(OtolithRegion.max(axis=c))\n",
    "        plt.gca().add_artist(ScaleBar(Data['Voxelsize'][whichfish], 'um', color=seaborn.color_palette()[c]))\n",
    "        plt.title('Extracted %s region %s' % (direction, OtolithRegion.shape))\n",
    "    outfilepath = os.path.join(os.path.dirname(Data['Folder'][whichfish]),\n",
    "                               '%s.%s.Otolither.Region.png' % (Data['Fish'][whichfish], Data['Scan'][whichfish]))\n",
    "    print(outfilepath)\n",
    "    plt.savefig(outfilepath,\n",
    "                transparent=True,\n",
    "                bbox_inches='tight')\n",
    "    plt.show()\n",
    "    return()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and display otolith for one fish\n",
    "display_otolith_position(int(whichone))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and display otolith for all fishes\n",
    "for fishnumber, row in tqdm(Data.iterrows(),\n",
    "                           desc='Extracting otolith regions',\n",
    "                           total=len(Data)):\n",
    "    otolither_region(fishnumber, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# items = 33\n",
    "# for fishnumber, row in tqdm(Data.iterrows(),\n",
    "#                           desc='Displaying otolith regions',\n",
    "#                           total=len(Data)):\n",
    "#     if fishnumber > items and fishnumber < items + 5:\n",
    "#         print(80*'-')\n",
    "#         print(fishnumber, row.Fish, row.Scan)\n",
    "#         display_otolith_position(fishnumber)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save out all otolith regions as .zarr files\n",
    "# Construct output file name\n",
    "Data['OutputNameOtolithRegion'] = [os.path.join(os.path.dirname(f), 'otolith.region.zarr') for f in Data['Folder']]\n",
    "# Replace otolith 'center' into name\n",
    "Data['OutputNameOtolithRegion'] = [n.replace('.region', '.at.%04d.%04d.%04d' % (ap, lt, dv)) for (n, ap, lt, dv) in zip(Data['OutputNameOtolithRegion'],\n",
    "                                                                                                                        Data['Otolith_Peak_Anteroposterior_fish'],\n",
    "                                                                                                                        Data['Otolith_Peak_Lateral_fish'],\n",
    "                                                                                                                        Data['Otolith_Peak_Dorsoventral_fish'])]\n",
    "# Replace otlith width into name\n",
    "Data['OutputNameOtolithRegion'] = [n.replace('.zarr', '.size.%04d.%04d.%04d.zarr' % (ap, lt, dv)) for (n, ap, lt, dv) in zip(Data['OutputNameOtolithRegion'],\n",
    "                                                                                                                             Data['Otolith_Width_Anteroposterior_fish'],\n",
    "                                                                                                                             Data['Otolith_Width_Lateral_fish'],\n",
    "                                                                                                                             Data['Otolith_Width_Dorsoventral_fish'])]\n",
    "# Actually save the regions out now\n",
    "for fishnumber, row in tqdm(Data.iterrows(),\n",
    "                          desc='Extracting otolith regions to .zarr files',\n",
    "                          total=len(Data)):\n",
    "    if not os.path.exists(row['OutputNameOtolithRegion']):\n",
    "        Reconstructions[fishnumber][Data['Otolith_Peak_Anteroposterior_fish'][fishnumber] - round(Data['Otolith_Width_Anteroposterior_fish'][fishnumber] / 2):\n",
    "                                  Data['Otolith_Peak_Anteroposterior_fish'][fishnumber] + round(Data['Otolith_Width_Anteroposterior_fish'][fishnumber] / 2),\n",
    "                                  Data['Otolith_Peak_Lateral_fish'][fishnumber] - round(Data['Otolith_Width_Lateral_fish'][fishnumber] / 2):\n",
    "                                  Data['Otolith_Peak_Lateral_fish'][fishnumber] + round(Data['Otolith_Width_Lateral_fish'][fishnumber] / 2),\n",
    "                                  Data['Otolith_Peak_Dorsoventral_fish'][fishnumber] - round(Data['Otolith_Width_Dorsoventral_fish'][fishnumber] / 2):\n",
    "                                  Data['Otolith_Peak_Dorsoventral_fish'][fishnumber] + round(Data['Otolith_Width_Dorsoventral_fish'][fishnumber] / 2),\n",
    "                                 ].rechunk('auto').to_zarr(row['OutputNameOtolithRegion'],\n",
    "                                                           overwrite=True,\n",
    "                                                           compressor=Blosc(cname='zstd',\n",
    "                                                                            shuffle=Blosc.BITSHUFFLE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the otoliths in again\n",
    "Otoliths = [dask.array.from_zarr(file) for file in Data['OutputNameOtolithRegion']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def baller(image, center, ball_diameter, verbose=True):\n",
    "#     # From SubMyocardAnalysis.ipynb in Neoangiogenesis project\n",
    "#     '''\n",
    "#     Cut out a spherical region at a defined coordinate\n",
    "#     Save out both the small cutout region and the cutout region masked into the orignal size stack\n",
    "#     '''\n",
    "#     if verbose:\n",
    "#         print('Center', center)\n",
    "#     if len(image.shape) != len(center):\n",
    "#         print('We need three coordinates for the center')\n",
    "#     # First, get a cubic cutout from the original image\n",
    "#     cutout = image[center[0] - round(ball_diameter / 2):center[0] + round(ball_diameter / 2 + 1),\n",
    "#                    center[1] - round(ball_diameter / 2):center[1] + round(ball_diameter / 2 + 1),\n",
    "#                    center[2] - round(ball_diameter / 2):center[2] + round(ball_diameter / 2 + 1)]\n",
    "#     print('cutout %s' % str(cutout.shape))\n",
    "#     # Generate us a 'ball' mask\n",
    "#     print(math.floor(cutout.shape[0]/2))\n",
    "#     ball = skimage.morphology.ball(math.floor(cutout.shape[0]/2), dtype='bool')\n",
    "#     print('ball %s' % str(ball.shape))\n",
    "#     cutout *= ball\n",
    "#     if verbose:\n",
    "#         plt.subplot(231)\n",
    "#         plt.imshow(image[center[0]])\n",
    "#         plt.scatter(center[2], center[1], marker='x', color='w')\n",
    "        \n",
    "#         plt.gca().add_artist(plt.Circle((center[2], center[1]), round(ball_diameter / 2), fill=False, color='w', label='Cutout'))\n",
    "#         plt.title('AP slice %s' % center[0])\n",
    "#         plt.subplot(232)\n",
    "#         plt.imshow(image[:,center[1],:])\n",
    "#         plt.scatter(center[2], center[0], marker='x', color='w')\n",
    "        \n",
    "#         plt.gca().add_artist(plt.Circle((center[2], center[0]), round(ball_diameter / 2), fill=False, color='w', label='Cutout'))\n",
    "#         plt.title('LT slice %s' % center[1])\n",
    "#         plt.subplot(233)        \n",
    "#         plt.imshow(image[:,:,center[2]])\n",
    "#         plt.scatter(center[2], center[0], marker='x', color='w')\n",
    "#         plt.gca().add_artist(plt.Circle((center[1], center[0]), round(ball_diameter / 2), fill=False, color='w', label='Cutout'))        \n",
    "#         plt.title('DV slice %s' % center[2])\n",
    "#         for c, direction in enumerate(directions):\n",
    "#             plt.subplot(2,3,4+c)\n",
    "#             plt.imshow(cutout.max(axis=c))\n",
    "#         plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we calculated everything, we can display it for *one* fish for the manuscript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take one fish\n",
    "whichone = Data[Data['Fish'] == '11322'].index[0]\n",
    "print('The fish we are looking for is item %s in our dataframe' % whichone)\n",
    "print('We are loading the extracted otolith region of %s' % os.path.join(Data.Fish[whichone], Data.Scan[whichone]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put extracted otolith into a variable for simpler handling\n",
    "o = Otoliths[whichone]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show central views\n",
    "plt.subplot(131)\n",
    "plt.imshow(o[o.shape[0]//2,:,:])\n",
    "plt.subplot(132)\n",
    "plt.imshow(o[:,o.shape[1]//2,:])\n",
    "plt.subplot(133)\n",
    "plt.imshow(o[:,:,o.shape[2]//2])\n",
    "plt.suptitle('Middle views of %s' % Data['Folder'][whichone])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute histogram\n",
    "h, b = dask.array.histogram(o[o>0], bins=2**8, range=(0,2**8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute us some (multiotsu) peaks\n",
    "peaks = skimage.filters.threshold_multiotsu(o.compute(), classes=5)\n",
    "print(peaks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histogram and overlay the peaks\n",
    "plt.semilogy(h)\n",
    "# Show multiotsu threshold\n",
    "for p in peaks:\n",
    "    plt.axvline(p, label=p, c='red')\n",
    "plt.xlim([0,2**8])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segmentor(image, peaks, verbose=False):\n",
    "    # https://scikit-image.org/docs/dev/auto_examples/segmentation/plot_random_walker_segmentation.html#sphx-glr-auto-examples-segmentation-plot-random-walker-segmentation-py\n",
    "    # Set the background/discarded pixels to -1, 'stuff to be segmented' 0 and markers to 1\n",
    "    # the details given on https://scipy-lectures.org/packages/scikit-image/index.html?highlight=random%20walker#random-walker-segmentation\n",
    "    # Set everything to unlabeled\n",
    "    if not len(peaks):\n",
    "        print('Also give us some peaks to work with')\n",
    "        print('skimage.filters.threshold_multiotsu(STACK.compute(), classes=5) is a good start')\n",
    "        return()\n",
    "    markers = numpy.zeros_like(image, dtype='uint8')\n",
    "    # Set everything below background to be discarded.\n",
    "    # This is the same as above\n",
    "    if verbose:\n",
    "        print('Discarding everything below %s' % peaks[1])\n",
    "    markers[image < peaks[1]] = -1\n",
    "    if verbose:\n",
    "        print('Setting everything between %s and %s to 1' % (peaks[1], peaks[2]))\n",
    "    markers[(image > peaks[1]) & (image < peaks[2])] = 1\n",
    "    if verbose:\n",
    "        print('Setting everything above %s to 2' % peaks[3])\n",
    "    markers[image > peaks[3]] = 2\n",
    "    # Do the segmentation now\n",
    "    labels = random_walker(image.astype('uint8'), markers, copy=False)\n",
    "    if verbose:\n",
    "        # markers = markers.compute()\n",
    "        plt.subplot(2, 3, 1)\n",
    "        plt.imshow(image)\n",
    "        # plt.imshow(dask.array.ma.masked_where(-1, markers),\n",
    "        #            cmap='viridis',\n",
    "        #            alpha=0.5)\n",
    "        plt.title('Original')\n",
    "        plt.axis('off')      \n",
    "        plt.subplot(2, 3, 2)\n",
    "        plt.imshow(markers)\n",
    "        plt.title('Markers')\n",
    "        plt.axis('off')\n",
    "        plt.subplot(2, 3, 3)\n",
    "        plt.imshow(labels)\n",
    "        plt.title('Labels')        \n",
    "        plt.axis('off')            \n",
    "        for c, value in enumerate(numpy.unique(markers)):\n",
    "            plt.subplot(2, len(numpy.unique(markers)), len(numpy.unique(markers)) + c + 1)\n",
    "            plt.imshow(image)\n",
    "            plt.imshow(dask.array.ma.masked_not_equal(markers, value),\n",
    "                       cmap='viridis_r')\n",
    "            plt.title('%s=%s' % (c, value))\n",
    "            plt.axis('off')\n",
    "        plt.show()\n",
    "    # Return labeled image as an interpolated 8bit image\n",
    "    print(numpy.unique(markers))    \n",
    "    print(numpy.unique(labels))\n",
    "    return(numpy.interp(labels, (labels.min(), labels.max()), (labels.min(), 255)).astype('uint8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subsample=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask = segmentor(o[::subsample,::subsample,::subsample].compute(),\n",
    "#                  peaks,\n",
    "#                  verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numpy.unique(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# o.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# whichslice = 250\n",
    "# plt.imshow(o[::subsample,::subsample,::subsample][:,:,whichslice])\n",
    "# plt.imshow(mask[:,:,whichslice],\n",
    "#            alpha=0.309,\n",
    "#            cmap='viridis')\n",
    "# plt.title(whichslice)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display MIP view of extracted otolith region *in* original data\n",
    "outputname = os.path.join(os.path.dirname(Data['Folder'][whichone]), Data['Fish'][whichone] + '.Otholith.Region.MIP.png')\n",
    "for c, direction in enumerate(directions):\n",
    "    plt.subplot(1,3,c+1)\n",
    "    plt.imshow(o.max(axis=c))    \n",
    "    plt.axis('off')\n",
    "    plt.gca().add_artist(ScaleBar(Data.Voxelsize[whichone], 'um'))\n",
    "    plt.title('%s view' % direction)\n",
    "plt.suptitle('MIPs of otolith region of fish %s' % Data['Fish'][whichone])\n",
    "if not os.path.exists(outputname):\n",
    "    plt.savefig(outputname,\n",
    "                transparent=True,\n",
    "                bbox_inches='tight')\n",
    "    print('Figure saved to %s' % outfilepath)\n",
    "plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_largest_regions(segmentation, number_of_regions=4, verbose=False):\n",
    "    # Get out biggest item from image\n",
    "    # First iteration based on https://stackoverflow.com/a/55110923/323100\n",
    "    # Since we want to select the several largest items, we improved on it basedd on\n",
    "    # https://github.com/numpy/numpy/issues/15128 and\n",
    "    #  https://stackoverflow.com/questions/6910641/how-do-i-get-indices-of-n-maximum-values-in-a-numpy-array#comment24252527_6910672\n",
    "    labels = skimage.measure.label(segmentation)\n",
    "    # print(numpy.argsort(-numpy.bincount(labels.flat, weights=labels.flat))[:number_of_regions])\n",
    "    # print(sorted(numpy.bincount(labels.flat, weights=labels.flat), reverse=True)[:number_of_regions])\n",
    "    # print(numpy.argmax(numpy.bincount(labels.flat, weights=labels.flat)))\n",
    "    # Initialize empty array to add into\n",
    "    largestCC = dask.array.zeros_like(segmentation)\n",
    "    for lbl in numpy.argsort(-numpy.bincount(labels.flat, weights=labels.flat))[:number_of_regions]:\n",
    "        largestCC += labels == lbl\n",
    "    if verbose:\n",
    "        plt.subplot(121)\n",
    "        plt.imshow(segmentation)\n",
    "        plt.subplot(122)\n",
    "        plt.imshow(largestCC)\n",
    "        plt.suptitle('Largest connected component')\n",
    "        plt.show()\n",
    "    return dask.array.rechunk(largestCC.astype('uint8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get out largest regions\n",
    "l = get_largest_regions((o>peaks[-1]), number_of_regions=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the largest regions as masks for the original data\n",
    "masked = dask.array.multiply(o,l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_zeros(arr):\n",
    "    \"\"\"Returns a trimmed view of an n-D array excluding any outer\n",
    "    regions which contain only zeros.\n",
    "    From https://stackoverflow.com/a/65547931/323100\n",
    "    \"\"\"\n",
    "    slices = tuple(slice(idx.min(), idx.max() + 1) for idx in numpy.nonzero(arr))\n",
    "    return arr[slices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trim the masked data\n",
    "masked_trimmed = dask.array.asarray(trim_zeros(masked.compute()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display MIP of extracted and masked otolith region\n",
    "outputname = os.path.join(os.path.dirname(Data['Folder'][whichone]), Data['Fish'][whichone] + '.Otholith.Region.Extracted.MIP.png')\n",
    "for c, direction in enumerate(directions):\n",
    "    plt.subplot(1,3,c+1)\n",
    "    plt.imshow(masked_trimmed.max(axis=c))\n",
    "    plt.axis('off')\n",
    "    plt.gca().add_artist(ScaleBar(Data.Voxelsize[whichone], 'um'))\n",
    "    plt.title('%s view' % direction)\n",
    "plt.suptitle('MIPs of extracted Otolith of fish %s' % Data['Fish'][whichone])\n",
    "if not os.path.exists(outputname):\n",
    "    plt.savefig(outputname,\n",
    "                transparent=True,\n",
    "                bbox_inches='tight')\n",
    "    print('Figure saved to %s' % outfilepath)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View one otolith in 3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the largestCC of the otolith\n",
    "subsample = 1\n",
    "plt_mask = k3d.volume(masked_trimmed[::subsample,::subsample,::subsample].astype(numpy.float16),\n",
    "                      scaling=[Data.Voxelsize[whichone],\n",
    "                               Data.Voxelsize[whichone],\n",
    "                               Data.Voxelsize[whichone]]\n",
    "                     )\n",
    "plot = k3d.plot()\n",
    "plot += plt_mask\n",
    "plot.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save out HTML page, which we could potentially upload as supplementary material\n",
    "outputname3d = os.path.join(os.path.dirname(Data['Folder'][whichone]), Data['Fish'][whichone] + '.Otholith.Region.3D.html')\n",
    "if not os.path.exists(outputname3d):\n",
    "    with open(outputname3d, \"w\") as f:\n",
    "        f.write(plot.get_snapshot())\n",
    "    print('3D view saved to %s' % outputname3d)\n",
    "else:\n",
    "    print('3D view was already saved to %s, not saving it again' % outputname3d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asdfasdfasdf==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load into K3D\n",
    "# plt_volume = k3d.volume(Otoliths[whichone][::subsample,::subsample,::subsample].astype(numpy.float16), color_range=[threshold,Otoliths[whichone].max().compute()])\n",
    "# plt_volume = k3d.volume((Otoliths[whichone][::subsample,::subsample,::subsample]).astype(numpy.float16))\n",
    "# plt_mask = k3d.volume(mask.astype(numpy.float16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# masked_trimmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data and visualize it with a mask\n",
    "# See bottom of https://k3d-jupyter.org/reference/factory.volume.html\n",
    "plt_volume = k3d.volume((o[::subsample,::subsample,::subsample]).astype(np.float16),\n",
    "                        mask=mask.astype(np.uint8),\n",
    "                        # mask_opacities=[0.025, 3.0],\n",
    "                        # color_range=[0, 700]\n",
    "                       )\n",
    "\n",
    "plot = k3d.plot()\n",
    "plot += plt_volume\n",
    "plot.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(mask[200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asdfasdfasdf==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "N = 200\n",
    "positions = np.random.randn(N,3).astype(np.float32)\n",
    "\n",
    "plot2 = k3d.plot()\n",
    "plot2 += k3d.text([\"lorem ipsum\"] * N, positions, is_html=True)\n",
    "plot2.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "We should do some connected component analysis and extract some of the bigger things in the cube.\n",
    "And plot only those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(o.max(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c, row in Data.iterrows():\n",
    "    print(row.Fish, row.Folder)\n",
    "    otolither_region(c, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# whichone = 5\n",
    "# out = Reconstructions[whichone][716:1009+716-500,300:-300,800:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(out.max(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a column for saving the otolith positions\n",
    "for d, direction in enumerate(directions):\n",
    "    Data['Otholith_Positions_' + direction] = ''\n",
    "    Data['Otholith_Position_Mean_' + direction] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect otolith positions\n",
    "for c, row in Data.iterrows():\n",
    "    print('Finding otolith position for %s/%s' % (row.Fish, row.Scan))\n",
    "    for d, direction in enumerate(directions):\n",
    "        Data.at[c, 'Otholith_Positions_' + direction] = otolither(row['MIP_' + direction], sigma=11, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save us the mean position\n",
    "for d, direction in enumerate(directions):\n",
    "    Data['Otholith_Position_Mean_' + direction] = [(numpy.mean(op[0]),\n",
    "                                                    numpy.mean(op[1])) for op in Data['Otholith_Positions_' + direction]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in Data['Otholith_Positions_Lateral']:\n",
    "    print(round(numpy.mean(i[0])),\n",
    "          round(numpy.mean(i[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in Data['Otholith_Position_Mean_Lateral']:\n",
    "    print(round(i[0]), round(i[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for whichone in range(len(Data)):\n",
    "    print(whichone, os.path.join(Data.Fish[whichone], Data.Scan[whichone]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "direction = 'Lateral'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c, row in Data.iterrows():\n",
    "    for d, direction in enumerate(directions):\n",
    "        plt.subplot(1, 3, d + 1)\n",
    "        plt.imshow(row['MIP_' + direction])\n",
    "        plt.title([round(i) for i in row['Otholith_Position_Mean_' + direction]])\n",
    "        plt.axhline(row['Otholith_Position_Mean_' + direction][1])\n",
    "        plt.axvline(row['Otholith_Position_Mean_' + direction][0])\n",
    "        plt.suptitle('%s/%s' % (row.Fish, row.Scan))\n",
    "        plt.gca().add_artist(ScaleBar(row.Voxelsize, 'um'))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect otolith position by looking for maximum gray value along fish\n",
    "for d, direction in enumerate(directions):\n",
    "    Data['GrayValueAlong_' + direction] = ''\n",
    "    Data['Otolith_MIP_Position_' + direction] = ''\n",
    "for whichone, row in Data.iterrows():\n",
    "    for d, direction in enumerate(directions):\n",
    "        # Calculate gray value sum along fish.\n",
    "        Data.at[whichone, 'GrayValueAlong_' + direction] = dask.array.sum(Data['MIP_' + direction][whichone],\n",
    "                                                                          axis=1)\n",
    "        # Maximum of this shoud give us the otolith position\n",
    "        Data.at[whichone,\n",
    "                'Otolith_MIP_Position_' + direction] = dask.array.argmax(dask.array.sum(Data['MIP_' + direction][whichone],\n",
    "                                                                                        axis=1))\n",
    "        # Plot what we found\n",
    "        plt.subplot(1, 3, d + 1)\n",
    "        plt.imshow(Data['MIP_' + direction][whichone])\n",
    "        # Plot the *rescaled* values over the image\n",
    "        plt.plot(rescale_linear(Data['GrayValueAlong_' + direction][whichone],\n",
    "                                100,\n",
    "                                Data['MIP_' + direction][whichone].shape[1] - 100),\n",
    "                 range(len(Data['GrayValueAlong_' + direction][whichone])),\n",
    "                 label='Normalized gray value sum along fish',\n",
    "                 color=seaborn.color_palette()[0])\n",
    "        plt.axhline(Data['Otolith_MIP_Position_' + direction][whichone],\n",
    "                    label='Max@%s' % Data['Otolith_MIP_Position_' + direction][whichone].compute(),\n",
    "                    color=seaborn.color_palette()[1])\n",
    "        plt.title('%s MIP' % direction)\n",
    "        plt.suptitle('%s/%s: MIPs of %s/%s' % (whichone, len(Data), Data.Fish[whichone], Data.Scan[whichone]))\n",
    "        plt.legend(loc='lower center')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c, row in Data.iterrows():\n",
    "    print(c, len(Data), os.path.join(row.Fish, row.Scan))\n",
    "    print('\\t Otolith from MIP',\n",
    "          row['Otolith_MIP_Position_Anteroposterior'].compute(),\n",
    "          row['Otolith_MIP_Position_Lateral'].compute(),\n",
    "          row['Otolith_MIP_Position_Dorsoventral'].compute())\n",
    "    print('\\t Otolith from otholither function',\n",
    "          row['Otholith_Position_Mean_Anteroposterior'],\n",
    "          row['Otholith_Position_Mean_Lateral'],\n",
    "          row['Otholith_Position_Mean_Dorsoventral'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c, direction in enumerate(directions):\n",
    "    plt.subplot(1, 3, c + 1)\n",
    "    plt.imshow(Data['MIP_' + direction][whichone])\n",
    "    # From otholither function\n",
    "    plt.axhline(Data['Otholith_Position_Mean_' + direction][whichone][1],\n",
    "                label='otholither mean position 1: %s' % round(Data['Otholith_Position_Mean_' + direction][whichone][1]),\n",
    "                color=seaborn.color_palette()[0])\n",
    "    plt.axvline(Data['Otholith_Position_Mean_' + direction][whichone][0],\n",
    "                label='otholither mean posistion 0: %s' % round(Data['Otholith_Position_Mean_' + direction][whichone][0]),\n",
    "                color=seaborn.color_palette()[1])\n",
    "    # From sum along axis\n",
    "    plt.axhline(Data['Otolith_MIP_Position_' + direction][whichone],\n",
    "                label='MIP sum: %s' % Data['Otolith_MIP_Position_' + direction][whichone].compute(),\n",
    "                color=seaborn.color_palette()[3])\n",
    "    plt.legend(loc='lower center')\n",
    "    plt.title(direction)\n",
    "    plt.suptitle('%s/%s' % (Data.Fish[whichone], Data.Scan[whichone]))\n",
    "    plt.gca().add_artist(ScaleBar(Data.Voxelsize[whichone], 'um'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for direction in directions:\n",
    "    print(direction, Data['MIP_' + direction][whichone].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for direction in directions:\n",
    "    print(direction, Data['Otholith_Position_Mean_' + direction][whichone])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for direction in directions:\n",
    "    print(direction, round(Data['Otolith_MIP_Position_' + direction][whichone].compute()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CAN WE CALCULATE BOTH DV and THE LT POSITION ON THE AP MIP?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get us positions of otolith in relation to original data\n",
    "position_ap = numpy.mean((Data['Otholith_Position_Mean_Lateral'][whichone][1],\n",
    "                          Data['Otholith_Position_Mean_Dorsoventral'][whichone][1],\n",
    "                          Data['Otolith_MIP_Position_Lateral'][whichone],\n",
    "                          Data['Otolith_MIP_Position_Dorsoventral'][whichone]),\n",
    "                         dtype='int')\n",
    "# laterally, we assume the center of the image for now\n",
    "# position_lt = numpy.mean((Data['Otholith_Position_Mean_' + direction][whichone][1],\n",
    "#                           Data['Otolith_MIP_Position_' + direction][whichone]),dtype='int')\n",
    "position_lt = Data.MIP_Dorsoventral[whichone].shape[1] // 2\n",
    "position_dv = numpy.mean((Data['Otholith_Position_Mean_Anteroposterior'][whichone][0],\n",
    "                          Data['Otholith_Position_Mean_Lateral'][whichone][0]),\n",
    "                         dtype='int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(position_ap)\n",
    "print(position_lt)\n",
    "print(position_dv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slicethickness = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c, direction in enumerate(directions):\n",
    "    plt.subplot(1, 3, c + 1)\n",
    "    plt.imshow(Data['MIP_' + direction][whichone])\n",
    "    if c == 0:\n",
    "        plt.fill_between(range(Data['MIP_' + direction][whichone].shape[1]),\n",
    "                         position_lt - slicethickness // 2,\n",
    "                         position_lt + slicethickness // 2,\n",
    "                         alpha=0.5)\n",
    "        plt.fill_between(range(position_dv - slicethickness // 2, position_dv + slicethickness // 2),\n",
    "                         1,\n",
    "                         Data['MIP_' + direction][whichone].shape[0] - 1,\n",
    "                         alpha=0.5)\n",
    "    elif c == 1:\n",
    "        plt.fill_between(range(Data['MIP_' + direction][whichone].shape[1]),\n",
    "                         position_ap - slicethickness // 2,\n",
    "                         position_ap + slicethickness // 2,\n",
    "                         alpha=0.5)\n",
    "        plt.fill_between(range(position_dv - slicethickness // 2, position_dv + slicethickness // 2),\n",
    "                         1,\n",
    "                         Data['MIP_' + direction][whichone].shape[0] - 1,\n",
    "                         alpha=0.5)\n",
    "    else:\n",
    "        plt.fill_between(range(Data['MIP_' + direction][whichone].shape[1]),\n",
    "                         position_ap - slicethickness // 2,\n",
    "                         position_ap + slicethickness // 2,\n",
    "                         alpha=0.5)\n",
    "        plt.fill_between(range(position_lt - slicethickness // 2, position_lt + slicethickness // 2),\n",
    "                         1,\n",
    "                         Data['MIP_' + direction][whichone].shape[0] - 1,\n",
    "                         alpha=0.5)\n",
    "    plt.title(direction)\n",
    "    plt.suptitle('%s/%s' % (Data.Fish[whichone], Data.Scan[whichone]))\n",
    "    plt.gca().add_artist(ScaleBar(Data.Voxelsize[whichone], 'um'))\n",
    "    \n",
    "outfilepath = os.path.join(os.path.dirname(Data['Folder'][whichone]),\n",
    "                           '%s.%s.Otolither.png' % (Data['Fish'][whichone], Data['Scan'][whichone]))\n",
    "if not os.path.exists(outfilepath):\n",
    "    plt.savefig(outfilepath,\n",
    "                oarent=True,\n",
    "                bbox_inches='tight')\n",
    "    print('Figure saved to %s' % outfilepath)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(position_ap)\n",
    "print(position_lt)\n",
    "print(position_dv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab region calculated above from reconstructions\n",
    "otolithregion = Reconstructions[whichone][position_ap - slicethickness // 2:position_ap + slicethickness // 2,\n",
    "                                          position_lt - slicethickness // 2:position_lt + slicethickness // 2,\n",
    "                                          position_dv - slicethickness // 2:position_dv + slicethickness // 2\n",
    "                                         ]\n",
    "for ax in range(3):\n",
    "    plt.subplot(1, 3, ax + 1)\n",
    "    plt.imshow(dask.array.max(otolithregion, axis=ax))\n",
    "    plt.suptitle('%s/%s: MIP from AP %s:%s, LT %s:%s, DV %s:%s' % (Data.Fish[whichone],\n",
    "                                                                   Data.Scan[whichone],\n",
    "                                                                   position_ap - slicethickness // 2, position_ap + slicethickness // 2,\n",
    "                                                                   position_lt - slicethickness // 2, position_lt + slicethickness // 2,\n",
    "                                                                   position_dv - slicethickness // 2, position_dv + slicethickness // 2))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold(otolithregion.compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold(otolithregion[otolithregion > 42].compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(otolithregion[slicethickness//2])\n",
    "plt.imshow(dask.array.ma.masked_equal(otolithregion[slicethickness//2] > 102,0), cmap='viridis', alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file = open('largest', 'wb')\n",
    "# pickle.dump(largest,file)\n",
    "# file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file = open('largest', 'rb')\n",
    "# largest = pickle.load(file)\n",
    "# file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# largest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Make file smaller for testing reasons\n",
    "# subsample = 4\n",
    "# largest_smaller = largest[::subsample, ::subsample, ::subsample]\n",
    "# largest_smaller.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsample = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vmin = threshold(otolithregion.compute())\n",
    "print(vmin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load into K3D\n",
    "plt_volume = k3d.volume(dask.array.ma.masked_less(otolithregion,102)[::subsample, ::subsample, ::subsample].astype(numpy.float16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the otolith\n",
    "plot = k3d.plot()\n",
    "plot += plt_volume\n",
    "plot.display()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot5 = k3d.get_plot()\n",
    "# plot5.snapshot_type = 'inline'\n",
    "# plot5.display()\n",
    "\n",
    "# data = plot5.get_snapshot()\n",
    "\n",
    "# with open('snapshot_inline.html', 'w') as f:\n",
    "#     f.write(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
