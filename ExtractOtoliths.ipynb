{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fiddle with the EAWAG scans\n",
    "Look at the orientation and see if we can do some cropping based on landmarks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "import os\n",
    "import glob\n",
    "import pandas\n",
    "import imageio\n",
    "import numpy\n",
    "import scipy\n",
    "import k3d\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib_scalebar.scalebar import ScaleBar\n",
    "import seaborn\n",
    "import dask\n",
    "import dask_image.imread\n",
    "import skimage\n",
    "from tqdm.auto import tqdm, trange\n",
    "import math\n",
    "from numcodecs import Blosc\n",
    "from skimage.segmentation import random_walker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set dask temporary folder\n",
    "# Do this before creating a client: https://stackoverflow.com/a/62804525/323100\n",
    "import tempfile\n",
    "if 'Linux' in platform.system():\n",
    "    # Check if me mounted the FastSSD, otherwise go to standard tmp file\n",
    "    if os.path.exists(os.path.join(os.sep, 'media', 'habi', 'Fast_SSD')):\n",
    "        tmp = os.path.join(os.sep, 'media', 'habi', 'Fast_SSD', 'tmp')\n",
    "    else:\n",
    "        tmp = tempfile.gettempdir()\n",
    "elif 'Darwin' in platform.system():\n",
    "    tmp = tempfile.gettempdir()\n",
    "else:\n",
    "    if 'anaklin' in platform.node():\n",
    "        tmp = os.path.join('F:\\\\tmp')\n",
    "    else:\n",
    "        tmp = os.path.join('D:\\\\tmp')\n",
    "dask.config.set({'temporary_directory': tmp})\n",
    "print('Dask temporary files go to %s' % dask.config.get('temporary_directory'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "client = Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('You can seee what DASK is doing at \"http://localhost:%s/status\"' % client.scheduler_info()['services']['dashboard'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Ignore warnings in the notebook\n",
    "# import warnings\n",
    "# warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seaborn theme\n",
    "seaborn.set_theme(context='notebook', style='dark')\n",
    "# Set up figure defaults\n",
    "plt.rc('image', cmap='gray', interpolation='nearest')  # Display all images in b&w and with 'nearest' interpolation\n",
    "plt.rcParams['figure.figsize'] = (8, 4.5)  # Size up figures a bit\n",
    "plt.rcParams['figure.dpi'] = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup scale bar defaults\n",
    "plt.rcParams['scalebar.location'] = 'lower right'\n",
    "plt.rcParams['scalebar.frameon'] = False\n",
    "plt.rcParams['scalebar.color'] = 'white'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display all plots identically\n",
    "lines = 3\n",
    "# And then do something like\n",
    "# plt.subplot(lines, int(numpy.ceil(len(Data) / float(lines))), c + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different locations if running either on Linux or Windows\n",
    "FastSSD = True\n",
    "overthere = False  # Load the data directly from the iee-research_storage drive\n",
    "nanoct = True  # Load the data directly from the 2214\n",
    "# to speed things up significantly\n",
    "if 'Linux' in platform.system():\n",
    "    if FastSSD:\n",
    "        BasePath = os.path.join(os.sep, 'media', 'habi', 'Fast_SSD')\n",
    "    elif overthere:\n",
    "        BasePath = os.path.join(os.sep, 'home', 'habi', 'research-storage-iee')\n",
    "    elif nanoct:\n",
    "        BasePath = os.path.join(os.path.sep, 'home', 'habi', '2214')\n",
    "    else:\n",
    "        BasePath = os.path.join(os.sep, 'home', 'habi', '1272')\n",
    "elif 'Darwin' in platform.system():\n",
    "    FastSSD = False\n",
    "    BasePath = os.path.join('/Users/habi/Dev/EAWAG/Data')\n",
    "elif 'Windows' in platform.system():\n",
    "    if FastSSD:\n",
    "        BasePath = os.path.join('F:\\\\')\n",
    "    else:\n",
    "        if 'anaklin' in platform.node():\n",
    "            BasePath = os.path.join('S:\\\\')\n",
    "        else:\n",
    "            BasePath = os.path.join('D:\\\\Results')\n",
    "if not overthere:\n",
    "    Root = os.path.join(BasePath, 'EAWAG')\n",
    "else:\n",
    "    Root = BasePath\n",
    "# if overthere:\n",
    "#         Root = os.path.join('I:\\\\microCTupload')\n",
    "print('We are loading all the data from %s' % Root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pixelsize(logfile):\n",
    "    \"\"\"Get the pixel size from the scan log file\"\"\"\n",
    "    with open(logfile, 'r') as f:\n",
    "        for line in f:\n",
    "            if 'Image Pixel' in line and 'Scaled' not in line:\n",
    "                pixelsize = float(line.split('=')[1])\n",
    "    return(pixelsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_git_hash():\n",
    "    '''\n",
    "    Get the current git hash from the repository.\n",
    "    Based on http://stackoverflow.com/a/949391/323100 and\n",
    "    http://stackoverflow.com/a/18283905/323100\n",
    "    '''\n",
    "    from subprocess import Popen, PIPE\n",
    "    import os\n",
    "    gitprocess = Popen(['git',\n",
    "                        '--git-dir',\n",
    "                        os.path.join(os.getcwd(), '.git'),\n",
    "                        'rev-parse',\n",
    "                        '--short',\n",
    "                        '--verify',\n",
    "                        'HEAD'],\n",
    "                       stdout=PIPE)\n",
    "    (output, _) = gitprocess.communicate()\n",
    "    return output.strip().decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Make directory for output\n",
    "# OutPutDir = os.path.join(os.getcwd(), 'Output', get_git_hash())\n",
    "# print('We are saving all the output to %s' % OutPutDir)\n",
    "# os.makedirs(OutPutDir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make us a dataframe for saving all that we need\n",
    "Data = pandas.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get *all* log files, unsorted but fast\n",
    "Data['LogFile'] = [os.path.join(root, name)\n",
    "                   for root, dirs, files in os.walk(Root)\n",
    "                   for name in files\n",
    "                   if name.endswith((\".log\"))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('We found %s log files in %s' % (len(Data), Root))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limit *all* the data to only the 'head' scans\n",
    "Data = Data[Data['LogFile'].str.contains('head')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('We have %s log files with \"head\" in their name in %s' % ((len(Data)), Root))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate folder name\n",
    "Data['Folder'] = [os.path.dirname(f) for f in Data['LogFile']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get rid of all non-rec logfiles\n",
    "for c, row in Data.iterrows():\n",
    "    if 'rec' not in row.Folder:\n",
    "        Data.drop([c], inplace=True)\n",
    "    elif 'SubScan' in row.Folder:\n",
    "        Data.drop([c], inplace=True)\n",
    "    elif 'rectmp.log' in row.LogFile:\n",
    "        Data.drop([c], inplace=True)\n",
    "# Reset dataframe to something that we would get if we only would have loaded the 'rec' files\n",
    "Data = Data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate us some meaningful colums\n",
    "Data['Fish'] = [l[len(Root) + 1:].split(os.sep)[0] for l in Data['LogFile']]\n",
    "Data['Scan'] = ['_'.join(l[len(Root) + 1:].split(os.sep)[1:-1]) for l in Data['LogFile']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the file names of the reconstructions\n",
    "Data['Reconstructions'] = [[os.path.join(root, name)\n",
    "                            for root, dirs, files in os.walk(f)\n",
    "                            for name in files\n",
    "                            if 'rec0' in name and name.endswith((\".png\"))] for f in Data['Folder']]\n",
    "# Count how many files we have\n",
    "Data['Number of reconstructions'] = [len(r) for r in Data.Reconstructions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop samples which have either not been reconstructed yet or of which we deleted the reconstructions with\n",
    "# `find . -name \"*rec*.png\" -type f -mtime +333 -delete`\n",
    "# Based on https://stackoverflow.com/a/13851602\n",
    "# for c, row in Data.iterrows():\n",
    "#     if not row['Number of reconstructions']:\n",
    "#         print('%s contains no PNG files, we might be currently reconstructing it' % row.Folder)\n",
    "Data = Data[Data['Number of reconstructions'] > 0]\n",
    "Data.reset_index(drop=True, inplace=True)\n",
    "print('We have %s folders with reconstructions in %s' % ((len(Data)), Root))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get parameters we need from the log files\n",
    "Data['Voxelsize'] = [get_pixelsize(log) for log in Data['LogFile']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load all reconstructions into DASK arrays\n",
    "Reconstructions = [None] * len(Data)\n",
    "for c, row in tqdm(Data.iterrows(),\n",
    "                   desc='Load reconstructions',\n",
    "                   total=len(Data)):\n",
    "    Reconstructions[c] = dask_image.imread.imread(os.path.join(row['Folder'],\n",
    "                                                               '*rec*.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check if something went wrong\n",
    "# for file in Data['OutputNameRec']:\n",
    "#     print(file)\n",
    "#     dask.array.from_zarr(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How big are the datasets?\n",
    "Data['Size'] = [rec.shape for rec in Reconstructions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The three cardinal directions\n",
    "# Names adapted to fishes: https://en.wikipedia.org/wiki/Fish_anatomy#Body\n",
    "directions = ['Anteroposterior',\n",
    "              'Lateral',\n",
    "              'Dorsoventral']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in previously generated MIPs or calculate them\n",
    "for d, direction in enumerate(directions):\n",
    "    Data['MIP_' + direction] = ''\n",
    "for c, row in tqdm(Data.iterrows(), desc='Calculating MIPs', total=len(Data)):\n",
    "    for d, direction in tqdm(enumerate(directions),\n",
    "                             desc='%s/%s: %s' % (row['Fish'], row['Scan'], direction),\n",
    "                             leave=False,\n",
    "                             total=len(directions)):\n",
    "        outfilepath = os.path.join(os.path.dirname(row['Folder']),\n",
    "                                   '%s.%s.MIP.%s.png' % (row['Fish'], row['Scan'], direction))\n",
    "        if os.path.exists(outfilepath):\n",
    "            Data.at[c, 'MIP_' + direction] = dask_image.imread.imread(outfilepath).squeeze()\n",
    "        else:\n",
    "            # Generate MIP\n",
    "            Data.at[c, 'MIP_' + direction] = Reconstructions[c].max(axis=d).compute().squeeze()\n",
    "            # Save it out\n",
    "            imageio.imwrite(outfilepath, Data.at[c, 'MIP_' + direction].astype('uint8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect views\n",
    "for c, row in tqdm(Data.iterrows(),\n",
    "                   desc='Saving overview of MIP images',\n",
    "                   total=len(Data)):\n",
    "    outfilepath = os.path.join(os.path.dirname(row['Folder']),\n",
    "                               '%s.%s.MIPs.png' % (row['Fish'], row['Scan']))\n",
    "    if not os.path.exists(outfilepath):    \n",
    "        print('%s/%s: %s' % (c, len(Data), os.path.join(row.Fish, row.Scan)))\n",
    "        for d, direction in tqdm(enumerate(directions),\n",
    "                                 desc='%s/%s' % (row['Fish'], row['Scan']),\n",
    "                                 leave=False,\n",
    "                                 total=len(directions)):\n",
    "            plt.subplot(1, 3, d + 1)\n",
    "            plt.imshow(row['MIP_' + direction])\n",
    "            plt.gca().add_artist(ScaleBar(row['Voxelsize'],\n",
    "                                          'um'))\n",
    "            plt.title('%s MIP' % direction)\n",
    "            plt.axis('off')\n",
    "            plt.title('%s\\n%s MIP' % (os.path.join(row['Fish'], row['Scan']), direction))\n",
    "            plt.savefig(outfilepath,\n",
    "                        transparent=True,\n",
    "                        bbox_inches='tight')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The functions below were copied from Hearts-Melly/SubMyocardAnalysis.ipynb, in which we also look at the orientation of things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From Hearts-Melly/SubMyocardAnalysis.ipynb\n",
    "def get_properties(roi, verbose=False):\n",
    "    # Label filled image\n",
    "    labeled_img = skimage.measure.label(roi)\n",
    "    # Extract regionprops of image and put data into pandas\n",
    "    # https://stackoverflow.com/a/66632023/323100\n",
    "    props = skimage.measure.regionprops_table(labeled_img,\n",
    "                                              properties=('label',\n",
    "                                                          'centroid',\n",
    "                                                          'area',\n",
    "                                                          'perimeter',\n",
    "                                                          'orientation'))\n",
    "    table = pandas.DataFrame(props)\n",
    "    table_sorted = table.sort_values(by='area', ascending=False)\n",
    "    # return only the region with the biggest area\n",
    "    properties = table_sorted.iloc[:1].reset_index()\n",
    "    if verbose:\n",
    "        plt.imshow(roi, alpha=0.5)\n",
    "        plt.title('Original')\n",
    "        plt.axis('off')\n",
    "        plt.imshow(numpy.ma.masked_equal(labeled_img, 0), cmap='viridis', alpha=0.5)\n",
    "        plt.title('Labelled')\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "    return(properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_largest_region(segmentation, verbose=False):\n",
    "    # Get out biggest item from https://stackoverflow.com/a/55110923/323100\n",
    "    labels = skimage.measure.label(segmentation)\n",
    "    assert(labels.max() != 0)  # assume at least 1 CC\n",
    "    largestCC = labels == numpy.argmax(numpy.bincount(labels.flat)[1:]) + 1\n",
    "    if verbose:\n",
    "        plt.subplot(121)\n",
    "        plt.imshow(segmentation)\n",
    "        plt.subplot(122)\n",
    "        plt.imshow(largestCC)\n",
    "        plt.suptitle('Largest connected component')\n",
    "        plt.show()\n",
    "    return largestCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def threshold(image, verbose=False):\n",
    "    # Calculate threshold of image where image is non-zero\n",
    "    threshold = skimage.filters.threshold_otsu(image[image > 0])\n",
    "    if verbose:\n",
    "        plt.subplot(121)\n",
    "        plt.imshow(image)\n",
    "        plt.imshow(dask.array.ma.masked_equal(image > threshold, 0),\n",
    "                   alpha=0.618,\n",
    "                   cmap='viridis_r')\n",
    "        plt.subplot(122)\n",
    "        plt.semilogy(histogram(image), label='Log-Histogram')\n",
    "        plt.axvline(threshold, label='Otsu threshold: %s' % threshold)\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    return(threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the histogram of an image\n",
    "# We can safely assume to only use 8bit images\n",
    "def histogram(img):\n",
    "    histogram, bins = dask.array.histogram(dask.array.array(img),\n",
    "                                           bins=2**8,\n",
    "                                           range=[0, 2**8])\n",
    "    return(histogram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whichone = 4\n",
    "print(os.path.join(Data.Fish[whichone], Data.Scan[whichone]))\n",
    "img = dask.array.asarray(Data.MIP_Anteroposterior[whichone])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = threshold(img.compute(), verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = get_largest_region(img > t, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_centroid(img, verbose=False):\n",
    "    props = get_properties(img)\n",
    "    # Drawing from https://scikit-image.org/docs/stable/auto_examples/segmentation/plot_regionprops.html\n",
    "    y0, x0 = props['centroid-0'], props['centroid-1']\n",
    "    if verbose:\n",
    "        plt.imshow(img)\n",
    "        plt.scatter(props['centroid-1'], props['centroid-0'], marker=None, color='r')\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "    return((x0, y0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_contour(filled_img, verbose=False):\n",
    "    # Contouring from https://scikit-image.org/docs/dev/auto_examples/segmentation/plot_regionprops.html\n",
    "    largest_region = get_largest_region(filled_img, verbose=False)\n",
    "    contour = skimage.measure.find_contours(largest_region)\n",
    "    # Even though we look only at the largest region, we still might get out more than one contour\n",
    "    # Let's thus sort the list and just continue with the longest one\n",
    "    (contour).sort(key=len)\n",
    "    cy, cx = contour[-1].T\n",
    "    if verbose:\n",
    "        plt.imshow(filled_img)\n",
    "        plt.plot(cx, cy, lw=1, c='r')\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "    return(cx, cy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contour = get_contour(img > t, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroid = get_centroid(img > t, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_orientation(img, x0, x1, x2, y0, y1, y2, self=False):\n",
    "    if self:\n",
    "        plt.imshow(img)\n",
    "    plt.plot((x0, x1), (y0, y1), '-r', linewidth=1)\n",
    "    plt.plot((x0, x2), (y0, y2), '-r', linewidth=1)\n",
    "    if self:\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "    return()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_orientation(img, voxelsize, length=10, verbose=False):\n",
    "    '''\n",
    "    Get and draw orientation onto image, with a (default) length of 10 mm\n",
    "    '''\n",
    "    props = get_properties(img)\n",
    "    whichlengthdowewant = length\n",
    "    reallength = whichlengthdowewant / voxelsize * 1000  # mm\n",
    "    # Drawing from https://scikit-image.org/docs/stable/auto_examples/segmentation/plot_regionprops.htm\n",
    "    x0, y0 = get_centroid(img)\n",
    "    x1 = x0 + math.cos(props['orientation']) * reallength\n",
    "    y1 = y0 - math.sin(props['orientation']) * reallength\n",
    "    x2 = x0 - math.sin(props['orientation']) * reallength\n",
    "    y2 = y0 - math.cos(props['orientation']) * reallength\n",
    "    if verbose:\n",
    "        plt.imshow(img)\n",
    "        plt.scatter(props['centroid-1'], props['centroid-0'], marker=None, color='r')\n",
    "        draw_orientation(img, x0, x1, x2, y0, y1, y2)\n",
    "        plt.gca().add_artist(ScaleBar(voxelsize, 'um'))\n",
    "        plt.title('Image with %s mm long orientation bars' % length)\n",
    "        plt.show()\n",
    "    return(x0, x1, x2, y0, y1, y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0, x1, x2, y0, y1, y2 = get_orientation(img > t,\n",
    "                                         Data.Voxelsize[4],\n",
    "                                         length=2,\n",
    "                                         verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The functions above were copied from Hearts-Melly/SubMyocardAnalysis.ipynb, in which we also look at the orientation of things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = get_properties(img > t)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the angle and centroid to rotate image\n",
    "img_rotated = numpy.empty_like(img)\n",
    "img_rotated = skimage.transform.rotate(img.compute(),\n",
    "                                       angle=numpy.rad2deg(a.orientation[0]),\n",
    "                                       center=(a['centroid-0'][0], a['centroid-1'][0]),\n",
    "                                       preserve_range=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy.rad2deg(a.orientation[0])+90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's show what we did\n",
    "plt.subplot(121)\n",
    "plt.imshow(img)\n",
    "plt.scatter(a['centroid-0'], a['centroid-1'], s=50)\n",
    "plt.title('Original image with centroid')\n",
    "plt.subplot(122)\n",
    "plt.imshow(img_rotated)\n",
    "plt.scatter(a['centroid-0'], a['centroid-1'], s=50)\n",
    "plt.title('Image rotated by %0.f°, with centroid' % numpy.rad2deg(a.orientation[0]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure out the otolith position on each of the directional views."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smoother(curve, frac=0.1):\n",
    "    ''' Smooth a curve '''\n",
    "    from statsmodels.nonparametric.smoothers_lowess import lowess\n",
    "    smoothed = lowess(curve, range(len(curve)), return_sorted=False, frac=frac)\n",
    "    return(smoothed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_minimum(curve, verbose=False):\n",
    "    '''\n",
    "    Function to detect get a 'border' based on the gray value along the image.\n",
    "    We do this by detecting the minimum of the derivative of the smoothed grayvalue curve\n",
    "    Based on https://stackoverflow.com/a/28541805/323100 and some manual tweaking\n",
    "    '''\n",
    "    smoothed = smoother(curve)\n",
    "    minimal_diff = numpy.argmin(numpy.diff(smoothed))\n",
    "    if verbose:\n",
    "        plt.plot(curve, alpha=0.6, label='Input curve')\n",
    "        plt.plot(smoothed, label='LOWESS smoothed')\n",
    "        plt.axvline(minimal_diff, c='r', label='Border')\n",
    "        plt.legend()\n",
    "    return(minimal_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_maximum(curve, verbose=False):\n",
    "    '''\n",
    "    Function to detect get a 'border' based on the gray value along the image.\n",
    "    We do this by detecting the minimum of the derivative of the smoothed grayvalue curve\n",
    "    Based on https://stackoverflow.com/a/28541805/323100 and some manual tweaking\n",
    "    '''\n",
    "    smoothed = smoother(curve)\n",
    "    maximal_diff = numpy.argmax(numpy.diff(smoothed))\n",
    "    if verbose:\n",
    "        plt.plot(curve, alpha=0.6, label='Input curve')\n",
    "        plt.plot(smoothed, label='LOWESS smoothed')\n",
    "        plt.axvline(maximal_diff, c='r', label='Border')\n",
    "        plt.legend()\n",
    "    return(maximal_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_peak(curve, start=None, stop=None, frac=0.25, height=0.25, verbose=False):\n",
    "    ''' Find a peak in the smoothed curve '''\n",
    "    # Peak finding from https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.find_peaks.html\n",
    "    # Mask a bit at the start and a bit start and end of curve, if desired\n",
    "    mask = numpy.zeros_like(curve)    \n",
    "    if start:\n",
    "        mask[:start] = 1\n",
    "    if stop:\n",
    "        mask[stop:] = 1\n",
    "    if start or stop:\n",
    "        original_curve = curve\n",
    "        smoothed = smoother(numpy.ma.masked_where(mask, curve).filled(fill_value=0), frac=frac)\n",
    "    else:\n",
    "        smoothed = smoother(curve, frac=frac)\n",
    "    if verbose:\n",
    "        print('The input curve has a length of %s' % len(curve))\n",
    "        if start:\n",
    "            print('We discard the %s values from the start' % start)\n",
    "        if stop:\n",
    "            print('We discard the values from %s to the end' % stop)            \n",
    "        print('The input to the smoother has a length of %s' % len(numpy.ma.masked_where(mask, curve).compressed()))\n",
    "    peak, _ = scipy.signal.find_peaks(smoothed, width=100)    \n",
    "    peak_value = smoothed[peak]\n",
    "    # Peak width from https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.peak_widths.html\n",
    "    results_width = scipy.signal.peak_widths(smoothed, peak, rel_height=height)\n",
    "    if len(peak) > 1:\n",
    "        # Return only 'higher' peak if we have several\n",
    "        peak = numpy.asarray(peak[numpy.argmax(peak_value)])\n",
    "        results_width = [item[numpy.argmax(peak_value)] for item in results_width]\n",
    "        peak_value = int(peak_value[numpy.argmax(peak_value)])\n",
    "    # Get actual width\n",
    "    width = results_width[0]\n",
    "    # if start:\n",
    "    #     peak = peak + start\n",
    "    #     results_width = [v[0]+start for v in results_width]\n",
    "    if verbose:\n",
    "        if start or stop:\n",
    "            plt.plot(original_curve, alpha=0.618, label='Original')\n",
    "        plt.plot(numpy.ma.masked_where(mask, curve), label='Input')\n",
    "        plt.plot(smoothed, label='Smoothed (frac=%s)' % frac)\n",
    "        plt.plot(peak,\n",
    "                 smoothed[peak],\n",
    "                 'x',\n",
    "                 color='C2',\n",
    "                 label='Peak@%s' % int(peak))\n",
    "        plt.hlines(*results_width[1:],\n",
    "                   color=\"C3\",\n",
    "                   label='Peak width at %d%%: %s' % (100*height, int(width)))\n",
    "        plt.legend()\n",
    "        # plt.xlim([0,len(curve)])\n",
    "        plt.show()\n",
    "\n",
    "    return(int(peak), int(peak_value), int(width))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put gray values in temporary lists\n",
    "whichone = 19\n",
    "gv =  [None] * len(directions) \n",
    "gh =  [None] * len(directions) \n",
    "for c, direction in enumerate(directions):\n",
    "    gv[c] = Reconstructions[whichone].max(axis=c).sum(axis=0).compute()\n",
    "    gh[c] = Reconstructions[whichone].max(axis=c).sum(axis=1).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_peak(gv[0], start=100, stop=1000, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in gh:\n",
    "    get_peak(i, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in gv:\n",
    "    get_peak(i, height=0.25, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rescale_linear(array, new_min, new_max):\n",
    "    \"\"\"Rescale an arrary linearly. From https://stackoverflow.com/a/50011743/323100\"\"\"\n",
    "    minimum, maximum = numpy.min(array), numpy.max(array)\n",
    "    m = (new_max - new_min) / (maximum - minimum)\n",
    "    b = new_min - m * minimum\n",
    "    return m * array + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_grayvalues(image, plane, which):\n",
    "    ''' get grayvalue along horizontal or vertical image plane '''\n",
    "    grayvalues = []\n",
    "    if plane == 'horizontal':\n",
    "        ax = 0\n",
    "    elif plane == 'vertical':\n",
    "        ax = 1\n",
    "    else:\n",
    "        print('No plane given, specify either \"plane=horizontal\" or \"plane=vertical\"')\n",
    "        print('Returning EMPTY grayvalues')        \n",
    "        return(grayvalues)\n",
    "    if which == 'sum':\n",
    "        grayvalues = image.sum(axis=ax)\n",
    "    elif which == 'max':\n",
    "        grayvalues = image.max(axis=ax)\n",
    "    else:\n",
    "        print('No method given, specify either \"which=max\" or \"which=sum\"')\n",
    "        print('Returning EMPTY grayvalues')\n",
    "        return(grayvalues)\n",
    "    return(grayvalues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dir in directions:\n",
    "    print(dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "planes = ['horizontal', 'vertical']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate empty columns to fill in the values we calculate below\n",
    "for c, direction in enumerate(directions):\n",
    "    Data['Discard_' + direction + '_fish'] = '' # General discard directions\n",
    "    Data['Otolith_Peak_' + direction + '_fish'] = '' # Merged peak position\n",
    "    Data['Otolith_Width_' + direction + '_fish'] = '' # Merged peak position    \n",
    "    for plane in planes:\n",
    "        Data['Discard_' + direction + '_image_' + plane] = '' # Copy the general discard values to the ones we need for each image\n",
    "        Data['Otolith_Peak_'  + direction + '_' + plane] = ''\n",
    "        Data['Otolith_Width_' + direction + '_' + plane] = ''\n",
    "        Data['Grayvalues_'    + direction + '_' + plane] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def otolither_region(whichone, discard_front=None, bottom=1000, showregion=True, verbose=False):\n",
    "    '''\n",
    "    Modeled after algorithm for finding the enamel/dentin border for the tooth project (https://github.com/habi/zmk-tooth-cohort/blob/master/ToothAnalysis.ipynb), we\n",
    "    look for a change and bump in the gray values along different axis of the fishes.\n",
    "    This works out nicely to detect the approximate region of the otoliths.\n",
    "    '''\n",
    "    if verbose:\n",
    "        print('We try to find the otholith for the %s scan of fish %s' % (Data.Scan[whichone], Data.Fish[whichone]))\n",
    "    \n",
    "    # Discard some regions of the images for finding the otoliths\n",
    "    # frontally: contains the teeth\n",
    "    # back: can contain labels or dorsal fins\n",
    "    Data.at[whichone, 'Discard_Anteroposterior_fish'] = [Data.MIP_Lateral[whichone].shape[0] // 4,\n",
    "                                                         Data.MIP_Lateral[whichone].shape[0] - Data.MIP_Lateral[whichone].shape[0] // 5]\n",
    "    # laterally otoliths are in the middle of the fish    \n",
    "    Data.at[whichone, 'Discard_Lateral_fish'] = [Data.MIP_Dorsoventral[whichone].shape[1] // 8,\n",
    "                                            Data.MIP_Dorsoventral[whichone].shape[1] - Data.MIP_Dorsoventral[whichone].shape[1] // 8]\n",
    "    # bottom: a lot of the fish, no otolith\n",
    "    # top: often empty\n",
    "    Data.at[whichone, 'Discard_Dorsoventral_fish'] = [Data.MIP_Anteroposterior[whichone].shape[1] // 3,\n",
    "                                                 Data.MIP_Anteroposterior[whichone].shape[1] - Data.MIP_Anteroposterior[whichone].shape[1] // 8]\n",
    "    \n",
    "    # Copy the discard values to different planar directions for every image.\n",
    "    # This is *very* hacky, but makes it all run in one single loop\n",
    "    Data.at[whichone, 'Discard_Anteroposterior_image_horizontal'] = Data['Discard_Dorsoventral_fish'][whichone]\n",
    "    Data.at[whichone, 'Discard_Anteroposterior_image_vertical']   = Data['Discard_Lateral_fish'][whichone]\n",
    "    Data.at[whichone, 'Discard_Lateral_image_horizontal']         = Data['Discard_Dorsoventral_fish'][whichone]\n",
    "    Data.at[whichone, 'Discard_Lateral_image_vertical']           = Data['Discard_Anteroposterior_fish'][whichone]\n",
    "    Data.at[whichone, 'Discard_Dorsoventral_image_horizontal']    = Data['Discard_Lateral_fish'][whichone]\n",
    "    Data.at[whichone, 'Discard_Dorsoventral_image_vertical']      = Data['Discard_Anteroposterior_fish'][whichone]    \n",
    "\n",
    "    if verbose:\n",
    "        print('We discard the ventral %s and the dorsal %s slices of the fish' % (Data['Discard_Dorsoventral_fish'][whichone][0], Data['Discard_Dorsoventral_fish'][whichone][1]))\n",
    "        print('We discard the anterior %s and the posterior %s slices of the fish' % (Data['Discard_Anteroposterior_fish'][whichone][0], Data['Discard_Anteroposterior_fish'][whichone][1]))\n",
    "        print('We discard the fish laterally between slices %s and %s' % (Data['Discard_Lateral_fish'][whichone][0], Data['Discard_Lateral_fish'][whichone][1]))\n",
    "\n",
    "    for direction in directions:\n",
    "        for plane in planes:\n",
    "            if verbose:\n",
    "                print('Calculating Grayvalues_' + direction + '_' + plane)\n",
    "            if ('Lateral' in direction) and ('horizontal' in plane):\n",
    "                method = 'max'\n",
    "            else:\n",
    "                method = 'sum'\n",
    "            Data.at[whichone, 'Grayvalues_' + direction + '_' + plane] = get_grayvalues(Data['MIP_' + direction][whichone],\n",
    "                                                                                        plane=plane,\n",
    "                                                                                        which=method)            \n",
    "            # Our peak finder function returns peak position, peak value and peak width\n",
    "            # We only need position and width and don't save the value (for now)\n",
    "            if verbose:\n",
    "                print(80*'-')\n",
    "                print(direction, plane)\n",
    "                print('GV length', len(Data['Grayvalues_' + direction + '_' + plane][whichone]))\n",
    "                print('For %s/%s we want to discard %s' % (direction, plane, Data.at[whichone, 'Discard_' + direction + '_image_' + plane]))\n",
    "                print('MIP shape', Data['MIP_' + direction][whichone].shape)\n",
    "\n",
    "                print(80*'-')\n",
    "            peak, _, width = get_peak(Data['Grayvalues_' + direction + '_' + plane][whichone],\n",
    "                                      start=Data['Discard_'+ direction + '_image_' + plane][whichone][0],\n",
    "                                      stop=Data['Discard_'+ direction + '_image_' + plane][whichone][1],\n",
    "                                      frac=0.25,\n",
    "                                      height=0.25,\n",
    "                                      verbose=False)\n",
    "            Data.at[whichone, 'Otolith_Peak_' + direction + '_' + plane] = peak\n",
    "            Data.at[whichone, 'Otolith_Width_' + direction + '_' + plane] = width\n",
    "    # Calculate means to be on the safe side\n",
    "    Data.at[whichone,'Otolith_Peak_Anteroposterior_fish'] = int(round(numpy.mean((Data['Otolith_Peak_Dorsoventral_vertical'][whichone], Data['Otolith_Peak_Lateral_vertical'][whichone]))))\n",
    "    Data.at[whichone,'Otolith_Peak_Lateral_fish'] = int(round(numpy.mean((Data['Otolith_Peak_Anteroposterior_vertical'][whichone], Data['Otolith_Peak_Dorsoventral_horizontal'][whichone]))))\n",
    "    Data.at[whichone,'Otolith_Peak_Dorsoventral_fish'] = int(round(numpy.mean((Data['Otolith_Peak_Lateral_vertical'][whichone], Data['Otolith_Peak_Anteroposterior_horizontal'][whichone]))))            \n",
    "    Data.at[whichone,'Otolith_Width_Anteroposterior_fish'] = int(round(numpy.mean((Data['Otolith_Width_Dorsoventral_vertical'][whichone], Data['Otolith_Width_Lateral_vertical'][whichone]))))\n",
    "    Data.at[whichone,'Otolith_Width_Lateral_fish'] = int(round(numpy.mean((Data['Otolith_Width_Anteroposterior_vertical'][whichone], Data['Otolith_Width_Dorsoventral_horizontal'][whichone]))))\n",
    "    Data.at[whichone,'Otolith_Width_Dorsoventral_fish'] = int(round(numpy.mean((Data['Otolith_Width_Lateral_vertical'][whichone], Data['Otolith_Width_Anteroposterior_horizontal'][whichone]))))                \n",
    "    return()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whichone = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "otolither_region(whichone, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_otolith_position(whichone):\n",
    "    # Display everything\n",
    "    # Based on https://matplotlib.org/tutorials/intermediate/gridspec.html\n",
    "    fig = plt.figure(constrained_layout=True)\n",
    "    gs = fig.add_gridspec(2, 3)\n",
    "    for c, direction in enumerate(directions):\n",
    "        mip = fig.add_subplot(gs[0, c])\n",
    "        # Show image\n",
    "        plt.imshow(Data['MIP_' + direction][whichone])\n",
    "        \n",
    "        # # Show discarded regions\n",
    "        # if not 'Dorsoventral' in direction:\n",
    "        #     plt.fill_betweenx(range(Data['MIP_' + direction][whichone].shape[0]),\n",
    "        #                       0, Data['Discard_Dorsoventral_fish'][whichone][0],\n",
    "        #                       alpha=0.309, label='discarded up to %s' % Data['Discard_Dorsoventral_fish'][whichone][0],\n",
    "        #                       color=seaborn.color_palette()[2])\n",
    "        #     plt.fill_betweenx(range(Data['MIP_' + direction][whichone].shape[0]),\n",
    "        #                       Data['Discard_Dorsoventral_fish'][whichone][1], Data['MIP_' + direction][whichone].shape[1] - 1,\n",
    "        #                       alpha=0.309, label='discarded from %s' % Data['Discard_Dorsoventral_fish'][whichone][1],\n",
    "        #                       color=seaborn.color_palette()[2])            \n",
    "        # else:\n",
    "        #     plt.fill_betweenx(range(Data['MIP_' + direction][whichone].shape[0]),\n",
    "        #                       0, Data['Discard_Lateral_fish'][whichone][0],\n",
    "        #                       alpha=0.309, label='discarded up to %s' % Data['Discard_Lateral_fish'][whichone][0],\n",
    "        #                       color=seaborn.color_palette()[1])\n",
    "        #     plt.fill_betweenx(range(Data['MIP_' + direction][whichone].shape[0]),\n",
    "        #                       Data['Discard_Lateral_fish'][whichone][1], Data['MIP_' + direction][whichone].shape[1] - 1,\n",
    "        #                       alpha=0.309, label='discarded from %s' % Data['Discard_Lateral_fish'][whichone][1],\n",
    "        #                       color=seaborn.color_palette()[1])\n",
    "        # if 'Ante' not in direction:\n",
    "        #     plt.fill_between(range(Data['MIP_' + direction][whichone].shape[1]),\n",
    "        #                      0,\n",
    "        #                      Data['Discard_Anteroposterior_fish'][whichone][0],\n",
    "        #                      alpha=0.309, label='discarded from %s' % Data['Discard_Anteroposterior_fish'][whichone][0],\n",
    "        #                      color=seaborn.color_palette()[0])\n",
    "        #     plt.fill_between(range(Data['MIP_' + direction][whichone].shape[1]),\n",
    "        #                      Data['Discard_Anteroposterior_fish'][whichone][1],\n",
    "        #                      Data['MIP_' + direction][whichone].shape[0] - 1,\n",
    "        #                      alpha=0.309, label='discarded from %s' % Data['Discard_Anteroposterior_fish'][whichone][1],\n",
    "        #                      color=seaborn.color_palette()[0])\n",
    "        # else:\n",
    "        #     plt.fill_between(range(Data['MIP_' + direction][whichone].shape[1]),\n",
    "        #                      0,\n",
    "        #                      Data['Discard_Lateral_fish'][whichone][0],\n",
    "        #                      alpha=0.309, label='discarded from %s' % Data['Discard_Lateral_fish'][whichone][0],\n",
    "        #                      color=seaborn.color_palette()[1])\n",
    "        #     plt.fill_between(range(Data['MIP_' + direction][whichone].shape[1]),\n",
    "        #                      Data['Discard_Lateral_fish'][whichone][1],\n",
    "        #                      Data['MIP_' + direction][whichone].shape[0] - 1,\n",
    "        #                      alpha=0.309, label='discarded from %s' % Data['Discard_Lateral_fish'][whichone][1],\n",
    "        #                      color=seaborn.color_palette()[1])\n",
    "        \n",
    "        # *Very* verbose way of drawing the region we look at\n",
    "        # But since we have all the values, we can easily put them where we want\n",
    "        plt.gca().add_patch(matplotlib.patches.Rectangle((Data['Discard_' + direction + '_image_horizontal'][whichone][0],\n",
    "                                                          Data['Discard_' + direction + '_image_vertical'][whichone][0]),\n",
    "                                                         Data['MIP_' + direction][whichone].shape[1] - (Data['MIP_' + direction][whichone].shape[1] - Data['Discard_' + direction + '_image_horizontal'][whichone][1])-Data['Discard_' + direction + '_image_horizontal'][whichone][0],\n",
    "                                                         Data['MIP_' + direction][whichone].shape[0] - (Data['MIP_' + direction][whichone].shape[0] - Data['Discard_' + direction + '_image_vertical'][whichone][1])-Data['Discard_' + direction + '_image_vertical'][whichone][0],\n",
    "                                                         edgecolor=seaborn.color_palette()[c],\n",
    "                                                         facecolor='none',\n",
    "                                                         label='Region for detection'))\n",
    "        \n",
    "        # Plot gray values onto the image\n",
    "        # plt.plot(rescale_linear(Data['Grayvalues_' + direction + '_horizontal'][whichone], 0, Data['MIP_' + direction][whichone].shape[0] - 1),\n",
    "        #          label='horizontal', color='gray')\n",
    "        # Plot *only* the values we're interested in, i.e. discard start and end\n",
    "        plt.plot(range(Data['Discard_'+ direction + '_image_horizontal'][whichone][0],len(Data['Grayvalues_' + direction + '_horizontal'][whichone][:Data['Discard_'+ direction + '_image_horizontal'][whichone][1]])),\n",
    "                 rescale_linear(Data['Grayvalues_' + direction + '_horizontal'][whichone][Data['Discard_'+ direction + '_image_horizontal'][whichone][0]:Data['Discard_'+ direction + '_image_horizontal'][whichone][1]],\n",
    "                                Data['Discard_' + direction + '_image_vertical'][whichone][0],\n",
    "                                Data['Discard_' + direction + '_image_vertical'][whichone][1]),\n",
    "                 # label='horizontal',\n",
    "                 color='lightgray', alpha=0.618)        \n",
    "        plt.plot(range(Data['Discard_'+ direction + '_image_horizontal'][whichone][0],len(Data['Grayvalues_' + direction + '_horizontal'][whichone][:Data['Discard_'+ direction + '_image_horizontal'][whichone][1]])),\n",
    "                 rescale_linear(smoother(Data['Grayvalues_' + direction + '_horizontal'][whichone][Data['Discard_'+ direction + '_image_horizontal'][whichone][0]:Data['Discard_'+ direction + '_image_horizontal'][whichone][1]]),\n",
    "                                Data['Discard_' + direction + '_image_vertical'][whichone][0],\n",
    "                                Data['Discard_' + direction + '_image_vertical'][whichone][1]),\n",
    "                 # label='horizontal',\n",
    "                 color='white', alpha=0.618)        \n",
    "        plt.plot(rescale_linear(Data['Grayvalues_' + direction + '_vertical'][whichone][Data['Discard_'+ direction + '_image_vertical'][whichone][0]:Data['Discard_'+ direction + '_image_vertical'][whichone][1]],\n",
    "                                Data['Discard_' + direction + '_image_horizontal'][whichone][0],\n",
    "                                Data['Discard_' + direction + '_image_horizontal'][whichone][1]),\n",
    "                 range(Data['Discard_'+ direction + '_image_vertical'][whichone][0],len(Data['Grayvalues_' + direction + '_vertical'][whichone][:Data['Discard_'+ direction + '_image_vertical'][whichone][1]])),\n",
    "                 # label='vertical',\n",
    "                 color='lightgray', alpha=0.618)        \n",
    "        plt.plot(rescale_linear(smoother(Data['Grayvalues_' + direction + '_vertical'][whichone][Data['Discard_'+ direction + '_image_vertical'][whichone][0]:Data['Discard_'+ direction + '_image_vertical'][whichone][1]]),\n",
    "                                Data['Discard_' + direction + '_image_horizontal'][whichone][0],\n",
    "                                Data['Discard_' + direction + '_image_horizontal'][whichone][1]),\n",
    "                 range(Data['Discard_'+ direction + '_image_vertical'][whichone][0],len(Data['Grayvalues_' + direction + '_vertical'][whichone][:Data['Discard_'+ direction + '_image_vertical'][whichone][1]])),\n",
    "                 # label='vertical',\n",
    "                 color='white', alpha=0.618)        \n",
    "        \n",
    "        # # Show peaks\n",
    "        plt.axhline(Data['Otolith_Peak_' + direction + '_vertical'][whichone],\n",
    "                    label='DV Otolith @ %s' % Data['Otolith_Peak_' + direction + '_vertical'][whichone],\n",
    "                    color='red')\n",
    "        plt.axvline(Data['Otolith_Peak_' + direction + '_horizontal'][whichone],\n",
    "                    label='DV Otolith @ %s' % Data['Otolith_Peak_' + direction + '_horizontal'][whichone],\n",
    "                    color='green')\n",
    "        \n",
    "        # *Very* verbose way of drawing the otolith region on top\n",
    "        # But since we have all the values, we can easily put them where we want\n",
    "        plt.gca().add_patch(matplotlib.patches.Rectangle((Data['Otolith_Peak_' + direction + '_horizontal'][whichone] - Data['Otolith_Width_' + direction + '_horizontal'][whichone] // 2,\n",
    "                                                          Data['Otolith_Peak_' + direction + '_vertical'][whichone] - Data['Otolith_Width_' + direction + '_vertical'][whichone] // 2),\n",
    "                                                         Data['Otolith_Width_' + direction + '_horizontal'][whichone],\n",
    "                                                         Data['Otolith_Width_' + direction + '_vertical'][whichone],\n",
    "                                                         color=seaborn.color_palette()[c],\n",
    "                                                         alpha=0.618,\n",
    "                                                         label='Otolith region (%sx%s)' % (Data['Otolith_Width_' + direction + '_fish'][whichone],\n",
    "                                                                                           Data['Otolith_Width_' + direction + '_fish'][whichone])))\n",
    "    \n",
    "        # plt.legend(loc='lower left')\n",
    "        \n",
    "        plt.gca().add_artist(ScaleBar(Data['Voxelsize'][whichone], 'um', color=seaborn.color_palette()[c]))\n",
    "        plt.title('%s MIP\\nwith a size of %s x %s px' % (direction, Data['MIP_' + direction][whichone].shape[0], Data['MIP_' + direction][whichone].shape[1]))\n",
    "        mip = fig.add_subplot(gs[1, c])\n",
    "        plt.imshow(Reconstructions[whichone][Data['Otolith_Peak_Anteroposterior_fish'][whichone]-Data['Otolith_Width_Anteroposterior_fish'][whichone]//2:\n",
    "                                             Data['Otolith_Peak_Anteroposterior_fish'][whichone]+Data['Otolith_Width_Anteroposterior_fish'][whichone]//2,\n",
    "                                             Data['Otolith_Peak_Lateral_fish'][whichone]-Data['Otolith_Width_Lateral_fish'][whichone]//2:\n",
    "                                             Data['Otolith_Peak_Lateral_fish'][whichone]+Data['Otolith_Width_Lateral_fish'][whichone]//2,\n",
    "                                             Data['Otolith_Peak_Dorsoventral_fish'][whichone]-Data['Otolith_Width_Dorsoventral_fish'][whichone]//2:\n",
    "                                             Data['Otolith_Peak_Dorsoventral_fish'][whichone]+Data['Otolith_Width_Dorsoventral_fish'][whichone]//2,\n",
    "                                            ].max(axis=c))\n",
    "        plt.title('Extracted %s region' % (direction))\n",
    "    plt.show()\n",
    "    return()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_otolith_position(whichone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for direction in directions:\n",
    "#     for view in views:\n",
    "#         print(direction, view, 'peak', Data['Otolith_Peak_' + direction + '_' + view][whichone])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for direction in directions:\n",
    "#     for view in views:\n",
    "#         print(direction, view, 'width', Data['Otolith_Width_' + direction + '_' + view][whichone])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for whichone, row in tqdm(Data.iterrows(),\n",
    "                          desc='Extracting otolith regions',\n",
    "                          total=len(Data)):\n",
    "    otolither_region(whichone, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for whichone, row in tqdm(Data.iterrows(),\n",
    "                          desc='Displaying otolith regions',\n",
    "                          total=len(Data)):\n",
    "    display_otolith_position(whichone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save out otolith regions as .zarr files\n",
    "Data['OutputNameOtolithRegion'] = [os.path.join(os.path.dirname(f), 'otolith.region.zarr') for f in Data['Folder']]\n",
    "for whichone, row in tqdm(Data.iterrows(),\n",
    "                          desc='Extracting otolith regions to .zarr files',\n",
    "                          total=len(Data)):\n",
    "    if not os.path.exists(row['OutputNameOtolithRegion']):\n",
    "        Reconstructions[whichone][Data['Otolith_Peak_Anteroposterior_fish'][whichone]-Data['Otolith_Width_Anteroposterior_fish'][whichone]//2:\n",
    "                                  Data['Otolith_Peak_Anteroposterior_fish'][whichone]+Data['Otolith_Width_Anteroposterior_fish'][whichone]//2,\n",
    "                                  Data['Otolith_Peak_Lateral_fish'][whichone]-Data['Otolith_Width_Lateral_fish'][whichone]//2:\n",
    "                                  Data['Otolith_Peak_Lateral_fish'][whichone]+Data['Otolith_Width_Lateral_fish'][whichone]//2,\n",
    "                                  Data['Otolith_Peak_Dorsoventral_fish'][whichone]-Data['Otolith_Width_Dorsoventral_fish'][whichone]//2:\n",
    "                                  Data['Otolith_Peak_Dorsoventral_fish'][whichone]+Data['Otolith_Width_Dorsoventral_fish'][whichone]//2,\n",
    "                                 ].to_zarr(row['OutputNameOtolithRegion'],\n",
    "                                           overwrite=True,\n",
    "                                           compressor=Blosc(cname='zstd',\n",
    "                                                            shuffle=Blosc.BITSHUFFLE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the otoliths in again\n",
    "Otoliths = [dask.array.from_zarr(file) for file in Data['OutputNameOtolithRegion']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asdfasdfasdf=="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whichone=12\n",
    "o = Otoliths[whichone]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Otoliths[whichone].max().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Otoliths[whichone].mean().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Otoliths[whichone].min().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = o[:,:,750].compute()\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histogramh,b = dask.array.histogram(o, bins=2**8, range=(0,2**8))\n",
    "plt.semilogy(h)\n",
    "# Caldulate multiotsu threshold and show them\n",
    "for t in skimage.filters.threshold_multiotsu(o.compute(), classes=5):\n",
    "    plt.axvline(t, label=t, c='red')\n",
    "plt.xlim([0,2**8])\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segmentor(image, peaks, verbose=False):\n",
    "    # https://scikit-image.org/docs/dev/auto_examples/segmentation/plot_random_walker_segmentation.html#sphx-glr-auto-examples-segmentation-plot-random-walker-segmentation-py\n",
    "    # Set the background/discarded pixels to -1, 'stuff to be segmented' 0 and markers to 1\n",
    "    # the details given on https://scipy-lectures.org/packages/scikit-image/index.html?highlight=random%20walker#random-walker-segmentation\n",
    "    # Set everything to unlabeled\n",
    "    if not len(peaks):\n",
    "        print('Also give us some peaks to work with')\n",
    "        print('skimage.filters.threshold_multiotsu(STACK.compute(), classes=5) is a good start')\n",
    "        return()\n",
    "    markers = numpy.zeros_like(image, dtype='uint8')\n",
    "    # Set everything below background to be discarded.\n",
    "    # This is the same as above\n",
    "    if verbose:\n",
    "        print('Discarding everything below %s' % peaks[1])\n",
    "    markers[image < peaks[1]] = -1\n",
    "    if verbose:\n",
    "        print('Setting everything between %s and %s to 1' % (peaks[1], peaks[2]))\n",
    "    markers[(image > peaks[1]) & (image < peaks[2])] = 1\n",
    "    if verbose:\n",
    "        print('Setting everything above %s to 2' % peaks[3])\n",
    "    markers[image > peaks[3]] = 2\n",
    "    # Do the segmentation now\n",
    "    labels = random_walker(image.astype('uint8'), markers, copy=False)\n",
    "    if verbose:\n",
    "        # markers = markers.compute()\n",
    "        plt.subplot(2, 3, 1)\n",
    "        plt.imshow(image)\n",
    "        # plt.imshow(dask.array.ma.masked_where(-1, markers),\n",
    "        #            cmap='viridis',\n",
    "        #            alpha=0.5)\n",
    "        plt.title('Original')\n",
    "        plt.axis('off')      \n",
    "        plt.subplot(2, 3, 2)\n",
    "        plt.imshow(markers)\n",
    "        plt.title('Markers')\n",
    "        plt.axis('off')\n",
    "        plt.subplot(2, 3, 3)\n",
    "        plt.imshow(labels)\n",
    "        plt.title('Labels')        \n",
    "        plt.axis('off')            \n",
    "        for c, value in enumerate(numpy.unique(markers)):\n",
    "            plt.subplot(2, len(numpy.unique(markers)), len(numpy.unique(markers)) + c + 1)\n",
    "            plt.imshow(image)\n",
    "            plt.imshow(dask.array.ma.masked_not_equal(markers, value),\n",
    "                       cmap='viridis_r')\n",
    "            plt.title('%s=%s' % (c, value))\n",
    "            plt.axis('off')\n",
    "        plt.show()\n",
    "    # Return labeled image as an interpolated 8bit image\n",
    "    print(numpy.unique(markers))    \n",
    "    print(numpy.unique(labels))\n",
    "    return(numpy.interp(labels, (labels.min(), labels.max()), (labels.min(), 255)).astype('uint8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# peaks=skimage.filters.threshold_multiotsu(o.compute(), classes=5)\n",
    "print(peaks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = segmentor(o[:,:,800].compute(),\n",
    "                 peaks,\n",
    "                 verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy.unique(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(o[:,:,800])\n",
    "plt.imshow(mask,\n",
    "           alpha=0.309,\n",
    "           cmap='viridis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asdfasdfasdfasdf=="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask.shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy.unique(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View one otolith in 3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whichone = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsample = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# threshold = threshold(Otoliths[whichone], verbose=False)\n",
    "# histogram = histogram(Otoliths[whichone])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.semilogy(histogram)\n",
    "plt.axvline(threshold, label='Threshold @%s' % threshold)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load into K3D\n",
    "# plt_volume = k3d.volume(Otoliths[whichone][::subsample,::subsample,::subsample].astype(numpy.float16), color_range=[threshold,Otoliths[whichone].max().compute()])\n",
    "plt_volume = k3d.volume((Otoliths[whichone][::subsample,::subsample,::subsample]>peaks[-1]).astype(numpy.float16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the otolith\n",
    "plot = k3d.plot()\n",
    "plot += plt_volume\n",
    "plot.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asdfasdfsda="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot5 = k3d.get_plot()\n",
    "# plot5.snapshot_type = 'inline'\n",
    "# plot5.display()\n",
    "\n",
    "# data = plot5.get_snapshot()\n",
    "\n",
    "# with open('snapshot_inline.html', 'w') as f:\n",
    "#     f.write(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c, row in Data.iterrows():\n",
    "    print(row.Fish, row.Folder)\n",
    "    otolither_region(c, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whichone = 5\n",
    "out = Reconstructions[5][716:1009+716-500,300:-300,800:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(out.max(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a column for saving the otolith positions\n",
    "for d, direction in enumerate(directions):\n",
    "    Data['Otholith_Positions_' + direction] = ''\n",
    "    Data['Otholith_Position_Mean_' + direction] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect otolith positions\n",
    "for c, row in Data.iterrows():\n",
    "    print('Finding otolith position for %s/%s' % (row.Fish, row.Scan))\n",
    "    for d, direction in enumerate(directions):\n",
    "        Data.at[c, 'Otholith_Positions_' + direction] = otolither(row['MIP_' + direction], sigma=11, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save us the mean position\n",
    "for d, direction in enumerate(directions):\n",
    "    Data['Otholith_Position_Mean_' + direction] = [(numpy.mean(op[0]),\n",
    "                                                    numpy.mean(op[1])) for op in Data['Otholith_Positions_' + direction]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in Data['Otholith_Positions_Lateral']:\n",
    "    print(round(numpy.mean(i[0])),\n",
    "          round(numpy.mean(i[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in Data['Otholith_Position_Mean_Lateral']:\n",
    "    print(round(i[0]), round(i[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for whichone in range(len(Data)):\n",
    "    print(whichone, os.path.join(Data.Fish[whichone], Data.Scan[whichone]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "direction = 'Lateral'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c, row in Data.iterrows():\n",
    "    for d, direction in enumerate(directions):\n",
    "        plt.subplot(1, 3, d + 1)\n",
    "        plt.imshow(row['MIP_' + direction])\n",
    "        plt.title([round(i) for i in row['Otholith_Position_Mean_' + direction]])\n",
    "        plt.axhline(row['Otholith_Position_Mean_' + direction][1])\n",
    "        plt.axvline(row['Otholith_Position_Mean_' + direction][0])\n",
    "        plt.suptitle('%s/%s' % (row.Fish, row.Scan))\n",
    "        plt.gca().add_artist(ScaleBar(row.Voxelsize, 'um'))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect otolith position by looking for maximum gray value along fish\n",
    "for d, direction in enumerate(directions):\n",
    "    Data['GrayValueAlong_' + direction] = ''\n",
    "    Data['Otolith_MIP_Position_' + direction] = ''\n",
    "for whichone, row in Data.iterrows():\n",
    "    for d, direction in enumerate(directions):\n",
    "        # Calculate gray value sum along fish.\n",
    "        Data.at[whichone, 'GrayValueAlong_' + direction] = dask.array.sum(Data['MIP_' + direction][whichone],\n",
    "                                                                          axis=1)\n",
    "        # Maximum of this shoud give us the otolith position\n",
    "        Data.at[whichone,\n",
    "                'Otolith_MIP_Position_' + direction] = dask.array.argmax(dask.array.sum(Data['MIP_' + direction][whichone],\n",
    "                                                                                        axis=1))\n",
    "        # Plot what we found\n",
    "        plt.subplot(1, 3, d + 1)\n",
    "        plt.imshow(Data['MIP_' + direction][whichone])\n",
    "        # Plot the *rescaled* values over the image\n",
    "        plt.plot(rescale_linear(Data['GrayValueAlong_' + direction][whichone],\n",
    "                                100,\n",
    "                                Data['MIP_' + direction][whichone].shape[1] - 100),\n",
    "                 range(len(Data['GrayValueAlong_' + direction][whichone])),\n",
    "                 label='Normalized gray value sum along fish',\n",
    "                 color=seaborn.color_palette()[0])\n",
    "        plt.axhline(Data['Otolith_MIP_Position_' + direction][whichone],\n",
    "                    label='Max@%s' % Data['Otolith_MIP_Position_' + direction][whichone].compute(),\n",
    "                    color=seaborn.color_palette()[1])\n",
    "        plt.title('%s MIP' % direction)\n",
    "        plt.suptitle('%s/%s: MIPs of %s/%s' % (whichone, len(Data), Data.Fish[whichone], Data.Scan[whichone]))\n",
    "        plt.legend(loc='lower center')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c, row in Data.iterrows():\n",
    "    print(c, len(Data), os.path.join(row.Fish, row.Scan))\n",
    "    print('\\t Otolith from MIP',\n",
    "          row['Otolith_MIP_Position_Anteroposterior'].compute(),\n",
    "          row['Otolith_MIP_Position_Lateral'].compute(),\n",
    "          row['Otolith_MIP_Position_Dorsoventral'].compute())\n",
    "    print('\\t Otolith from otholither function',\n",
    "          row['Otholith_Position_Mean_Anteroposterior'],\n",
    "          row['Otholith_Position_Mean_Lateral'],\n",
    "          row['Otholith_Position_Mean_Dorsoventral'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whichone = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c, direction in enumerate(directions):\n",
    "    plt.subplot(1, 3, c + 1)\n",
    "    plt.imshow(Data['MIP_' + direction][whichone])\n",
    "    # From otholither function\n",
    "    plt.axhline(Data['Otholith_Position_Mean_' + direction][whichone][1],\n",
    "                label='otholither mean position 1: %s' % round(Data['Otholith_Position_Mean_' + direction][whichone][1]),\n",
    "                color=seaborn.color_palette()[0])\n",
    "    plt.axvline(Data['Otholith_Position_Mean_' + direction][whichone][0],\n",
    "                label='otholither mean posistion 0: %s' % round(Data['Otholith_Position_Mean_' + direction][whichone][0]),\n",
    "                color=seaborn.color_palette()[1])\n",
    "    # From sum along axis\n",
    "    plt.axhline(Data['Otolith_MIP_Position_' + direction][whichone],\n",
    "                label='MIP sum: %s' % Data['Otolith_MIP_Position_' + direction][whichone].compute(),\n",
    "                color=seaborn.color_palette()[3])\n",
    "    plt.legend(loc='lower center')\n",
    "    plt.title(direction)\n",
    "    plt.suptitle('%s/%s' % (Data.Fish[whichone], Data.Scan[whichone]))\n",
    "    plt.gca().add_artist(ScaleBar(Data.Voxelsize[whichone], 'um'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for direction in directions:\n",
    "    print(direction, Data['MIP_' + direction][whichone].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for direction in directions:\n",
    "    print(direction, Data['Otholith_Position_Mean_' + direction][whichone])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for direction in directions:\n",
    "    print(direction, round(Data['Otolith_MIP_Position_' + direction][whichone].compute()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CAN WE CALCULATE BOTH DV and THE LT POSITION ON THE AP MIP?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get us positions of otolith in relation to original data\n",
    "position_ap = numpy.mean((Data['Otholith_Position_Mean_Lateral'][whichone][1],\n",
    "                          Data['Otholith_Position_Mean_Dorsoventral'][whichone][1],\n",
    "                          Data['Otolith_MIP_Position_Lateral'][whichone],\n",
    "                          Data['Otolith_MIP_Position_Dorsoventral'][whichone]),\n",
    "                         dtype='int')\n",
    "# laterally, we assume the center of the image for now\n",
    "# position_lt = numpy.mean((Data['Otholith_Position_Mean_' + direction][whichone][1],\n",
    "#                           Data['Otolith_MIP_Position_' + direction][whichone]),dtype='int')\n",
    "position_lt = Data.MIP_Dorsoventral[whichone].shape[1] // 2\n",
    "position_dv = numpy.mean((Data['Otholith_Position_Mean_Anteroposterior'][whichone][0],\n",
    "                          Data['Otholith_Position_Mean_Lateral'][whichone][0]),\n",
    "                         dtype='int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(position_ap)\n",
    "print(position_lt)\n",
    "print(position_dv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slicethickness = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c, direction in enumerate(directions):\n",
    "    plt.subplot(1, 3, c + 1)\n",
    "    plt.imshow(Data['MIP_' + direction][whichone])\n",
    "    if c == 0:\n",
    "        plt.fill_between(range(Data['MIP_' + direction][whichone].shape[1]),\n",
    "                         position_lt - slicethickness // 2,\n",
    "                         position_lt + slicethickness // 2,\n",
    "                         alpha=0.5)\n",
    "        plt.fill_between(range(position_dv - slicethickness // 2, position_dv + slicethickness // 2),\n",
    "                         1,\n",
    "                         Data['MIP_' + direction][whichone].shape[0] - 1,\n",
    "                         alpha=0.5)\n",
    "    elif c == 1:\n",
    "        plt.fill_between(range(Data['MIP_' + direction][whichone].shape[1]),\n",
    "                         position_ap - slicethickness // 2,\n",
    "                         position_ap + slicethickness // 2,\n",
    "                         alpha=0.5)\n",
    "        plt.fill_between(range(position_dv - slicethickness // 2, position_dv + slicethickness // 2),\n",
    "                         1,\n",
    "                         Data['MIP_' + direction][whichone].shape[0] - 1,\n",
    "                         alpha=0.5)\n",
    "    else:\n",
    "        plt.fill_between(range(Data['MIP_' + direction][whichone].shape[1]),\n",
    "                         position_ap - slicethickness // 2,\n",
    "                         position_ap + slicethickness // 2,\n",
    "                         alpha=0.5)\n",
    "        plt.fill_between(range(position_lt - slicethickness // 2, position_lt + slicethickness // 2),\n",
    "                         1,\n",
    "                         Data['MIP_' + direction][whichone].shape[0] - 1,\n",
    "                         alpha=0.5)\n",
    "    plt.title(direction)\n",
    "    plt.suptitle('%s/%s' % (Data.Fish[whichone], Data.Scan[whichone]))\n",
    "    plt.gca().add_artist(ScaleBar(Data.Voxelsize[whichone], 'um'))\n",
    "    \n",
    "outfilepath = os.path.join(os.path.dirname(Data['Folder'][whichone]),\n",
    "                           '%s.%s.Otolither.png' % (Data['Fish'][whichone], Data['Scan'][whichone]))\n",
    "if not os.path.exists(outfilepath):\n",
    "    plt.savefig(outfilepath,\n",
    "                oarent=True,\n",
    "                bbox_inches='tight')\n",
    "    print('Figure saved to %s' % outfilepath)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(position_ap)\n",
    "print(position_lt)\n",
    "print(position_dv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab region calculated above from reconstructions\n",
    "otolithregion = Reconstructions[whichone][position_ap - slicethickness // 2:position_ap + slicethickness // 2,\n",
    "                                          position_lt - slicethickness // 2:position_lt + slicethickness // 2,\n",
    "                                          position_dv - slicethickness // 2:position_dv + slicethickness // 2\n",
    "                                         ]\n",
    "for ax in range(3):\n",
    "    plt.subplot(1, 3, ax + 1)\n",
    "    plt.imshow(dask.array.max(otolithregion, axis=ax))\n",
    "    plt.suptitle('%s/%s: MIP from AP %s:%s, LT %s:%s, DV %s:%s' % (Data.Fish[whichone],\n",
    "                                                                   Data.Scan[whichone],\n",
    "                                                                   position_ap - slicethickness // 2, position_ap + slicethickness // 2,\n",
    "                                                                   position_lt - slicethickness // 2, position_lt + slicethickness // 2,\n",
    "                                                                   position_dv - slicethickness // 2, position_dv + slicethickness // 2))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold(otolithregion.compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold(otolithregion[otolithregion > 42].compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(otolithregion[slicethickness//2])\n",
    "plt.imshow(dask.array.ma.masked_equal(otolithregion[slicethickness//2] > 102,0), cmap='viridis', alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file = open('largest', 'wb')\n",
    "# pickle.dump(largest,file)\n",
    "# file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file = open('largest', 'rb')\n",
    "# largest = pickle.load(file)\n",
    "# file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# largest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Make file smaller for testing reasons\n",
    "# subsample = 4\n",
    "# largest_smaller = largest[::subsample, ::subsample, ::subsample]\n",
    "# largest_smaller.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsample = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vmin = threshold(otolithregion.compute())\n",
    "print(vmin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load into K3D\n",
    "plt_volume = k3d.volume(dask.array.ma.masked_less(otolithregion,102)[::subsample, ::subsample, ::subsample].astype(numpy.float16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the otolith\n",
    "plot = k3d.plot()\n",
    "plot += plt_volume\n",
    "plot.display()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot5 = k3d.get_plot()\n",
    "# plot5.snapshot_type = 'inline'\n",
    "# plot5.display()\n",
    "\n",
    "# data = plot5.get_snapshot()\n",
    "\n",
    "# with open('snapshot_inline.html', 'w') as f:\n",
    "#     f.write(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
