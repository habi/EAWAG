{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fiddle with the EAWAG scans\n",
    "Look at the orientation and see if we can do some cropping based on landmarks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "import os\n",
    "import glob\n",
    "import pandas\n",
    "import imageio\n",
    "import numpy\n",
    "import scipy\n",
    "import k3d\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib_scalebar.scalebar import ScaleBar\n",
    "import seaborn\n",
    "import dask\n",
    "import dask_image.imread\n",
    "import skimage\n",
    "from tqdm.auto import tqdm, trange\n",
    "import math\n",
    "from numcodecs import Blosc\n",
    "from skimage.segmentation import random_walker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set dask temporary folder\n",
    "# Do this before creating a client: https://stackoverflow.com/a/62804525/323100\n",
    "import tempfile\n",
    "if 'Linux' in platform.system():\n",
    "    # Check if me mounted the FastSSD, otherwise go to standard tmp file\n",
    "    if os.path.exists(os.path.join(os.sep, 'media', 'habi', 'Fast_SSD')):\n",
    "        tmp = os.path.join(os.sep, 'media', 'habi', 'Fast_SSD', 'tmp')\n",
    "    else:\n",
    "        tmp = tempfile.gettempdir()\n",
    "elif 'Darwin' in platform.system():\n",
    "    tmp = tempfile.gettempdir()\n",
    "else:\n",
    "    if 'anaklin' in platform.node():\n",
    "        tmp = os.path.join('F:\\\\tmp')\n",
    "    else:\n",
    "        tmp = os.path.join('D:\\\\tmp')\n",
    "dask.config.set({'temporary_directory': tmp})\n",
    "print('Dask temporary files go to %s' % dask.config.get('temporary_directory'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "client = Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('You can seee what DASK is doing at \"http://localhost:%s/status\"' % client.scheduler_info()['services']['dashboard'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Ignore warnings in the notebook\n",
    "# import warnings\n",
    "# warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seaborn theme\n",
    "seaborn.set_theme(context='notebook', style='ticks')\n",
    "# Set up figure defaults\n",
    "plt.rc('image', cmap='gray', interpolation='nearest')  # Display all images in b&w and with 'nearest' interpolation\n",
    "plt.rcParams['figure.figsize'] = (16, 9)  # Size up figures a bit\n",
    "plt.rcParams['figure.dpi'] = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup scale bar defaults\n",
    "plt.rcParams['scalebar.location'] = 'lower right'\n",
    "plt.rcParams['scalebar.frameon'] = False\n",
    "plt.rcParams['scalebar.color'] = 'white'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display all plots identically\n",
    "lines = 3\n",
    "# And then do something like\n",
    "# plt.subplot(lines, int(numpy.ceil(len(Data) / float(lines))), c + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different locations if running either on Linux or Windows\n",
    "FastSSD = True\n",
    "overthere = False  # Load the data directly from the iee-research_storage drive\n",
    "nanoct = True  # Load the data directly from the 2214\n",
    "# to speed things up significantly\n",
    "if 'Linux' in platform.system():\n",
    "    if FastSSD:\n",
    "        BasePath = os.path.join(os.sep, 'media', 'habi', 'Fast_SSD')\n",
    "    elif overthere:\n",
    "        BasePath = os.path.join(os.sep, 'home', 'habi', 'research-storage-iee')\n",
    "    elif nanoct:\n",
    "        BasePath = os.path.join(os.path.sep, 'home', 'habi', '2214')\n",
    "    else:\n",
    "        BasePath = os.path.join(os.sep, 'home', 'habi', '1272')\n",
    "elif 'Darwin' in platform.system():\n",
    "    FastSSD = False\n",
    "    BasePath = os.path.join('/Users/habi/Dev/EAWAG/Data')\n",
    "elif 'Windows' in platform.system():\n",
    "    if FastSSD:\n",
    "        BasePath = os.path.join('F:\\\\')\n",
    "    else:\n",
    "        if 'anaklin' in platform.node():\n",
    "            BasePath = os.path.join('S:\\\\')\n",
    "        else:\n",
    "            BasePath = os.path.join('D:\\\\Results')\n",
    "if not overthere:\n",
    "    Root = os.path.join(BasePath, 'EAWAG')\n",
    "else:\n",
    "    Root = BasePath\n",
    "# if overthere:\n",
    "#         Root = os.path.join('I:\\\\microCTupload')\n",
    "print('We are loading all the data from %s' % Root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pixelsize(logfile):\n",
    "    \"\"\"Get the pixel size from the scan log file\"\"\"\n",
    "    with open(logfile, 'r') as f:\n",
    "        for line in f:\n",
    "            if 'Image Pixel' in line and 'Scaled' not in line:\n",
    "                pixelsize = float(line.split('=')[1])\n",
    "    return(pixelsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_git_hash():\n",
    "    '''\n",
    "    Get the current git hash from the repository.\n",
    "    Based on http://stackoverflow.com/a/949391/323100 and\n",
    "    http://stackoverflow.com/a/18283905/323100\n",
    "    '''\n",
    "    from subprocess import Popen, PIPE\n",
    "    import os\n",
    "    gitprocess = Popen(['git',\n",
    "                        '--git-dir',\n",
    "                        os.path.join(os.getcwd(), '.git'),\n",
    "                        'rev-parse',\n",
    "                        '--short',\n",
    "                        '--verify',\n",
    "                        'HEAD'],\n",
    "                       stdout=PIPE)\n",
    "    (output, _) = gitprocess.communicate()\n",
    "    return output.strip().decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Make directory for output\n",
    "# OutPutDir = os.path.join(os.getcwd(), 'Output', get_git_hash())\n",
    "# print('We are saving all the output to %s' % OutPutDir)\n",
    "# os.makedirs(OutPutDir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make us a dataframe for saving all that we need\n",
    "Data = pandas.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get *all* log files, unsorted but fast\n",
    "Data['LogFile'] = [os.path.join(root, name)\n",
    "                   for root, dirs, files in os.walk(Root)\n",
    "                   for name in files\n",
    "                   if name.endswith((\".log\"))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from zmk-tooth-cohort/ToothAnalysis.ipynb\n",
    "if not len(Data):\n",
    "    # Our dataframe is empty.\n",
    "    # We might be running on Binder, i.e. offer to download some sample data :)\n",
    "    import requests\n",
    "    # Inform user\n",
    "    print('You are most probably running the notebook on binder and thus do not have access to the data files.')\n",
    "    print('We are downloading and unpacking *one* cichlid head scan from https://osf.io/8vjxc/')\n",
    "    print('The data file is approximately 500 MB big, so this will take a while.')\n",
    "    # Change root folder\n",
    "    Root = 'data'\n",
    "    os.makedirs(Root, exist_ok=True)\n",
    "    # Download from osf.io\n",
    "    filelist = ['https://files.de-1.osf.io/v1/resources/8vjxc/providers/osfstorage/63d3bfa83c39ad005f30d024/?zip=']\n",
    "    fishlist = ['104016']\n",
    "    for c, file in enumerate(filelist):\n",
    "        os.makedirs(os.path.join(Root, fishlist[c]), exist_ok=True)\n",
    "        if os.path.exists(os.path.join(Root, fishlist[c], fishlist[c] + '.zip')):\n",
    "            print('ZIP file for %s was already downloaded' % fishlist[c])\n",
    "        else:\n",
    "            # Progress bar for download: https://github.com/tqdm/tqdm/blob/master/examples/tqdm_requests.py\n",
    "            # via https://stackoverflow.com/questions/43743438/using-tqdm-to-add-a-progress-bar-when-downloading-files/44920494#comment118687934_44920494\n",
    "            response = requests.get(filelist[c], stream=True)\n",
    "            with open(os.path.join(Root, fishlist[c], fishlist[c] + '.zip'), \"wb\") as fout:\n",
    "                with tqdm(unit='B',\n",
    "                          unit_scale=True,\n",
    "                          unit_divisor=1024,\n",
    "                          miniters=1,\n",
    "                          desc=fishlist[c] + '.zip',\n",
    "                          total=int(response.headers.get('content-length', 0))) as pbar:\n",
    "                    for chunk in response.iter_content(chunk_size=4096):\n",
    "                        fout.write(chunk)\n",
    "                        pbar.update(len(chunk))\n",
    "    # Unzip the files\n",
    "    import zipfile\n",
    "    ziplist = glob.glob(os.path.join(Root, '**', '*.zip'))\n",
    "    for downloaded_file in tqdm(ziplist,\n",
    "                                total=len(ziplist),\n",
    "                                desc='Unzipping %s files' % len(ziplist)):\n",
    "        with zipfile.ZipFile(downloaded_file, 'r') as zip_file:\n",
    "            # print(zip_file.namelist())\n",
    "            for filename in tqdm(iterable=zip_file.namelist(),\n",
    "                             desc=downloaded_file,\n",
    "                             total=len(zip_file.namelist()),\n",
    "                             leave=False):\n",
    "                if not os.path.exists(os.path.join(Root, fishlist[c], filename)):\n",
    "                    zip_file.extract(member=filename, path=os.path.join(Root, fishlist[c]))\n",
    "    # Search for log files again\n",
    "    Data['LogFile'] = [os.path.join(root, name)\n",
    "                       for root, dirs, files in os.walk(Root)\n",
    "                       for name in files\n",
    "                       if name.endswith((\".log\"))]                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('We found %s log files in %s' % (len(Data), Root))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limit *all* the data to only the 'head' scans\n",
    "Data = Data[Data['LogFile'].str.contains('head')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('We have %s log files with \"head\" in their name in %s' % ((len(Data)), Root))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate folder name\n",
    "Data['Folder'] = [os.path.dirname(f) for f in Data['LogFile']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get rid of all non-rec logfiles\n",
    "for c, row in Data.iterrows():\n",
    "    if 'rec' not in row.Folder:\n",
    "        Data.drop([c], inplace=True)\n",
    "    elif 'SubScan' in row.Folder:\n",
    "        Data.drop([c], inplace=True)\n",
    "    elif 'rectmp.log' in row.LogFile:\n",
    "        Data.drop([c], inplace=True)\n",
    "# Reset dataframe to something that we would get if we only would have loaded the 'rec' files\n",
    "Data = Data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate us some meaningful colums\n",
    "Data['Fish'] = [l[len(Root) + 1:].split(os.sep)[0] for l in Data['LogFile']]\n",
    "Data['Scan'] = ['.'.join(l[len(Root) + 1:].split(os.sep)[1:-1]) for l in Data['LogFile']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the file names of the reconstructions\n",
    "Data['Reconstructions'] = [[os.path.join(root, name)\n",
    "                            for root, dirs, files in os.walk(f)\n",
    "                            for name in files\n",
    "                            if 'rec0' in name and name.endswith((\".png\"))] for f in Data['Folder']]\n",
    "# Count how many files we have\n",
    "Data['Number of reconstructions'] = [len(r) for r in Data.Reconstructions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop samples which have either not been reconstructed yet or of which we deleted the reconstructions with\n",
    "# `find . -name \"*rec*.png\" -type f -mtime +333 -delete`\n",
    "# Based on https://stackoverflow.com/a/13851602\n",
    "# for c, row in Data.iterrows():\n",
    "#     if not row['Number of reconstructions']:\n",
    "#         print('%s contains no PNG files, we might be currently reconstructing it' % row.Folder)\n",
    "Data = Data[Data['Number of reconstructions'] > 0]\n",
    "Data.reset_index(drop=True, inplace=True)\n",
    "print('We have %s folders with reconstructions in %s' % ((len(Data)), Root))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get parameters we need from the log files\n",
    "Data['Voxelsize'] = [get_pixelsize(log) for log in Data['LogFile']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load all reconstructions into DASK arrays\n",
    "Reconstructions = [None] * len(Data)\n",
    "for c, row in tqdm(Data.iterrows(),\n",
    "                   desc='Load reconstructions',\n",
    "                   total=len(Data)):\n",
    "    Reconstructions[c] = dask_image.imread.imread(os.path.join(row['Folder'],\n",
    "                                                               '*rec*.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check if something went wrong\n",
    "# for file in Data['OutputNameRec']:\n",
    "#     print(file)\n",
    "#     dask.array.from_zarr(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How big are the datasets?\n",
    "Data['Size'] = [rec.shape for rec in Reconstructions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The three cardinal directions\n",
    "# Names adapted to fishes: https://en.wikipedia.org/wiki/Fish_anatomy#Body\n",
    "directions = ['Anteroposterior',\n",
    "              'Lateral',\n",
    "              'Dorsoventral']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in previously generated MIPs or calculate them\n",
    "for d, direction in enumerate(directions):\n",
    "    Data['MIP_' + direction] = ''\n",
    "for c, row in tqdm(Data.iterrows(), desc='Calculating MIPs', total=len(Data)):\n",
    "    for d, direction in tqdm(enumerate(directions),\n",
    "                             desc='%s/%s: %s' % (row['Fish'], row['Scan'], direction),\n",
    "                             leave=False,\n",
    "                             total=len(directions)):\n",
    "        outfilepath = os.path.join(os.path.dirname(row['Folder']),\n",
    "                                   '%s.%s.MIP.%s.png' % (row['Fish'], row['Scan'], direction))\n",
    "        if os.path.exists(outfilepath):\n",
    "            Data.at[c, 'MIP_' + direction] = dask_image.imread.imread(outfilepath).squeeze()\n",
    "        else:\n",
    "            # Generate MIP\n",
    "            Data.at[c, 'MIP_' + direction] = Reconstructions[c].max(axis=d).compute().squeeze()\n",
    "            # Save it out\n",
    "            imageio.imwrite(outfilepath, Data.at[c, 'MIP_' + direction].astype('uint8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect views\n",
    "for c, row in tqdm(Data.iterrows(),\n",
    "                   desc='Saving overview of MIP images',\n",
    "                   total=len(Data)):\n",
    "    outfilepath = os.path.join(os.path.dirname(row['Folder']),\n",
    "                               '%s.%s.MIPs.png' % (row['Fish'], row['Scan']))\n",
    "    if not os.path.exists(outfilepath):    \n",
    "        for d, direction in tqdm(enumerate(directions),\n",
    "                                 desc='%s/%s' % (row['Fish'], row['Scan']),\n",
    "                                 leave=False,\n",
    "                                 total=len(directions)):\n",
    "            plt.subplot(1, 3, d + 1)\n",
    "            plt.imshow(row['MIP_' + direction])\n",
    "            plt.gca().add_artist(ScaleBar(row['Voxelsize'],\n",
    "                                          'um'))\n",
    "            plt.title('%s MIP' % direction)\n",
    "            plt.axis('off')\n",
    "            plt.title('%s\\n%s MIP' % (os.path.join(row['Fish'], row['Scan']), direction))\n",
    "            plt.savefig(outfilepath,\n",
    "                        transparent=True,\n",
    "                        bbox_inches='tight')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The functions below were copied from Hearts-Melly/SubMyocardAnalysis.ipynb, in which we also look at the orientation of things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def threshold(image, verbose=False):\n",
    "    # Calculate threshold of image where image is non-zero\n",
    "    threshold = skimage.filters.threshold_otsu(image[image > 0])\n",
    "    if verbose:\n",
    "        plt.subplot(121)\n",
    "        plt.imshow(image)\n",
    "        plt.imshow(dask.array.ma.masked_equal(image > threshold, 0),\n",
    "                   alpha=0.618,\n",
    "                   cmap='viridis_r')\n",
    "        plt.subplot(122)\n",
    "        plt.semilogy(histogram(image), label='Log-Histogram')\n",
    "        plt.axvline(threshold, label='Otsu threshold: %s' % threshold)\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    return(threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the histogram of an image\n",
    "# We can safely assume to only use 8bit images\n",
    "def histogram(img):\n",
    "    histogram, bins = dask.array.histogram(dask.array.array(img),\n",
    "                                           bins=2**8,\n",
    "                                           range=[0, 2**8])\n",
    "    return(histogram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure out the otolith position on each of the directional views."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smoother(curve, frac=0.1):\n",
    "    ''' Smooth a curve '''\n",
    "    from statsmodels.nonparametric.smoothers_lowess import lowess\n",
    "    smoothed = lowess(curve, range(len(curve)), return_sorted=False, frac=frac)\n",
    "    return(smoothed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_minimum(curve, verbose=False):\n",
    "    '''\n",
    "    Function to detect get a 'border' based on the gray value along the image.\n",
    "    We do this by detecting the minimum of the derivative of the smoothed grayvalue curve\n",
    "    Based on https://stackoverflow.com/a/28541805/323100 and some manual tweaking\n",
    "    '''\n",
    "    smoothed = smoother(curve)\n",
    "    minimal_diff = numpy.argmin(numpy.diff(smoothed))\n",
    "    if verbose:\n",
    "        plt.plot(curve, alpha=0.6, label='Input curve')\n",
    "        plt.plot(smoothed, label='LOWESS smoothed')\n",
    "        plt.axvline(minimal_diff, c='r', label='Border')\n",
    "        plt.legend()\n",
    "    return(minimal_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_maximum(curve, verbose=False):\n",
    "    '''\n",
    "    Function to detect get a 'border' based on the gray value along the image.\n",
    "    We do this by detecting the minimum of the derivative of the smoothed grayvalue curve\n",
    "    Based on https://stackoverflow.com/a/28541805/323100 and some manual tweaking\n",
    "    '''\n",
    "    smoothed = smoother(curve)\n",
    "    maximal_diff = numpy.argmax(numpy.diff(smoothed))\n",
    "    if verbose:\n",
    "        plt.plot(curve, alpha=0.6, label='Input curve')\n",
    "        plt.plot(smoothed, label='LOWESS smoothed')\n",
    "        plt.axvline(maximal_diff, c='r', label='Border')\n",
    "        plt.legend()\n",
    "    return(maximal_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_peak(curve, start=None, stop=None, frac=0.25, height=0.25, verbose=False):\n",
    "    ''' Find a peak in the smoothed curve '''\n",
    "    # Peak finding from https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.find_peaks.html\n",
    "    # Mask a bit at the start and a bit start and end of curve, if desired\n",
    "    mask = dask.array.zeros_like(curve)\n",
    "    if start:\n",
    "        mask[:start] = 1\n",
    "    if stop:\n",
    "        mask[stop:] = 1\n",
    "    if start or stop:\n",
    "        original_curve = curve\n",
    "        smoothed = smoother(dask.array.ma.filled(dask.array.ma.masked_where(mask, curve), fill_value=0),\n",
    "                            frac=frac)\n",
    "    else:\n",
    "        smoothed = smoother(curve, frac=frac)\n",
    "    if verbose:\n",
    "        print('The input curve has a length of %s' % len(curve))\n",
    "        if start:\n",
    "            print('We discard the %s values from the start' % start)\n",
    "        if stop:\n",
    "            print('We discard the values from %s to the end' % stop)            \n",
    "        print('The input to the smoother has a length of %s' % (len(mask) - dask.array.count_nonzero(mask)).compute())\n",
    "    peak, _ = scipy.signal.find_peaks(smoothed, width=100)    \n",
    "    peak_value = dask.array.asarray(smoothed[peak])\n",
    "    # Peak width from https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.peak_widths.html\n",
    "    results_width = scipy.signal.peak_widths(smoothed, peak, rel_height=height)\n",
    "    if len(peak) > 1:\n",
    "        # Return only 'higher' peak if we have several\n",
    "        peak = dask.array.asarray(peak[dask.array.argmax(peak_value)])\n",
    "        results_width = [item[dask.array.argmax(peak_value)] for item in results_width]\n",
    "        peak_value = int(peak_value[dask.array.argmax(peak_value)])\n",
    "    # Get actual width\n",
    "    width = results_width[0]\n",
    "    # if start:\n",
    "    #     peak = peak + start\n",
    "    #     results_width = [v[0]+start for v in results_width]\n",
    "    if verbose:\n",
    "        if start or stop:\n",
    "            plt.plot(original_curve, alpha=0.618, label='Original')\n",
    "        plt.plot(dask.array.ma.masked_where(mask, curve), label='Input')\n",
    "        plt.plot(smoothed, label='Smoothed (frac=%s)' % frac)\n",
    "        plt.plot(peak,\n",
    "                 smoothed[peak],\n",
    "                 'x',\n",
    "                 color='C2',\n",
    "                 label='Peak@%s' % int(peak))\n",
    "        plt.hlines(*results_width[1:],\n",
    "                   color=\"C3\",\n",
    "                   label='Peak width at %d%%: %s' % (100*height, int(width)))\n",
    "        plt.legend()\n",
    "        # plt.xlim([0,len(curve)])\n",
    "        plt.show()\n",
    "\n",
    "    return(int(peak), int(peak_value), int(width))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put gray values of one fish in lists, so we can fiddle with them\n",
    "# gv =  [None] * len(directions) \n",
    "# gh =  [None] * len(directions) \n",
    "# for c, direction in enumerate(directions):\n",
    "#     gv[c] = Reconstructions[whichone].max(axis=c).sum(axis=0)\n",
    "#     gh[c] = Reconstructions[whichone].max(axis=c).sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One peak for testing\n",
    "# get_peak(gv[1], start=100, stop=1111, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rescale_linear(array, new_min, new_max):\n",
    "    \"\"\"Rescale an arrary linearly. From https://stackoverflow.com/a/50011743/323100\"\"\"\n",
    "    minimum, maximum = numpy.min(array), numpy.max(array)\n",
    "    m = (new_max - new_min) / (maximum - minimum)\n",
    "    b = new_min - m * minimum\n",
    "    return m * array + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_grayvalues(image, plane, which):\n",
    "    ''' get grayvalue along horizontal or vertical image plane '''\n",
    "    grayvalues = []\n",
    "    if plane == 'horizontal':\n",
    "        ax = 0\n",
    "    elif plane == 'vertical':\n",
    "        ax = 1\n",
    "    else:\n",
    "        print('No plane given, specify either \"plane=horizontal\" or \"plane=vertical\"')\n",
    "        print('Returning EMPTY grayvalues')        \n",
    "        return(grayvalues)\n",
    "    if which == 'sum':\n",
    "        grayvalues = image.sum(axis=ax)\n",
    "    elif which == 'max':\n",
    "        grayvalues = image.max(axis=ax)\n",
    "    else:\n",
    "        print('No method given, specify either \"which=max\" or \"which=sum\"')\n",
    "        print('Returning EMPTY grayvalues')\n",
    "        return(grayvalues)\n",
    "    return(grayvalues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dir in directions:\n",
    "    print(dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "planes = ['horizontal', 'vertical']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate empty columns to fill in the values we calculate below\n",
    "Data['Otolith_Sphere_Diameter_fish'] = '' # Merged peak position\n",
    "for c, direction in enumerate(directions):\n",
    "    Data['Discard_' + direction + '_fish'] = '' # General discard directions\n",
    "    Data['Otolith_Peak_' + direction + '_fish'] = '' # Merged peak position\n",
    "    Data['Otolith_Width_' + direction + '_fish'] = '' # Merged peak position\n",
    "    for plane in planes:\n",
    "        Data['Discard_' + direction + '_image_' + plane] = '' # Copy the general discard values to the ones we need for each image\n",
    "        Data['Otolith_Peak_'  + direction + '_' + plane] = ''\n",
    "        Data['Otolith_Width_' + direction + '_' + plane] = ''\n",
    "        Data['Grayvalues_'    + direction + '_' + plane] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def otolither_region(whichfish, discard_front=None, bottom=1000, showregion=True, verbose=False):\n",
    "    '''\n",
    "    Modeled after algorithm for finding the enamel/dentin border for the tooth project (https://github.com/habi/zmk-tooth-cohort/blob/master/ToothAnalysis.ipynb), we\n",
    "    look for a change and bump in the gray values along different axis of the fishes.\n",
    "    This works out nicely to detect the approximate region of the otoliths.\n",
    "    '''\n",
    "    if verbose:\n",
    "        print('We try to find the otolith for the %s scan of fish %s' % (Data.Scan[whichfish], Data.Fish[whichfish]))\n",
    "    \n",
    "    # Discard some regions of the images for finding the otoliths\n",
    "    # frontally: contains the teeth\n",
    "    # back: can contain labels or dorsal fins\n",
    "    Data.at[whichfish, 'Discard_Anteroposterior_fish'] = [round(Data.MIP_Lateral[whichfish].shape[0] / 5),\n",
    "                                                          round(Data.MIP_Lateral[whichfish].shape[0] - Data.MIP_Lateral[whichfish].shape[0] / 5)]\n",
    "    # laterally otoliths are in the middle of the fish    \n",
    "    Data.at[whichfish, 'Discard_Lateral_fish'] = [round(Data.MIP_Dorsoventral[whichfish].shape[1] / 8),\n",
    "                                                 round(Data.MIP_Dorsoventral[whichfish].shape[1] - Data.MIP_Dorsoventral[whichfish].shape[1] / 8)]\n",
    "    # bottom: a lot of the fish, no otolith\n",
    "    # top: often empty\n",
    "    Data.at[whichfish, 'Discard_Dorsoventral_fish'] = [round(Data.MIP_Anteroposterior[whichfish].shape[1] / 3),\n",
    "                                                      round(Data.MIP_Anteroposterior[whichfish].shape[1] - Data.MIP_Anteroposterior[whichfish].shape[1] / 10)]\n",
    "    \n",
    "    # Copy the discard values to different planar directions for every image.\n",
    "    # This is *very* hacky, but makes it all run in one single loop\n",
    "    Data.at[whichfish, 'Discard_Anteroposterior_image_horizontal'] = Data['Discard_Dorsoventral_fish'][whichfish]\n",
    "    Data.at[whichfish, 'Discard_Anteroposterior_image_vertical']   = Data['Discard_Lateral_fish'][whichfish]\n",
    "    Data.at[whichfish, 'Discard_Lateral_image_horizontal']         = Data['Discard_Dorsoventral_fish'][whichfish]\n",
    "    Data.at[whichfish, 'Discard_Lateral_image_vertical']           = Data['Discard_Anteroposterior_fish'][whichfish]\n",
    "    Data.at[whichfish, 'Discard_Dorsoventral_image_horizontal']    = Data['Discard_Lateral_fish'][whichfish]\n",
    "    Data.at[whichfish, 'Discard_Dorsoventral_image_vertical']      = Data['Discard_Anteroposterior_fish'][whichfish]    \n",
    "\n",
    "    if verbose:\n",
    "        print('We discard the ventral %s and the dorsal %s slices of the fish' % (Data['Discard_Dorsoventral_fish'][whichfish][0], Data['Discard_Dorsoventral_fish'][whichfish][1]))\n",
    "        print('We discard the anterior %s and the posterior %s slices of the fish' % (Data['Discard_Anteroposterior_fish'][whichfish][0], Data['Discard_Anteroposterior_fish'][whichfish][1]))\n",
    "        print('We discard the fish laterally between slices %s and %s' % (Data['Discard_Lateral_fish'][whichfish][0], Data['Discard_Lateral_fish'][whichfish][1]))\n",
    "\n",
    "    for direction in directions:\n",
    "        for plane in planes:\n",
    "            if verbose:\n",
    "                print('Calculating Grayvalues_' + direction + '_' + plane)\n",
    "            if ('Dorso' not in direction) and ('hor' in plane):\n",
    "                method = 'max'\n",
    "            else:\n",
    "                method = 'sum'\n",
    "            Data.at[whichfish, 'Grayvalues_' + direction + '_' + plane] = get_grayvalues(Data['MIP_' + direction][whichfish],\n",
    "                                                                                        plane=plane,\n",
    "                                                                                        which=method)            \n",
    "                                                                                        # which='sum')            \n",
    "            # Our peak finder function returns peak position, peak value and peak width\n",
    "            # We only need position and width and don't save the value (for now)\n",
    "            if verbose:\n",
    "                print(80*'-')\n",
    "                print(direction, plane)\n",
    "                print('GV length', len(Data['Grayvalues_' + direction + '_' + plane][whichfish]))\n",
    "                print('For %s/%s we want to discard %s' % (direction, plane, Data.at[whichfish, 'Discard_' + direction + '_image_' + plane]))\n",
    "                print('MIP shape', Data['MIP_' + direction][whichfish].shape)\n",
    "\n",
    "                print(80*'-')\n",
    "            height = 0.25\n",
    "            frac = 0.1\n",
    "            if 'vert' in plane:\n",
    "                height=0.333\n",
    "            # elif ('Ant' in direction) and ('hor' in plane):\n",
    "            #     frac=0.5\n",
    "            #     height=0.25                \n",
    "            # elif ('Lat' in direction) and ('vert' in plane):\n",
    "            #     height=0.2             \n",
    "            elif ('Dors' not in direction) and ('hor' in plane):\n",
    "                frac=0.5\n",
    "                height=0.333\n",
    "            # elif ('Ante' not in direction) and ('vert' in plane):\n",
    "            #     height=0.2\n",
    "            #     frac = 0.1                \n",
    "            elif ('Dors' in direction) and ('hor' in plane):\n",
    "                height=0.5                \n",
    "            # else:\n",
    "            #     pass\n",
    "            peak, _, width = get_peak(Data['Grayvalues_' + direction + '_' + plane][whichfish],\n",
    "                                      start=Data['Discard_'+ direction + '_image_' + plane][whichfish][0],\n",
    "                                      stop=Data['Discard_'+ direction + '_image_' + plane][whichfish][1],\n",
    "                                      frac=frac,\n",
    "                                      height=height,\n",
    "                                      verbose=verbose)\n",
    "            Data.at[whichfish, 'Otolith_Peak_' + direction + '_' + plane] = peak\n",
    "            Data.at[whichfish, 'Otolith_Width_' + direction + '_' + plane] = width\n",
    "            \n",
    "    # Calculate means found values, to be applied as cutout region\n",
    "    Data.at[whichfish,'Otolith_Peak_Anteroposterior_fish'] = int(round(numpy.mean((Data['Otolith_Peak_Dorsoventral_vertical'][whichfish],\n",
    "                                                                                  Data['Otolith_Peak_Lateral_vertical'][whichfish]))))\n",
    "    Data.at[whichfish,'Otolith_Width_Anteroposterior_fish'] = int(round(numpy.mean((Data['Otolith_Width_Dorsoventral_vertical'][whichfish],\n",
    "                                                                                  Data['Otolith_Width_Lateral_vertical'][whichfish]))))\n",
    "    Data.at[whichfish,'Otolith_Peak_Lateral_fish']         = int(round(numpy.mean((Data['Otolith_Peak_Dorsoventral_horizontal'][whichfish],\n",
    "                                                                                  Data['Otolith_Peak_Anteroposterior_vertical'][whichfish]))))\n",
    "    Data.at[whichfish,'Otolith_Width_Lateral_fish']         = int(round(numpy.mean((Data['Otolith_Width_Dorsoventral_horizontal'][whichfish],\n",
    "                                                                                  Data['Otolith_Width_Anteroposterior_vertical'][whichfish]))))\n",
    "    Data.at[whichfish,'Otolith_Peak_Dorsoventral_fish']    = int(round(numpy.mean((Data['Otolith_Peak_Anteroposterior_horizontal'][whichfish],\n",
    "                                                                                  Data['Otolith_Peak_Lateral_horizontal'][whichfish]))))            \n",
    "    Data.at[whichfish,'Otolith_Width_Dorsoventral_fish']    = int(round(numpy.mean((Data['Otolith_Width_Anteroposterior_horizontal'][whichfish],\n",
    "                                                                                  Data['Otolith_Width_Lateral_horizontal'][whichfish]))))                \n",
    "    Data.at[whichfish,'Otolith_Sphere_Diameter_fish'] = int(round(numpy.mean((Data['Otolith_Width_Anteroposterior_fish'][whichfish],\n",
    "                                                                             Data['Otolith_Width_Lateral_fish'][whichfish],\n",
    "                                                                             Data['Otolith_Width_Dorsoventral_fish'][whichfish]))))\n",
    "    # Safety-check of values\n",
    "    # peak-width/2 should not be negative or peak+width/2 should not be larger than image width\n",
    "    # \"I'm looking at you, 11729/head!\"\n",
    "    for direction in directions:\n",
    "        if Data['Otolith_Peak_' + direction + '_fish'][whichfish] - round(Data['Otolith_Width_' +  direction + '_fish'][whichfish] / 2) < 0:\n",
    "            print('%s peak of %s shifted by %s to make sure that we are not out of the fish' % (direction,\n",
    "                                                                                                os.path.join(Data['Fish'][whichfish], Data['Scan'][whichfish]),\n",
    "                                                                                                Data['Otolith_Peak_' + direction + '_fish'][whichfish] - round(Data['Otolith_Width_' +  direction + '_fish'][whichfish] / 2)))            \n",
    "            Data.at[whichfish, 'Otolith_Peak_' + direction + '_fish'] -= Data['Otolith_Peak_' + direction + '_fish'][whichfish] - round(Data['Otolith_Width_' +  direction + '_fish'][whichfish] / 2)\n",
    "    return()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search and use one specific fish\n",
    "whichone = Data[Data['Fish'] == '104016'].index[0]\n",
    "print('The fish we are looking for is item %s in our dataframe' % whichone)\n",
    "print('We are loading the extracted otolith region of %s' % os.path.join(Data.Fish[whichone], Data.Scan[whichone]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate otolith region for the fish we look for\n",
    "otolither_region(whichone, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_otolith_position(whichfish):\n",
    "    # Display everything\n",
    "    # Based on https://matplotlib.org/tutorials/intermediate/gridspec.html\n",
    "    fig = plt.figure(constrained_layout=True)\n",
    "    gs = fig.add_gridspec(2, 3)\n",
    "    for c, direction in enumerate(directions):\n",
    "        mip = fig.add_subplot(gs[0, c])\n",
    "        # Show image\n",
    "        plt.imshow(Data['MIP_' + direction][whichfish])\n",
    "        # *Very* verbose way of drawing the region we look at\n",
    "        # But since we have all the values, we can easily put them where we want\n",
    "        # Rectangle(xy, width, height)\n",
    "        plt.gca().add_patch(matplotlib.patches.Rectangle((Data['Discard_' + direction + '_image_horizontal'][whichfish][0],\n",
    "                                                          Data['Discard_' + direction + '_image_vertical'][whichfish][0]),\n",
    "                                                         Data['MIP_' + direction][whichfish].shape[1] - (Data['MIP_' + direction][whichfish].shape[1] - Data['Discard_' + direction + '_image_horizontal'][whichfish][1]) - Data['Discard_' + direction + '_image_horizontal'][whichfish][0],\n",
    "                                                         Data['MIP_' + direction][whichfish].shape[0] - (Data['MIP_' + direction][whichfish].shape[0] - Data['Discard_' + direction + '_image_vertical'][whichfish][1]) - Data['Discard_' + direction + '_image_vertical'][whichfish][0],\n",
    "                                                         edgecolor=seaborn.color_palette()[c],\n",
    "                                                         facecolor='none',\n",
    "                                                         label='Region for detection'))\n",
    "        \n",
    "        # Plot gray values onto the image\n",
    "        # plt.plot(rescale_linear(Data['Grayvalues_' + direction + '_horizontal'][whichfish], 0, Data['MIP_' + direction][whichfish].shape[0] - 1),\n",
    "        #          label='horizontal', color='gray')\n",
    "        # Plot *only* the values we're interested in, i.e. discard start and end\n",
    "        # plt.plot(range(Data['Discard_'+ direction + '_image_horizontal'][whichfish][0],len(Data['Grayvalues_' + direction + '_horizontal'][whichfish][:Data['Discard_'+ direction + '_image_horizontal'][whichfish][1]])),\n",
    "        #          rescale_linear(Data['Grayvalues_' + direction + '_horizontal'][whichfish][Data['Discard_'+ direction + '_image_horizontal'][whichfish][0]:Data['Discard_'+ direction + '_image_horizontal'][whichfish][1]],\n",
    "        #                         Data['Discard_' + direction + '_image_vertical'][whichfish][0],\n",
    "        #                         Data['Discard_' + direction + '_image_vertical'][whichfish][1]),\n",
    "        #          # label='horizontal',\n",
    "        #          color='lightgray', alpha=0.618)        \n",
    "        plt.plot(range(Data['Discard_'+ direction + '_image_horizontal'][whichfish][0],len(Data['Grayvalues_' + direction + '_horizontal'][whichfish][:Data['Discard_'+ direction + '_image_horizontal'][whichfish][1]])),\n",
    "                 rescale_linear(smoother(Data['Grayvalues_' + direction + '_horizontal'][whichfish][Data['Discard_'+ direction + '_image_horizontal'][whichfish][0]:Data['Discard_'+ direction + '_image_horizontal'][whichfish][1]]),\n",
    "                                Data['Discard_' + direction + '_image_vertical'][whichfish][0],\n",
    "                                Data['Discard_' + direction + '_image_vertical'][whichfish][1]),\n",
    "                 # label='horizontal',\n",
    "                 color='white', alpha=0.618)        \n",
    "        # plt.plot(rescale_linear(Data['Grayvalues_' + direction + '_vertical'][whichfish][Data['Discard_'+ direction + '_image_vertical'][whichfish][0]:Data['Discard_'+ direction + '_image_vertical'][whichfish][1]],\n",
    "        #                         Data['Discard_' + direction + '_image_horizontal'][whichfish][0],\n",
    "        #                         Data['Discard_' + direction + '_image_horizontal'][whichfish][1]),\n",
    "        #          range(Data['Discard_'+ direction + '_image_vertical'][whichfish][0],len(Data['Grayvalues_' + direction + '_vertical'][whichfish][:Data['Discard_'+ direction + '_image_vertical'][whichfish][1]])),\n",
    "        #          # label='vertical',\n",
    "        #          color='lightgray', alpha=0.618)        \n",
    "        plt.plot(rescale_linear(smoother(Data['Grayvalues_' + direction + '_vertical'][whichfish][Data['Discard_'+ direction + '_image_vertical'][whichfish][0]:Data['Discard_'+ direction + '_image_vertical'][whichfish][1]]),\n",
    "                                Data['Discard_' + direction + '_image_horizontal'][whichfish][0],\n",
    "                                Data['Discard_' + direction + '_image_horizontal'][whichfish][1]),\n",
    "                 range(Data['Discard_'+ direction + '_image_vertical'][whichfish][0],len(Data['Grayvalues_' + direction + '_vertical'][whichfish][:Data['Discard_'+ direction + '_image_vertical'][whichfish][1]])),\n",
    "                 # label='vertical',\n",
    "                 color='white', alpha=0.618)        \n",
    "        \n",
    "        # Show peaks from this direction\n",
    "        plt.axhline(Data['Otolith_Peak_' + direction + '_vertical'][whichfish],\n",
    "                    label='vertical %s @ %s' % (direction, Data['Otolith_Peak_' + direction + '_vertical'][whichfish]),\n",
    "                    color=seaborn.color_palette()[c],\n",
    "                    alpha=0.618)\n",
    "        plt.axvline(Data['Otolith_Peak_' + direction + '_horizontal'][whichfish],\n",
    "                    label='horizontal %s @ %s' % (direction, Data['Otolith_Peak_' + direction + '_horizontal'][whichfish]),\n",
    "                    color=seaborn.color_palette()[c],\n",
    "                    alpha=0.618)\n",
    "\n",
    "        # Show peaks from other directions and chosen peak\n",
    "        if 'Ante' in direction:\n",
    "            plt.axhline(Data['Otolith_Peak_Dorsoventral_horizontal'][whichfish],\n",
    "                        color=seaborn.color_palette()[2],\n",
    "                        alpha=0.618)         \n",
    "            plt.axvline(Data['Otolith_Peak_Lateral_horizontal'][whichfish],\n",
    "                        color=seaborn.color_palette()[1],\n",
    "                        alpha=0.618)\n",
    "            plt.axhline(Data['Otolith_Peak_Lateral_fish'][whichfish],\n",
    "                        label='Mean LT @ %s' % Data['Otolith_Peak_Lateral_fish'][whichfish],\n",
    "                        color='white')            \n",
    "            plt.axvline(Data['Otolith_Peak_Dorsoventral_fish'][whichfish],\n",
    "                        label='Mean DV @ %s' % Data['Otolith_Peak_Dorsoventral_fish'][whichfish],\n",
    "                        color='white')\n",
    "        elif 'Late' in direction:\n",
    "            plt.axhline(Data['Otolith_Peak_Dorsoventral_vertical'][whichfish],\n",
    "                        color=seaborn.color_palette()[2],\n",
    "                        alpha=0.618)\n",
    "            plt.axvline(Data['Otolith_Peak_Anteroposterior_horizontal'][whichfish],\n",
    "                        color=seaborn.color_palette()[0],\n",
    "                        alpha=0.618)\n",
    "            plt.axhline(Data['Otolith_Peak_Anteroposterior_fish'][whichfish],\n",
    "                        label='Mean AP @ %s' % Data['Otolith_Peak_Anteroposterior_fish'][whichfish],\n",
    "                        color='white')            \n",
    "            plt.axvline(Data['Otolith_Peak_Dorsoventral_fish'][whichfish],\n",
    "                        label='Mean DV @ %s' % Data['Otolith_Peak_Dorsoventral_fish'][whichfish],\n",
    "                        color='white')\n",
    "        else:\n",
    "            plt.axhline(Data['Otolith_Peak_Lateral_vertical'][whichfish],\n",
    "                        color=seaborn.color_palette()[1],\n",
    "                        alpha=0.618)\n",
    "            plt.axvline(Data['Otolith_Peak_Anteroposterior_vertical'][whichfish],\n",
    "                        color=seaborn.color_palette()[0],\n",
    "                        alpha=0.618)            \n",
    "            plt.axhline(Data['Otolith_Peak_Anteroposterior_fish'][whichfish],\n",
    "                        label='Mean AP @ %s' % Data['Otolith_Peak_Anteroposterior_fish'][whichfish],\n",
    "                        color='white')\n",
    "            plt.axvline(Data['Otolith_Peak_Lateral_fish'][whichfish],\n",
    "                        label='Mean LT @ %s' % Data['Otolith_Peak_Lateral_fish'][whichfish],\n",
    "                        color='white')            \n",
    "                \n",
    "        \n",
    "        # *Very* verbose way of drawing the otolith region on top\n",
    "        # But since we have all the values, we can easily put them where we want\n",
    "        if 'Ante' in direction:\n",
    "            ol = 'Otolith region (%sx%s)' % (Data['Otolith_Width_Dorsoventral_fish'][whichfish], Data['Otolith_Width_Lateral_fish'][whichfish])\n",
    "        elif 'Later' in direction:\n",
    "            ol = 'Otolith region (%sx%s)' % (Data['Otolith_Width_Dorsoventral_fish'][whichfish], Data['Otolith_Width_Anteroposterior_fish'][whichfish])\n",
    "        else:\n",
    "            ol = 'Otolith region (%sx%s)' % (Data['Otolith_Width_Lateral_fish'][whichfish], Data['Otolith_Width_Anteroposterior_fish'][whichfish])\n",
    "            \n",
    "        plt.gca().add_patch(matplotlib.patches.Rectangle((Data['Otolith_Peak_' + direction + '_horizontal'][whichfish] - round(Data['Otolith_Width_' + direction + '_horizontal'][whichfish] / 2),\n",
    "                                                          Data['Otolith_Peak_' + direction + '_vertical'][whichfish] - round(Data['Otolith_Width_' + direction + '_vertical'][whichfish] / 2)),\n",
    "                                                         Data['Otolith_Width_' + direction + '_horizontal'][whichfish],\n",
    "                                                         Data['Otolith_Width_' + direction + '_vertical'][whichfish],\n",
    "                                                         color=seaborn.color_palette()[c],\n",
    "                                                         alpha=0.618,\n",
    "                                                         label=ol))\n",
    "    \n",
    "        # plt.legend(loc='lower left')\n",
    "        \n",
    "        plt.gca().add_artist(ScaleBar(Data['Voxelsize'][whichfish], 'um', color=seaborn.color_palette()[c]))\n",
    "        plt.title('%s MIP\\nwith a size of %s x %s px' % (direction, Data['MIP_' + direction][whichfish].shape[0], Data['MIP_' + direction][whichfish].shape[1]))\n",
    "        mip = fig.add_subplot(gs[1, c])\n",
    "        OtolithRegion = Reconstructions[whichfish][Data['Otolith_Peak_Anteroposterior_fish'][whichfish] - round(Data['Otolith_Width_Anteroposterior_fish'][whichfish] / 2):\n",
    "                                                  Data['Otolith_Peak_Anteroposterior_fish'][whichfish] + round(Data['Otolith_Width_Anteroposterior_fish'][whichfish] / 2),\n",
    "                                                  Data['Otolith_Peak_Lateral_fish'][whichfish] - round(Data['Otolith_Width_Lateral_fish'][whichfish] / 2):\n",
    "                                                  Data['Otolith_Peak_Lateral_fish'][whichfish] + round(Data['Otolith_Width_Lateral_fish'][whichfish] / 2),\n",
    "                                                  Data['Otolith_Peak_Dorsoventral_fish'][whichfish] - round(Data['Otolith_Width_Dorsoventral_fish'][whichfish] / 2):\n",
    "                                                  Data['Otolith_Peak_Dorsoventral_fish'][whichfish] + round(Data['Otolith_Width_Dorsoventral_fish'][whichfish] / 2)\n",
    "                                                 ].compute()\n",
    "        plt.imshow(OtolithRegion.max(axis=c))\n",
    "        plt.gca().add_artist(ScaleBar(Data['Voxelsize'][whichfish], 'um', color=seaborn.color_palette()[c]))\n",
    "        plt.title('Extracted %s region %s' % (direction, OtolithRegion.shape))\n",
    "    outfilename = os.path.join(os.path.dirname(Data['Folder'][whichfish]),\n",
    "                               '%s.%s.Otolither.Position.png' % (Data['Fish'][whichfish], Data['Scan'][whichfish]))\n",
    "    plt.savefig(outfilename,\n",
    "                transparent=True,\n",
    "                bbox_inches='tight')\n",
    "    plt.show()\n",
    "    return()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and display otolith for one fish\n",
    "display_otolith_position(whichone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate otolith region for all fishes\n",
    "for fishnumber, row in tqdm(Data.iterrows(),\n",
    "                           desc='Extracting otolith regions',\n",
    "                           total=len(Data)):\n",
    "    otolither_region(fishnumber, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# items = 33\n",
    "# for fishnumber, row in tqdm(Data.iterrows(),\n",
    "#                           desc='Displaying otolith regions',\n",
    "#                           total=len(Data)):\n",
    "#     if fishnumber > items and fishnumber < items + 5:\n",
    "#         print(80*'-')\n",
    "#         print(fishnumber, row.Fish, row.Scan)\n",
    "#         display_otolith_position(fishnumber)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save out all otolith regions as .zarr files for quick access later\n",
    "# Construct output file name\n",
    "Data['OutputNameOtolithRegion'] = [os.path.join(os.path.dirname(folder), '%s.%s.Otolith.region.zarr' % (fish, scan)) for folder, fish, scan in zip(Data['Folder'],\n",
    "                                                                                                                                                   Data['Fish'],\n",
    "                                                                                                                                                   Data['Scan'])]\n",
    "# Replace otolith 'center' into name\n",
    "Data['OutputNameOtolithRegion'] = [n.replace('.region', '.at.%04d.%04d.%04d' % (ap, lt, dv)) for (n, ap, lt, dv) in zip(Data['OutputNameOtolithRegion'],\n",
    "                                                                                                                        Data['Otolith_Peak_Anteroposterior_fish'],\n",
    "                                                                                                                        Data['Otolith_Peak_Lateral_fish'],\n",
    "                                                                                                                        Data['Otolith_Peak_Dorsoventral_fish'])]\n",
    "# Replace otlith width into name\n",
    "Data['OutputNameOtolithRegion'] = [n.replace('.zarr', '.size.%04d.%04d.%04d.zarr' % (ap, lt, dv)) for (n, ap, lt, dv) in zip(Data['OutputNameOtolithRegion'],\n",
    "                                                                                                                             Data['Otolith_Width_Anteroposterior_fish'],\n",
    "                                                                                                                             Data['Otolith_Width_Lateral_fish'],\n",
    "                                                                                                                             Data['Otolith_Width_Dorsoventral_fish'])]\n",
    "# Actually save the regions out now\n",
    "for fishnumber, row in tqdm(Data.iterrows(),\n",
    "                          desc='Extracting otolith regions to .zarr files',\n",
    "                          total=len(Data)):\n",
    "    if not os.path.exists(row['OutputNameOtolithRegion']):\n",
    "        Reconstructions[fishnumber][Data['Otolith_Peak_Anteroposterior_fish'][fishnumber] - round(Data['Otolith_Width_Anteroposterior_fish'][fishnumber] / 2):\n",
    "                                  Data['Otolith_Peak_Anteroposterior_fish'][fishnumber] + round(Data['Otolith_Width_Anteroposterior_fish'][fishnumber] / 2),\n",
    "                                  Data['Otolith_Peak_Lateral_fish'][fishnumber] - round(Data['Otolith_Width_Lateral_fish'][fishnumber] / 2):\n",
    "                                  Data['Otolith_Peak_Lateral_fish'][fishnumber] + round(Data['Otolith_Width_Lateral_fish'][fishnumber] / 2),\n",
    "                                  Data['Otolith_Peak_Dorsoventral_fish'][fishnumber] - round(Data['Otolith_Width_Dorsoventral_fish'][fishnumber] / 2):\n",
    "                                  Data['Otolith_Peak_Dorsoventral_fish'][fishnumber] + round(Data['Otolith_Width_Dorsoventral_fish'][fishnumber] / 2),\n",
    "                                 ].rechunk('auto').to_zarr(row['OutputNameOtolithRegion'],\n",
    "                                                           overwrite=True,\n",
    "                                                           compressor=Blosc(cname='zstd',\n",
    "                                                                            shuffle=Blosc.BITSHUFFLE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved out otolith regions again\n",
    "Otoliths = [dask.array.from_zarr(file) for file in Data['OutputNameOtolithRegion']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we calculated everything, we can display it for *one* fish for the manuscript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search and use one specific fish\n",
    "print('The fish we are looking for is item %s in our dataframe' % whichone)\n",
    "print('We are loading the extracted otolith region of %s' % os.path.join(Data.Fish[whichone], Data.Scan[whichone]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put extracted otolith into a variable for simpler handling\n",
    "o = Otoliths[whichone]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show central views\n",
    "plt.subplot(131)\n",
    "plt.imshow(o[o.shape[0]//2,:,:])\n",
    "plt.subplot(132)\n",
    "plt.imshow(o[:,o.shape[1]//2,:])\n",
    "plt.subplot(133)\n",
    "plt.imshow(o[:,:,o.shape[2]//2])\n",
    "plt.suptitle('Middle views of %s' % Data['Folder'][whichone])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute histogram\n",
    "h, b = dask.array.histogram(o[o>0], bins=2**8, range=(0,2**8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute us some (multiotsu) peaks\n",
    "peaks = skimage.filters.threshold_multiotsu(o.compute(), classes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histogram and overlay the peaks\n",
    "plt.semilogy(h)\n",
    "# Show multiotsu threshold\n",
    "for p in peaks:\n",
    "    plt.axvline(p, label=p, c='red')\n",
    "plt.xlim([0,2**8])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display MIP view of extracted otolith region *in* original data\n",
    "outputname = os.path.join(os.path.dirname(Data['Folder'][whichone]),\n",
    "                          '%s.%s.otolith.region.MIP.png' % (Data['Fish'][whichone], Data['Scan'][whichone]))\n",
    "for c, direction in enumerate(directions):\n",
    "    plt.subplot(1,3,c+1)\n",
    "    plt.imshow(o.max(axis=c))    \n",
    "    plt.axis('off')\n",
    "    plt.gca().add_artist(ScaleBar(Data.Voxelsize[whichone], 'um'))\n",
    "    plt.title('%s view' % direction)\n",
    "plt.suptitle('MIPs of otolith region of fish %s' % Data['Fish'][whichone])\n",
    "if not os.path.exists(outputname):\n",
    "    plt.savefig(outputname,\n",
    "                transparent=True,\n",
    "                bbox_inches='tight')\n",
    "    print('Figure saved to %s' % outputname)\n",
    "plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_largest_regions(segmentation, number_of_regions=4, verbose=False):\n",
    "    # Get out biggest item from image\n",
    "    # First iteration based on https://stackoverflow.com/a/55110923/323100\n",
    "    # Since we want to select the several largest items, we improved on it basedd on\n",
    "    # https://github.com/numpy/numpy/issues/15128 and\n",
    "    #  https://stackoverflow.com/questions/6910641/how-do-i-get-indices-of-n-maximum-values-in-a-numpy-array#comment24252527_6910672\n",
    "    labels = skimage.measure.label(segmentation)\n",
    "    # print(numpy.argsort(-numpy.bincount(labels.flat, weights=labels.flat))[:number_of_regions])\n",
    "    # print(sorted(numpy.bincount(labels.flat, weights=labels.flat), reverse=True)[:number_of_regions])\n",
    "    # print(numpy.argmax(numpy.bincount(labels.flat, weights=labels.flat)))\n",
    "    # Initialize empty array to add into\n",
    "    largestCC = dask.array.zeros_like(segmentation)\n",
    "    for lbl in numpy.argsort(-numpy.bincount(labels.flat, weights=labels.flat))[:number_of_regions]:\n",
    "        largestCC += labels == lbl\n",
    "    if verbose:\n",
    "        plt.subplot(121)\n",
    "        plt.imshow(segmentation)\n",
    "        plt.subplot(122)\n",
    "        plt.imshow(largestCC)\n",
    "        plt.suptitle('Largest connected component')\n",
    "        plt.show()\n",
    "    return dask.array.rechunk(largestCC.astype('uint8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get out largest regions\n",
    "l = get_largest_regions((o>peaks[-1]), number_of_regions=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the largest regions as masks for the original data\n",
    "masked = dask.array.multiply(o, l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_zeros(arr):\n",
    "    \"\"\"Returns a trimmed view of an n-D array excluding any outer\n",
    "    regions which contain only zeros.\n",
    "    From https://stackoverflow.com/a/65547931/323100\n",
    "    \"\"\"\n",
    "    slices = tuple(slice(idx.min(), idx.max() + 1) for idx in numpy.nonzero(arr))\n",
    "    return arr[slices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trim the masked data\n",
    "masked_trimmed = dask.array.asarray(trim_zeros(masked.compute()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display MIP of extracted and masked otolith region\n",
    "outputname = os.path.join(os.path.dirname(Data['Folder'][whichone]),\n",
    "                          '%s.%s.otolith.region.extracted.MIP.png' % (Data['Fish'][whichone], Data['Scan'][whichone]))\n",
    "for c, direction in enumerate(directions):\n",
    "    plt.subplot(1,3,c+1)\n",
    "    plt.imshow(masked_trimmed.max(axis=c))\n",
    "    plt.axis('off')\n",
    "    plt.gca().add_artist(ScaleBar(Data.Voxelsize[whichone], 'um'))\n",
    "    plt.title('%s view' % direction)\n",
    "plt.suptitle('MIPs of extracted Otolith of fish %s' % Data['Fish'][whichone])\n",
    "if not os.path.exists(outputname):\n",
    "    plt.savefig(outputname,\n",
    "                transparent=True,\n",
    "                bbox_inches='tight')\n",
    "    print('Figure saved to %s' % outputname)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View the otolith in 3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the largestCC of the otolith in 3D\n",
    "subsample = 1\n",
    "plt_mask = k3d.volume(masked_trimmed[::subsample,::subsample,::subsample].astype(numpy.float16),\n",
    "                      scaling=[Data.Voxelsize[whichone],\n",
    "                               Data.Voxelsize[whichone],\n",
    "                               Data.Voxelsize[whichone]]\n",
    "                     )\n",
    "plot = k3d.plot()\n",
    "plot += plt_mask\n",
    "plot.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save out HTML page, which we could potentially upload as supplementary material\n",
    "outputname = os.path.join(os.path.dirname(Data['Folder'][whichone]),\n",
    "                          '%s.%s.otolith.region.3D.html' % (Data['Fish'][whichone], Data['Scan'][whichone]))\n",
    "if not os.path.exists(outputname):\n",
    "    with open(outputname, \"w\") as f:\n",
    "        f.write(plot.get_snapshot())\n",
    "    print('3D view saved to %s' % outputname)\n",
    "else:\n",
    "    print('3D view was already saved to %s, not saving it again' % outputname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Do the extraction dance for all the fishes\n",
    "# for fishnumber, row in tqdm(Data.iterrows(),\n",
    "#                            desc='Displaying otolith regions and stuff',\n",
    "#                            total=len(Data)):\n",
    "#     outputnameMIP = os.path.join(os.path.dirname(Data['Folder'][fishnumber]), Data['Fish'][fishnumber] + '.Otolith.Region.MIP.png')\n",
    "#     outputnameTrimmedMIP = os.path.join(os.path.dirname(Data['Folder'][fishnumber]), Data['Fish'][fishnumber] + '.Otolith.Region.Extracted.MIP.png')    \n",
    "#     o = Otoliths[fishnumber]\n",
    "#     h, b = dask.array.histogram(o[o>0], bins=2**8, range=(0,2**8))\n",
    "#     peaks = skimage.filters.threshold_multiotsu(o.compute(), classes=5)\n",
    "#     if not os.path.exists(outputnameMIP):        \n",
    "#         # Display MIP view of extracted otolith region *in* original data\n",
    "#         for c, direction in enumerate(directions):\n",
    "#             plt.subplot(1,3,c+1)\n",
    "#             plt.imshow(o.max(axis=c))    \n",
    "#             plt.axis('off')\n",
    "#             plt.gca().add_artist(ScaleBar(Data.Voxelsize[fishnumber], 'um'))\n",
    "#             plt.title('%s view' % direction)\n",
    "#         plt.suptitle('MIPs of otolith region of fish %s' % Data['Fish'][fishnumber])\n",
    "#         plt.savefig(outputnameMIP,\n",
    "#                     transparent=True,\n",
    "#                     bbox_inches='tight')\n",
    "#         print('Figure saved to %s' % outputnameMIP)\n",
    "#         plt.show()\n",
    "#     if not os.path.exists(outputnameTrimmedMIP):\n",
    "#         # Get out largest regions\n",
    "#         try:\n",
    "#             l = get_largest_regions((o>peaks[-1]), number_of_regions=4)\n",
    "#         except MemoryError:\n",
    "#             print('Could not label region, probably too big')\n",
    "#             l = dask.array.zeros_like(o)\n",
    "#             l[100:-100,100:-100,100:-100] = o[100:-100,100:-100,100:-100]\n",
    "#         masked = dask.array.multiply(o, l)\n",
    "#         masked_trimmed = dask.array.asarray(trim_zeros(masked.compute()))\n",
    "#         # Display MIP of extracted and masked otolith region\n",
    "#         for c, direction in enumerate(directions):\n",
    "#             plt.subplot(1,3,c+1)\n",
    "#             plt.imshow(masked_trimmed.max(axis=c))\n",
    "#             plt.axis('off')\n",
    "#             plt.gca().add_artist(ScaleBar(Data.Voxelsize[fishnumber], 'um'))\n",
    "#             plt.title('%s view' % direction)\n",
    "#         plt.suptitle('MIPs of extracted Otolith of fish %s' % Data['Fish'][fishnumber])\n",
    "#         plt.savefig(outputnameTrimmedMIP,\n",
    "#                     transparent=True,\n",
    "#                     bbox_inches='tight')\n",
    "#         print('Figure saved to %s' % outputnameTrimmedMIP)\n",
    "#         plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
