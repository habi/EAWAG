{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fiddle with the EAWAG scans\n",
    "Look at the orientation and see if we can do some cropping based on landmarks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "import os\n",
    "import glob\n",
    "import pandas\n",
    "import imageio\n",
    "import numpy\n",
    "import scipy\n",
    "import k3d\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib_scalebar.scalebar import ScaleBar\n",
    "import seaborn\n",
    "import dask\n",
    "import dask_image.imread\n",
    "from dask.distributed import Client, LocalCluster\n",
    "import skimage\n",
    "from tqdm.auto import tqdm, trange\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set dask temporary folder\n",
    "# Do this before creating a client: https://stackoverflow.com/a/62804525/323100\n",
    "import tempfile\n",
    "if 'Linux' in platform.system():\n",
    "    tmp = os.path.join(os.sep, 'media', 'habi', 'Fast_SSD')\n",
    "elif 'Darwin' in platform.system():\n",
    "    tmp = tempfile.gettempdir()\n",
    "else:\n",
    "    if 'anaklin' in platform.node():\n",
    "        tmp = os.path.join('F:\\\\')\n",
    "    else:\n",
    "        tmp = os.path.join('D:\\\\')\n",
    "dask.config.set({'temporary_directory': os.path.join(tmp, 'tmp')})\n",
    "print('Dask temporary files go to %s' % dask.config.get('temporary_directory'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start cluster and client now, after setting tempdir\n",
    "cluster = LocalCluster()\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('You can seee what DASK is doing at \"http://localhost:%s/status\"' % client.scheduler_info()['services']['dashboard'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Ignore warnings in the notebook\n",
    "# import warnings\n",
    "# warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up figure defaults\n",
    "plt.rc('image', cmap='gray', interpolation='nearest')  # Display all images in b&w and with 'nearest' interpolation\n",
    "plt.rcParams['figure.figsize'] = (8, 4.5)  # Size up figures a bit\n",
    "plt.rcParams['figure.dpi'] = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup scale bar defaults\n",
    "plt.rcParams['scalebar.location'] = 'lower right'\n",
    "plt.rcParams['scalebar.frameon'] = False\n",
    "plt.rcParams['scalebar.color'] = 'white'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display all plots identically\n",
    "lines = 3\n",
    "# And then do something like\n",
    "# plt.subplot(lines, int(numpy.ceil(len(Data) / float(lines))), c + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different locations if running either on Linux or Windows\n",
    "FastSSD = True\n",
    "overthere = False  # Load the data directly from the iee-research_storage drive\n",
    "nanoct = True  # Load the data directly from the 2214\n",
    "# to speed things up significantly\n",
    "if 'Linux' in platform.system():\n",
    "    if FastSSD:\n",
    "        BasePath = os.path.join(os.sep, 'media', 'habi', 'Fast_SSD')\n",
    "    elif overthere:\n",
    "        BasePath = os.path.join(os.sep, 'home', 'habi', 'research-storage-iee')\n",
    "    elif nanoct:\n",
    "        BasePath = os.path.join(os.path.sep, 'home', 'habi', '2214')\n",
    "    else:\n",
    "        BasePath = os.path.join(os.sep, 'home', 'habi', '1272')\n",
    "elif 'Darwin' in platform.system():\n",
    "    FastSSD = False\n",
    "    BasePath = os.path.join('/Users/habi/Dev/EAWAG/Data')\n",
    "elif 'Windows' in platform.system():\n",
    "    if FastSSD:\n",
    "        BasePath = os.path.join('F:\\\\')\n",
    "    else:\n",
    "        if 'anaklin' in platform.node():\n",
    "            BasePath = os.path.join('S:\\\\')\n",
    "        else:\n",
    "            BasePath = os.path.join('D:\\\\Results')\n",
    "if not overthere:\n",
    "    Root = os.path.join(BasePath, 'EAWAG')\n",
    "else:\n",
    "    Root = BasePath\n",
    "# if overthere:\n",
    "#         Root = os.path.join('I:\\\\microCTupload')\n",
    "print('We are loading all the data from %s' % Root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pixelsize(logfile):\n",
    "    \"\"\"Get the pixel size from the scan log file\"\"\"\n",
    "    with open(logfile, 'r') as f:\n",
    "        for line in f:\n",
    "            if 'Image Pixel' in line and 'Scaled' not in line:\n",
    "                pixelsize = float(line.split('=')[1])\n",
    "    return(pixelsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_git_hash():\n",
    "    '''\n",
    "    Get the current git hash from the repository.\n",
    "    Based on http://stackoverflow.com/a/949391/323100 and\n",
    "    http://stackoverflow.com/a/18283905/323100\n",
    "    '''\n",
    "    from subprocess import Popen, PIPE\n",
    "    import os\n",
    "    gitprocess = Popen(['git',\n",
    "                        '--git-dir',\n",
    "                        os.path.join(os.getcwd(), '.git'),\n",
    "                        'rev-parse',\n",
    "                        '--short',\n",
    "                        '--verify',\n",
    "                        'HEAD'],\n",
    "                       stdout=PIPE)\n",
    "    (output, _) = gitprocess.communicate()\n",
    "    return output.strip().decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Make directory for output\n",
    "# OutPutDir = os.path.join(os.getcwd(), 'Output', get_git_hash())\n",
    "# print('We are saving all the output to %s' % OutPutDir)\n",
    "# os.makedirs(OutPutDir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make us a dataframe for saving all that we need\n",
    "Data = pandas.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get *all* log files\n",
    "# Sort them by time, not name\n",
    "Data['LogFile'] = [f for f in sorted(glob.glob(os.path.join(Root, '**', '*.log'), recursive=True), key=os.path.getmtime)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate folder name\n",
    "Data['Folder'] = [os.path.dirname(f) for f in Data['LogFile']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get rid of all non-rec logfiles\n",
    "for c, row in Data.iterrows():\n",
    "    if 'rec' not in row.Folder:\n",
    "        Data.drop([c], inplace=True)\n",
    "    elif 'SubScan' in row.Folder:\n",
    "        Data.drop([c], inplace=True)\n",
    "    elif 'rectmp.log' in row.LogFile:\n",
    "        Data.drop([c], inplace=True)\n",
    "# Reset dataframe to something that we would get if we only would have loaded the 'rec' files\n",
    "Data = Data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate us some meaningful colums\n",
    "Data['Fish'] = [l[len(Root) + 1:].split(os.sep)[0] for l in Data['LogFile']]\n",
    "Data['Scan'] = ['_'.join(l[len(Root) + 1:].split(os.sep)[1:-1]) for l in Data['LogFile']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the file names of the reconstructions\n",
    "Data['Reconstructions'] = [sorted(glob.glob(os.path.join(f, '*rec0*.png'))) for f in Data['Folder']]\n",
    "Data['Number of reconstructions'] = [len(r) for r in Data.Reconstructions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop samples which have either not been reconstructed yet or of which we deleted the reconstructions with\n",
    "# `find . -name \"*rec*.png\" -type f -mtime +333 -delete`\n",
    "# Based on https://stackoverflow.com/a/13851602\n",
    "# for c, row in Data.iterrows():\n",
    "#     if not row['Number of reconstructions']:\n",
    "#         print('%s contains no PNG files, we might be currently reconstructing it' % row.Folder)\n",
    "Data = Data[Data['Number of reconstructions'] > 0]\n",
    "Data.reset_index(drop=True, inplace=True)\n",
    "print('We have %s folders with reconstructions' % (len(Data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get parameters we need from the log files\n",
    "Data['Voxelsize'] = [get_pixelsize(log) for log in Data['LogFile']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data['PreviewImagePath'] = [sorted(glob.glob(os.path.join(f, '*_spr.bmp')))[0] for f in Data['Folder']]\n",
    "# Squeeze the preview images, so we don't have to do it afterwards\n",
    "# https://github.com/dask/dask-image/issues/239#issuecomment-874411053\n",
    "Data['PreviewImage'] = [dask_image.imread.imread(pip).squeeze()\n",
    "                        if pip\n",
    "                        else numpy.random.random((100, 100)) for pip in Data['PreviewImagePath']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set how many lines we want the images to have\n",
    "lines = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c, row in Data.iterrows():\n",
    "    plt.subplot(lines, int(numpy.ceil(len(Data) / float(lines))), c + 1)\n",
    "    plt.imshow(row.PreviewImage)\n",
    "    plt.title(os.path.join(row['Fish'], row['Scan']))\n",
    "    plt.gca().add_artist(ScaleBar(row['Voxelsize'],\n",
    "                                  'um',\n",
    "                                  color='black',\n",
    "                                  frameon=True))\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(Root, 'ScanOverviews.png'),\n",
    "            bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load all reconstructions into DASK arrays\n",
    "Reconstructions = [None] * len(Data)\n",
    "for c, row in tqdm(Data.iterrows(),\n",
    "                   desc='Load reconstructions',\n",
    "                   total=len(Data)):\n",
    "    Reconstructions[c] = dask_image.imread.imread(os.path.join(row['Folder'],\n",
    "                                                               '*rec*.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check if something went wrong\n",
    "# for file in Data['OutputNameRec']:\n",
    "#     print(file)\n",
    "#     dask.array.from_zarr(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How big are the datasets?\n",
    "Data['Size'] = [rec.shape for rec in Reconstructions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The three cardinal directions\n",
    "# Names adapted to fishes: https://en.wikipedia.org/wiki/Fish_anatomy#Body\n",
    "directions = ['Anteroposterior',\n",
    "              'Lateral',\n",
    "              'Dorsoventral']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read or calculate the directional MIPs, put them into the dataframe and save them to disk\n",
    "for d, direction in enumerate(directions):\n",
    "    Data['MIP_' + direction] = ''\n",
    "for c, row in tqdm(Data.iterrows(), desc='Calculating MIPs', total=len(Data)):\n",
    "    for d, direction in tqdm(enumerate(directions),\n",
    "                             desc='%s/%s: %s' % (row['Fish'], row['Scan'], direction),\n",
    "                             leave=False,\n",
    "                             total=len(directions)):\n",
    "        outfilepath = os.path.join(os.path.dirname(row['Folder']),\n",
    "                                   '%s.%s.MIP.%s.png' % (row['Fish'], row['Scan'], direction))\n",
    "        if os.path.exists(outfilepath):\n",
    "            Data.at[c, 'MIP_' + direction] = dask_image.imread.imread(outfilepath).squeeze()\n",
    "        else:\n",
    "            # Generate MIP\n",
    "            Data.at[c, 'MIP_' + direction] = Reconstructions[c].max(axis=d).compute().squeeze()\n",
    "            # Save it out\n",
    "            imageio.imwrite(outfilepath, Data.at[c, 'MIP_' + direction].astype('uint8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show MIP slices\n",
    "for c, row in tqdm(Data.iterrows(),\n",
    "                   desc='Saving overview of MIP images',\n",
    "                   total=len(Data)):\n",
    "    outfilepath = os.path.join(os.path.dirname(row['Folder']),\n",
    "                               '%s.%s.MIPs.png' % (row['Fish'], row['Scan']))\n",
    "    print('%s/%s: %s' % (c, len(Data), os.path.join(row.Fish, row.Scan)))\n",
    "    for d, direction in tqdm(enumerate(directions),\n",
    "                             desc='%s/%s' % (row['Fish'], row['Scan']),\n",
    "                             total=len(directions)):\n",
    "        plt.subplot(1, 3, d + 1)\n",
    "        plt.imshow(row['MIP_' + direction])\n",
    "        plt.gca().add_artist(ScaleBar(row['Voxelsize'],\n",
    "                                      'um'))\n",
    "        plt.title('%s MIP' % direction)\n",
    "        plt.axis('off')\n",
    "        plt.title('%s\\n%s MIP' % (os.path.join(row['Fish'], row['Scan']), direction))\n",
    "    if not os.path.exists(outfilepath):\n",
    "        plt.savefig(outfilepath,\n",
    "                    transparent=True,\n",
    "                    bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for c, row in Data.iterrows():\n",
    "#     # load one Anteroposterior MIP into an image for fiddling with it\n",
    "#     plt.imshow(row.MIP_Anteroposterior)\n",
    "#     plt.title('%s/%s: %s/%s' % (c, len(Data), row.Fish, row.Scan))\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The functions below were copied from Hearts-Melly/SubMyocardAnalysis.ipynb, in which we also look at the orientation of things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From Hearts-Melly/SubMyocardAnalysis.ipynb\n",
    "def get_properties(roi, verbose=False):\n",
    "    # Label filled image\n",
    "    labeled_img = skimage.measure.label(roi)\n",
    "    # Extract regionprops of image and put data into pandas\n",
    "    # https://stackoverflow.com/a/66632023/323100\n",
    "    props = skimage.measure.regionprops_table(labeled_img,\n",
    "                                              properties=('label',\n",
    "                                                          'centroid',\n",
    "                                                          'area',\n",
    "                                                          'perimeter',\n",
    "                                                          'orientation'))\n",
    "    table = pandas.DataFrame(props)\n",
    "    table_sorted = table.sort_values(by='area', ascending=False)\n",
    "    # return only the region with the biggest area\n",
    "    properties = table_sorted.iloc[:1].reset_index()\n",
    "    if verbose:\n",
    "        plt.imshow(roi, alpha=0.5)\n",
    "        plt.title('Original')\n",
    "        plt.axis('off')\n",
    "        plt.imshow(numpy.ma.masked_equal(labeled_img, 0), cmap='viridis', alpha=0.5)\n",
    "        plt.title('Labelled')\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "    return(properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_largest_region(segmentation, verbose=False):\n",
    "    # Get out biggest item from https://stackoverflow.com/a/55110923/323100\n",
    "    labels = skimage.measure.label(segmentation)\n",
    "    assert(labels.max() != 0)  # assume at least 1 CC\n",
    "    largestCC = labels == numpy.argmax(numpy.bincount(labels.flat)[1:]) + 1\n",
    "    if verbose:\n",
    "        plt.subplot(121)\n",
    "        plt.imshow(segmentation)\n",
    "        plt.subplot(122)\n",
    "        plt.imshow(largestCC)\n",
    "        plt.suptitle('Largest connected component')\n",
    "        plt.show()\n",
    "    return largestCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def threshold(image, verbose=False):\n",
    "    # Calculate threshold of image where image is non-zero\n",
    "    threshold = skimage.filters.threshold_otsu(image[image > 0])\n",
    "    if verbose:\n",
    "        plt.subplot(121)\n",
    "        plt.imshow(image)\n",
    "        plt.imshow(dask.array.ma.masked_equal(image > threshold, 0),\n",
    "                   alpha=0.618,\n",
    "                   cmap='viridis_r')\n",
    "        plt.subplot(122)\n",
    "        plt.semilogy(histogram(image), label='Log-Histogram')\n",
    "        plt.axvline(threshold, label='Otsu threshold: %s' % threshold)\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    return(threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the histogram of an image\n",
    "# We can safely assume to only use 8bit images\n",
    "def histogram(img):\n",
    "    histogram, bins = dask.array.histogram(dask.array.array(img),\n",
    "                                           bins=2**8,\n",
    "                                           range=[0, 2**8])\n",
    "    return(histogram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whichone = 18\n",
    "print(os.path.join(Data.Fish[whichone], Data.Scan[whichone]))\n",
    "img = Data.MIP_Anteroposterior[whichone]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = threshold(img.compute(), verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = get_largest_region(img > t, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_centroid(img, verbose=False):\n",
    "    props = get_properties(img)\n",
    "    # Drawing from https://scikit-image.org/docs/stable/auto_examples/segmentation/plot_regionprops.html\n",
    "    y0, x0 = props['centroid-0'], props['centroid-1']\n",
    "    if verbose:\n",
    "        plt.imshow(img)\n",
    "        plt.scatter(props['centroid-1'], props['centroid-0'], marker=None, color='r')\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "    return((x0, y0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_contour(filled_img, verbose=False):\n",
    "    # Contouring from https://scikit-image.org/docs/dev/auto_examples/segmentation/plot_regionprops.html\n",
    "    largest_region = get_largest_region(filled_img, verbose=False)\n",
    "    contour = skimage.measure.find_contours(largest_region)\n",
    "    # Even though we look only at the largest region, we still might get out more than one contour\n",
    "    # Let's thus sort the list and just continue with the longest one\n",
    "    (contour).sort(key=len)\n",
    "    cy, cx = contour[-1].T\n",
    "    if verbose:\n",
    "        plt.imshow(filled_img)\n",
    "        plt.plot(cx, cy, lw=1, c='r')\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "    return(cx, cy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contour = get_contour(img > t, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroid = get_centroid(img > t, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_orientation(img, x0, x1, x2, y0, y1, y2, self=False):\n",
    "    if self:\n",
    "        plt.imshow(img)\n",
    "    plt.plot((x0, x1), (y0, y1), '-r', linewidth=1)\n",
    "    plt.plot((x0, x2), (y0, y2), '-r', linewidth=1)\n",
    "    if self:\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "    return()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_orientation(img, voxelsize, length=5000, verbose=False):\n",
    "    props = get_properties(img)\n",
    "    whichlengthdowewant = length\n",
    "    reallength = whichlengthdowewant / voxelsize  # um\n",
    "    # Drawing from https://scikit-image.org/docs/stable/auto_examples/segmentation/plot_regionprops.htm\n",
    "    x0, y0 = get_centroid(img)\n",
    "    x1 = x0 + math.cos(props['orientation']) * reallength\n",
    "    y1 = y0 - math.sin(props['orientation']) * reallength\n",
    "    x2 = x0 - math.sin(props['orientation']) * reallength\n",
    "    y2 = y0 - math.cos(props['orientation']) * reallength\n",
    "    if verbose:\n",
    "        plt.imshow(img)\n",
    "        plt.scatter(props['centroid-1'], props['centroid-0'], marker=None, color='r')\n",
    "        draw_orientation(img, x0, x1, x2, y0, y1, y2)\n",
    "        plt.gca().add_artist(ScaleBar(voxelsize, 'um'))\n",
    "        plt.title('Image with %s um long orientation bars' % length)\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "    return(x0, x1, x2, y0, y1, y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0, x1, x2, y0, y1, y2 = get_orientation(img > t,\n",
    "                                         Data.Voxelsize[4],\n",
    "                                         length=10000,\n",
    "                                         verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The functions above were copied from Hearts-Melly/SubMyocardAnalysis.ipynb, in which we also look at the orientation of things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = get_properties(img > t)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the angle and centroid to rotate image\n",
    "img_rotated = numpy.empty_like(img)\n",
    "img_rotated = skimage.transform.rotate(img.compute(),\n",
    "                                       angle=numpy.rad2deg(a.orientation[0]),\n",
    "                                       center=(a['centroid-0'][0], a['centroid-1'][0]),\n",
    "                                       preserve_range=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's show what we did\n",
    "plt.subplot(121)\n",
    "plt.imshow(img)\n",
    "plt.scatter(a['centroid-0'], a['centroid-1'], s=50)\n",
    "plt.title('Original image with centroid')\n",
    "plt.subplot(122)\n",
    "plt.imshow(img_rotated)\n",
    "plt.scatter(a['centroid-0'], a['centroid-1'], s=50)\n",
    "plt.title('Image rotated by %0.fÂ°, with centroid' % numpy.rad2deg(a.orientation[0]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect otoliths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy of otolith detection function from Zebrafish-Carolina/Muscles-Volumentry\n",
    "def otolither(img, sigma=5, level=0.9, threshold=None, verbose=False):\n",
    "    '''\n",
    "    Function to detect the otoliths in the Anteroposterior MIPs.\n",
    "    We simply look for peaks in the gray values :)\n",
    "    '''\n",
    "    # Smooth image for less noise\n",
    "    smoothed = scipy.ndimage.gaussian_filter(img, sigma=sigma, order=0)\n",
    "    # If we don't set a threshold for 'detecting' the otoliths, set it at `level' of the maximal brightness of the smoothed image\n",
    "    if not threshold:\n",
    "        threshold = level * smoothed.max()\n",
    "    # Detect peaks in smoothed image, in x- and y-direction\n",
    "    x = numpy.mean(smoothed > threshold, axis=0)\n",
    "    y = numpy.mean(smoothed > threshold, axis=1)\n",
    "    peaksx, _ = scipy.signal.find_peaks(x)\n",
    "    peaksy, _ = scipy.signal.find_peaks(y)\n",
    "    if verbose:\n",
    "        plt.subplot(131)\n",
    "        plt.imshow(img)\n",
    "        plt.title('Input image')\n",
    "        plt.subplot(132)\n",
    "        plt.semilogy(histogram(img), label='Histogram')\n",
    "        plt.axvline(threshold, label='%s%% of max@%s' % (int(100 * level), int(round(threshold))), color=seaborn.color_palette()[1])\n",
    "        plt.legend()\n",
    "        plt.title('Histogram')\n",
    "        plt.subplot(133)\n",
    "        plt.imshow(smoothed)\n",
    "        plt.title('Smoothed image\\nwith location of otoliths')\n",
    "        plt.imshow(numpy.ma.masked_less(img, threshold), cmap='viridis', alpha=0.618)\n",
    "        for c, p in enumerate(peaksx):\n",
    "            plt.axvline(p, alpha=0.618)\n",
    "        for p in peaksy:\n",
    "            plt.axhline(p, alpha=0.618)\n",
    "        plt.axvline(numpy.mean(peaksx), color='r')\n",
    "        plt.axhline(numpy.mean(peaksy), color='r')\n",
    "        plt.show()\n",
    "    return([peaksx.tolist(), peaksy.tolist()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a column for saving the otolith positions\n",
    "for d, direction in enumerate(directions):\n",
    "    Data['Otholith_Positions_' + direction] = ''\n",
    "    Data['Otholith_Position_Mean_' + direction] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect otolith positions\n",
    "for c, row in Data.iterrows():\n",
    "    print('Finding otolith position for %s/%s' % (row.Fish, row.Scan))\n",
    "    for d, direction in enumerate(directions):\n",
    "        Data.at[c, 'Otholith_Positions_' + direction] = otolither(row['MIP_' + direction], verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save us the mean position\n",
    "for d, direction in enumerate(directions):\n",
    "    Data['Otholith_Position_Mean_' + direction] = [(numpy.mean(op[0]),\n",
    "                                                    numpy.mean(op[1])) for op in Data['Otholith_Positions_' + direction]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in Data['Otholith_Positions_Lateral']:\n",
    "    print(round(numpy.mean(i[0])),\n",
    "          round(numpy.mean(i[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in Data['Otholith_Position_Mean_Lateral']:\n",
    "    print(round(i[0]), round(i[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for whichone in range(len(Data)):\n",
    "    print(whichone, os.path.join(Data.Fish[whichone], Data.Scan[whichone]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "direction = 'Lateral'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c, row in Data.iterrows():\n",
    "    for d, direction in enumerate(directions):\n",
    "        plt.subplot(1, 3, d + 1)\n",
    "        plt.imshow(row['MIP_' + direction])\n",
    "        plt.title([round(i) for i in row['Otholith_Position_Mean_' + direction]])\n",
    "        plt.axhline(row['Otholith_Position_Mean_' + direction][1])\n",
    "        plt.axvline(row['Otholith_Position_Mean_' + direction][0])\n",
    "        plt.suptitle('%s/%s' % (row.Fish, row.Scan))\n",
    "        plt.gca().add_artist(ScaleBar(row.Voxelsize, 'um'))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/a/50011743/323100\n",
    "def rescale_linear(array, new_min, new_max):\n",
    "    \"\"\"Rescale an arrary linearly.\"\"\"\n",
    "    minimum, maximum = numpy.min(array), numpy.max(array)\n",
    "    m = (new_max - new_min) / (maximum - minimum)\n",
    "    b = new_min - m * minimum\n",
    "    return m * array + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect otollith position by looking for maximum gray value along fish\n",
    "for d, direction in enumerate(directions):\n",
    "    Data['GrayValueAlong_' + direction] = ''\n",
    "    Data['Otolith_MIP_Position_' + direction] = ''\n",
    "for whichone, row in Data.iterrows():\n",
    "    for d, direction in enumerate(directions):\n",
    "        # Calculate gray value sum along fish.\n",
    "        Data.at[whichone, 'GrayValueAlong_' + direction] = dask.array.sum(Data['MIP_' + direction][whichone],\n",
    "                                                                          axis=1)\n",
    "        # Maximum of this shoud give us the otolith position\n",
    "        Data.at[whichone,\n",
    "                'Otolith_MIP_Position_' + direction] = dask.array.argmax(dask.array.sum(Data['MIP_' + direction][whichone],\n",
    "                                                                                        axis=1))\n",
    "        # Plot what we found\n",
    "        plt.subplot(1, 3, d + 1)\n",
    "        plt.imshow(Data['MIP_' + direction][whichone])\n",
    "        # Plot the *rescaled* values over the image\n",
    "        plt.plot(rescale_linear(Data['GrayValueAlong_' + direction][whichone],\n",
    "                                100,\n",
    "                                Data['MIP_' + direction][whichone].shape[1] - 100),\n",
    "                 range(len(Data['GrayValueAlong_' + direction][whichone])),\n",
    "                 label='Normalized gray value sum along fish',\n",
    "                 color=seaborn.color_palette()[0])\n",
    "        plt.axhline(Data['Otolith_MIP_Position_' + direction][whichone],\n",
    "                    label='Max@%s' % Data['Otolith_MIP_Position_' + direction][whichone].compute(),\n",
    "                    color=seaborn.color_palette()[1])\n",
    "        plt.title('%s MIP' % direction)\n",
    "        plt.suptitle('%s/%s: MIPs of %s/%s' % (whichone, len(Data), Data.Fish[whichone], Data.Scan[whichone]))\n",
    "        plt.legend(loc='lower center')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c, row in Data.iterrows():\n",
    "    print(c, len(Data), os.path.join(row.Fish, row.Scan))\n",
    "    print('\\t Otolith from MIP',\n",
    "          row['Otolith_MIP_Position_Anteroposterior'].compute(),\n",
    "          row['Otolith_MIP_Position_Lateral'].compute(),\n",
    "          row['Otolith_MIP_Position_Dorsoventral'].compute())\n",
    "    print('\\t Otolith from otholither function',\n",
    "          row['Otholith_Position_Mean_Anteroposterior'],\n",
    "          row['Otholith_Position_Mean_Lateral'],\n",
    "          row['Otholith_Position_Mean_Dorsoventral'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whichone = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c, direction in enumerate(directions):\n",
    "    plt.subplot(1, 3, c + 1)\n",
    "    plt.imshow(Data['MIP_' + direction][whichone])\n",
    "    # From otholither function\n",
    "    plt.axhline(Data['Otholith_Position_Mean_' + direction][whichone][1],\n",
    "                label='otholither mean position 1: %s' % round(Data['Otholith_Position_Mean_' + direction][whichone][1]),\n",
    "                color=seaborn.color_palette()[0])\n",
    "    plt.axvline(Data['Otholith_Position_Mean_' + direction][whichone][0],\n",
    "                label='otholither mean posistion 0: %s' % round(Data['Otholith_Position_Mean_' + direction][whichone][0]),\n",
    "                color=seaborn.color_palette()[1])\n",
    "    # From sum along axis\n",
    "    plt.axhline(Data['Otolith_MIP_Position_' + direction][whichone],\n",
    "                label='MIP sum: %s' % Data['Otolith_MIP_Position_' + direction][whichone].compute(),\n",
    "                color=seaborn.color_palette()[3])\n",
    "    plt.legend(loc='lower center')\n",
    "    plt.title(direction)\n",
    "    plt.suptitle('%s/%s' % (Data.Fish[whichone], Data.Scan[whichone]))\n",
    "    plt.gca().add_artist(ScaleBar(Data.Voxelsize[whichone], 'um'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for direction in directions:\n",
    "    print(direction, Data['MIP_' + direction][whichone].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for direction in directions:\n",
    "    print(direction, Data['Otholith_Position_Mean_' + direction][whichone])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for direction in directions:\n",
    "    print(direction, round(Data['Otolith_MIP_Position_' + direction][whichone].compute()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CAN WE CALCULATE BOTH DV and THE LT POSITION ON THE AP MIP?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get us positions of otolith in relation to original data\n",
    "position_ap = numpy.mean((Data['Otholith_Position_Mean_Lateral'][whichone][1],\n",
    "                          Data['Otholith_Position_Mean_Dorsoventral'][whichone][1],\n",
    "                          Data['Otolith_MIP_Position_Lateral'][whichone],\n",
    "                          Data['Otolith_MIP_Position_Dorsoventral'][whichone]),\n",
    "                         dtype='int')\n",
    "# laterally, we assume the center of the image for now\n",
    "# position_lt = numpy.mean((Data['Otholith_Position_Mean_' + direction][whichone][1],\n",
    "#                           Data['Otolith_MIP_Position_' + direction][whichone]),dtype='int')\n",
    "position_lt = Data.MIP_Dorsoventral[whichone].shape[1] // 2\n",
    "position_dv = numpy.mean((Data['Otholith_Position_Mean_Anteroposterior'][whichone][0],\n",
    "                          Data['Otholith_Position_Mean_Lateral'][whichone][0]),\n",
    "                         dtype='int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(position_ap)\n",
    "print(position_lt)\n",
    "print(position_dv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slicethickness = 400\n",
    "for c, direction in enumerate(directions):\n",
    "    plt.subplot(1, 3, c + 1)\n",
    "    plt.imshow(Data['MIP_' + direction][whichone])\n",
    "    if c == 0:\n",
    "        plt.fill_between(range(Data['MIP_' + direction][whichone].shape[1]),\n",
    "                         position_lt - slicethickness // 2,\n",
    "                         position_lt + slicethickness // 2,\n",
    "                         alpha=0.5)\n",
    "        plt.fill_between(range(position_dv - slicethickness // 2, position_dv + slicethickness // 2),\n",
    "                         1,\n",
    "                         Data['MIP_' + direction][whichone].shape[0] - 1,\n",
    "                         alpha=0.5)\n",
    "    elif c == 1:\n",
    "        plt.fill_between(range(Data['MIP_' + direction][whichone].shape[1]),\n",
    "                         position_ap - slicethickness // 2,\n",
    "                         position_ap + slicethickness // 2,\n",
    "                         alpha=0.5)\n",
    "        plt.fill_between(range(position_dv - slicethickness // 2, position_dv + slicethickness // 2),\n",
    "                         1,\n",
    "                         Data['MIP_' + direction][whichone].shape[0] - 1,\n",
    "                         alpha=0.5)\n",
    "    else:\n",
    "        plt.fill_between(range(Data['MIP_' + direction][whichone].shape[1]),\n",
    "                         position_ap - slicethickness // 2,\n",
    "                         position_ap + slicethickness // 2,\n",
    "                         alpha=0.5)\n",
    "        plt.fill_between(range(position_lt - slicethickness // 2, position_lt + slicethickness // 2),\n",
    "                         1,\n",
    "                         Data['MIP_' + direction][whichone].shape[0] - 1,\n",
    "                         alpha=0.5)\n",
    "    plt.title(direction)\n",
    "    plt.suptitle('%s/%s' % (Data.Fish[whichone], Data.Scan[whichone]))\n",
    "    plt.gca().add_artist(ScaleBar(Data.Voxelsize[whichone], 'um'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab region calculated above from reconstructions\n",
    "slicethickness = 888\n",
    "otolithregion = Reconstructions[whichone][position_ap - slicethickness // 2:position_ap + slicethickness // 2,\n",
    "                                          position_lt - slicethickness // 2:position_lt + slicethickness // 2,\n",
    "                                          position_dv - slicethickness // 2:position_dv + slicethickness // 2].compute()\n",
    "for ax in range(3):\n",
    "    plt.subplot(1, 3, ax + 1)\n",
    "    plt.imshow(dask.array.max(otolithregion, axis=ax))\n",
    "    plt.suptitle('%s/%s: MIP from AP %s:%s, LT %s:%s, DV %s:%s' % (Data.Fish[whichone],\n",
    "                                                                   Data.Scan[whichone],\n",
    "                                                                   position_ap - slicethickness // 2, position_ap + slicethickness // 2,\n",
    "                                                                   position_lt - slicethickness // 2, position_lt + slicethickness // 2,\n",
    "                                                                   position_dv - slicethickness // 2, position_dv + slicethickness // 2))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold(otolithregion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold(otolithregion[otolithregion > 65])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(otolithregion[444])\n",
    "plt.imshow(otolithregion[444] > 106, cmap='viridis', alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file = open('largest', 'wb')\n",
    "# pickle.dump(largest,file)\n",
    "# file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file = open('largest', 'rb')\n",
    "# largest = pickle.load(file)\n",
    "# file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# largest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Make file smaller for testing reasons\n",
    "# subsample = 4\n",
    "# largest_smaller = largest[::subsample, ::subsample, ::subsample]\n",
    "# largest_smaller.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsample = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vmin = threshold(otolithregion)\n",
    "print(vmin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load into K3D\n",
    "plt_volume = k3d.volume(otolithregion[::subsample, ::subsample, ::subsample].astype(numpy.float16),\n",
    "                        color_range=[vmin, 2**8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the otolith\n",
    "plot = k3d.plot()\n",
    "plot += plt_volume\n",
    "plot.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
