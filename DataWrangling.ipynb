{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handle the 'data' of the fishes\n",
    "Wrestle with our data and Mikkis XLS sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "import os\n",
    "import glob\n",
    "import pandas\n",
    "import imageio\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib_scalebar.scalebar import ScaleBar\n",
    "import seaborn\n",
    "import dask\n",
    "import dask_image.imread\n",
    "from dask.distributed import Client, LocalCluster\n",
    "from numcodecs import Blosc\n",
    "import skimage\n",
    "from tqdm import notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dask temporarry files go to D:\\tmp\n"
     ]
    }
   ],
   "source": [
    "# Set dask temporary folder\n",
    "# Do this before creating a client: https://stackoverflow.com/a/62804525/323100\n",
    "import tempfile\n",
    "if 'Linux' in platform.system():\n",
    "    tmp = os.path.join(os.sep, 'media', 'habi', 'Fast_SSD')\n",
    "elif 'Darwin' in platform.system():\n",
    "    tmp = tempfile.gettempdir()\n",
    "else:\n",
    "    if 'anaklin' in platform.node():\n",
    "        tmp = os.path.join('F:\\\\')\n",
    "    else:\n",
    "        tmp = os.path.join('D:\\\\')\n",
    "dask.config.set({'temporary_directory': os.path.join(tmp, 'tmp')})\n",
    "print('Dask temporarry files go to %s' % dask.config.get('temporary_directory'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\haberthu\\Miniconda3\\lib\\site-packages\\distributed\\node.py:160: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 63712 instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Start cluster and client now, after setting tempdir\n",
    "cluster = LocalCluster(n_workers=8)\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You can seee what DASK is doing at \"http://localhost:63712/status\"\n"
     ]
    }
   ],
   "source": [
    "print('You can seee what DASK is doing at \"http://localhost:%s/status\"' % client.scheduler_info()['services']['dashboard'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Ignore warnings in the notebook\n",
    "# import warnings\n",
    "# warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up figure defaults\n",
    "plt.rc('image', cmap='gray', interpolation='nearest')  # Display all images in b&w and with 'nearest' interpolation\n",
    "plt.rcParams['figure.figsize'] = (16, 9)  # Size up figures a bit\n",
    "plt.rcParams['figure.dpi'] = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup scale bar defaults\n",
    "plt.rcParams['scalebar.location'] = 'lower right'\n",
    "plt.rcParams['scalebar.frameon'] = False\n",
    "plt.rcParams['scalebar.color'] = 'white'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display all plots identically\n",
    "lines = 3\n",
    "# And then do something like\n",
    "# plt.subplot(lines, numpy.ceil(len(Data) / float(lines)), c + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are loading all the data from D:\\Results\\EAWAG\n"
     ]
    }
   ],
   "source": [
    "# Different locations if running either on Linux or Windows\n",
    "FastSSD = False\n",
    "# to speed things up significantly\n",
    "if 'Linux' in platform.system():\n",
    "    if FastSSD:\n",
    "        BasePath = os.path.join(os.sep, 'media', 'habi', 'Fast_SSD')\n",
    "    else:\n",
    "        BasePath = os.path.join(os.sep, 'home', 'habi', '1272')\n",
    "elif 'Darwin' in platform.system():\n",
    "    FastSSD = False\n",
    "    BasePath = os.path.join('/Users/habi/Dev/EAWAG/Data')\n",
    "elif 'Windows' in platform.system():\n",
    "    if FastSSD:\n",
    "        BasePath = os.path.join('F:\\\\')\n",
    "    else:\n",
    "        if 'anaklin' in platform.node():\n",
    "            BasePath = os.path.join('S:\\\\')\n",
    "        else:\n",
    "            BasePath = os.path.join('D:\\\\Results')\n",
    "Root = os.path.join(BasePath, 'EAWAG')\n",
    "print('We are loading all the data from %s' % Root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pixelsize(logfile):\n",
    "    \"\"\"Get the pixel size from the scan log file\"\"\"\n",
    "    pixelsize=None    \n",
    "    with open(logfile, 'r') as f:\n",
    "        for line in f:\n",
    "            if 'Image Pixel' in line and 'Scaled' not in line:\n",
    "                pixelsize = float(line.split('=')[1])\n",
    "    return(pixelsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_projectionsize(logfile):\n",
    "    \"\"\"How big did we set the camera?\"\"\"\n",
    "    with open(logfile, 'r') as f:\n",
    "        for line in f:\n",
    "            if 'Number Of Rows' in line:\n",
    "                y = int(line.split('=')[1])\n",
    "            if 'Number Of Columns' in line:\n",
    "                x = int(line.split('=')[1])                \n",
    "    return(x*y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filter(logfile):\n",
    "    \"\"\"Get the filter we used whole scanning from the scan log file\"\"\"\n",
    "    with open(logfile, 'r') as f:\n",
    "        for line in f:\n",
    "            if 'Filter=' in line:\n",
    "                whichfilter = line.split('=')[1].strip()\n",
    "    return(whichfilter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_exposuretime(logfile):\n",
    "    \"\"\"Get the exposure time size from the scan log file\"\"\"\n",
    "    with open(logfile, 'r') as f:\n",
    "        for line in f:\n",
    "            if 'Exposure' in line:\n",
    "                exposuretime = int(line.split('=')[1])\n",
    "    return(exposuretime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ringartefact(logfile):\n",
    "    \"\"\"Get the ring artefact correction from the  scan log file\"\"\"\n",
    "    with open(logfile, 'r') as f:\n",
    "        for line in f:\n",
    "            if 'Ring Artifact' in line:\n",
    "                ringartefactcorrection = int(line.split('=')[1])\n",
    "    return(ringartefactcorrection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reconstruction_grayvalue(logfile):\n",
    "    grayvalue = None\n",
    "    \"\"\"How did we map the brightness of the reconstructions?\"\"\"\n",
    "    with open(logfile, 'r') as f:\n",
    "        for line in f:\n",
    "            if 'Maximum for' in line:\n",
    "                grayvalue = float(line.split('=')[1])\n",
    "    return(grayvalue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_beamhardening(logfile):\n",
    "    \"\"\"Get the beamhardening correction from the  scan log file\"\"\"\n",
    "    with open(logfile, 'r') as f:\n",
    "        for line in f:\n",
    "            if 'Hardening' in line:\n",
    "                beamhardeningcorrection = int(line.split('=')[1])\n",
    "    return(beamhardeningcorrection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rotationstep(logfile):\n",
    "    \"\"\"Get the rotation step from the scan log file\"\"\"\n",
    "    with open(logfile, 'r') as f:\n",
    "        for line in f:\n",
    "            if 'Rotation Step' in line:\n",
    "                rotstep = float(line.split('=')[1])\n",
    "    return(rotstep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_frameaveraging(logfile):\n",
    "    \"\"\"Get the frame averaging from the scan log file\"\"\"\n",
    "    with open(logfile, 'r') as f:\n",
    "        for line in f:\n",
    "            if 'Averaging' in line:\n",
    "                avg = line.split('=')[1]\n",
    "    return(avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_machine(logfile):\n",
    "    \"\"\"Get the machine we used to scan\"\"\"\n",
    "    with open(logfile, 'r') as f:\n",
    "        for line in f:\n",
    "            if 'Scanner' in line:\n",
    "                machine = line.split('=')[1].strip()\n",
    "    return(machine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scantime(logfile):\n",
    "    \"\"\"How long did we scan?\"\"\"\n",
    "    with open(logfile, 'r') as f:\n",
    "        for line in f:\n",
    "            if 'Scan duration' in line:\n",
    "                time = line.split('=')[1].strip()\n",
    "    return(pandas.to_timedelta(time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stacks(logfile):\n",
    "    \"\"\"How many stacks/connected scans did we make?\"\"\"\n",
    "    stacks = 1\n",
    "    with open(logfile, 'r') as f:\n",
    "        for line in f:\n",
    "            if 'conn' in line:\n",
    "                stacks = int(line.split('=')[1])\n",
    "    return(stacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scandate(logfile, verbose=False):\n",
    "    \"\"\"When did we scan the fish?\"\"\"\n",
    "    with open(logfile, 'r') as f:\n",
    "        for line in f:\n",
    "            if 'Study Date and Time' in line:\n",
    "                if verbose:\n",
    "                    print('Found \"date\" line: %s' % line.strip())\n",
    "                datestring = line.split('=')[1].strip().replace('  ', ' ')\n",
    "                if verbose:\n",
    "                    print('The date string is: %s' % datestring)\n",
    "                date = pandas.to_datetime(datestring , format='%d %b %Y %Hh:%Mm:%Ss')\n",
    "                if verbose:\n",
    "                    print('Parsed to: %s' % date)\n",
    "                (date)\n",
    "    return(date.isoformat())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_git_hash():\n",
    "    '''\n",
    "    Get the current git hash from the repository.\n",
    "    Based on http://stackoverflow.com/a/949391/323100 and\n",
    "    http://stackoverflow.com/a/18283905/323100\n",
    "    '''\n",
    "    from subprocess import Popen, PIPE\n",
    "    import os\n",
    "    gitprocess = Popen(['git',\n",
    "                        '--git-dir',\n",
    "                        os.path.join(os.getcwd(), '.git'),\n",
    "                        'rev-parse',\n",
    "                        '--short',\n",
    "                        '--verify',\n",
    "                        'HEAD'],\n",
    "                       stdout=PIPE)\n",
    "    (output, _) = gitprocess.communicate()\n",
    "    return output.strip().decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Make directory for output\n",
    "# OutPutDir = os.path.join(os.getcwd(), 'Output', get_git_hash())\n",
    "# print('We are saving all the output to %s' % OutPutDir)\n",
    "# os.makedirs(OutPutDir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make us a dataframe for saving all that we need\n",
    "Data = pandas.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get *all* log files\n",
    "# Sort them by time, not name\n",
    "Data['LogFile'] = [f for f in sorted(glob.glob(os.path.join(Root, '**', '*.log'), recursive=True), key=os.path.getmtime)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all folders\n",
    "Data['Folder'] = [os.path.dirname(f) for f in Data['LogFile']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "- 105005_104015\\proj\\105005_104015~00.log is missing matching reconstructions\n",
      "[]\n",
      "- 105005_104015\\proj\\105005_104015~01.log is missing matching reconstructions\n",
      "[]\n",
      "- 105005_104015\\proj\\105005_104015~02.log is missing matching reconstructions\n",
      "[]\n",
      "- 105005_104015\\proj\\105005_104015~03.log is missing matching reconstructions\n",
      "[]\n",
      "- 105005_104015\\proj\\105005_104015~04.log is missing matching reconstructions\n",
      "[]\n",
      "- 105005_104015\\proj\\105005_104015.log is missing matching reconstructions\n",
      "[]\n",
      "- 104671_156645\\proj\\104671_156645~00.log is missing matching reconstructions\n",
      "[]\n",
      "- 104671_156645\\proj\\104671_156645~01.log is missing matching reconstructions\n",
      "[]\n",
      "- 104671_156645\\proj\\104671_156645~02.log is missing matching reconstructions\n",
      "[]\n",
      "- 104671_156645\\proj\\104671_156645.log is missing matching reconstructions\n"
     ]
    }
   ],
   "source": [
    "# Check for samples which are not yet reconstructed\n",
    "for c, row in Data.iterrows():\n",
    "    # Iterate over every 'proj' folder\n",
    "    if 'proj' in row.Folder:\n",
    "        if not 'TScopy' in row.Folder and not 'PR' in row.Folder:\n",
    "            # If there's nothing with 'rec*' on the same level, then tell us        \n",
    "            if not glob.glob(row.Folder.replace('proj', 'rec')):\n",
    "                print(glob.glob(row.Folder.replace('proj', 'rec')))\n",
    "                print('- %s is missing matching reconstructions' % row.LogFile[len(Root)+1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get rid of all non-rec logfiles\n",
    "for c, row in Data.iterrows():\n",
    "    if 'rec' not in row.Folder:\n",
    "        Data.drop([c], inplace=True)\n",
    "    elif 'rectmp.log' in row.LogFile:\n",
    "        Data.drop([c], inplace=True)\n",
    "# Reset dataframe to something that we would get if we only would have loaded the 'rec' files\n",
    "Data = Data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate us some meaningful colums\n",
    "Data['Fish'] = [l[len(Root)+1:].split(os.sep)[0] for l in Data['LogFile']]\n",
    "Data['Scan'] = ['_'.join(l[len(Root)+1:].split(os.sep)[1:-1]) for l in Data['LogFile']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the file names of the reconstructions\n",
    "Data['Reconstructions'] = [sorted(glob.glob(os.path.join(f, '*rec0*.png'))) for f in Data['Folder']]\n",
    "Data['Number of reconstructions'] = [len(r) for r in Data.Reconstructions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 108 folders with reconstructions\n"
     ]
    }
   ],
   "source": [
    "# Drop samples which have not been reconstructed yet\n",
    "# Based on https://stackoverflow.com/a/13851602\n",
    "# for c,row in Data.iterrows():\n",
    "#     if not row['Number of reconstructions']:\n",
    "#         print('%s contains no PNG files, we might be currently reconstructing it' % row.Folder)\n",
    "Data = Data[Data['Number of reconstructions'] > 0]\n",
    "Data.reset_index(drop=True, inplace=True)\n",
    "print('We have %s folders with reconstructions' % (len(Data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get parameters to doublecheck from logfiles\n",
    "Data['Voxelsize'] = [get_pixelsize(log) for log in Data['LogFile']]\n",
    "Data['Filter'] = [get_filter(log) for log in Data['LogFile']]\n",
    "Data['Exposuretime'] = [get_exposuretime(log) for log in Data['LogFile']]\n",
    "Data['Scanner'] = [get_machine(log) for log in Data['LogFile']]\n",
    "Data['Averaging'] = [get_frameaveraging(log) for log in Data['LogFile']]\n",
    "Data['ProjectionSize'] = [get_projectionsize(log) for log in Data['LogFile']]\n",
    "Data['RotationStep'] = [get_rotationstep(log) for log in Data['LogFile']]\n",
    "Data['CameraWindow'] = [round((ps ** 0.5)/100)*100  for ps in Data['ProjectionSize']]\n",
    "Data['Grayvalue'] = [get_reconstruction_grayvalue(log) for log in Data['LogFile']]\n",
    "Data['RingartefactCorrection'] = [get_ringartefact(log) for log in Data['LogFile']]\n",
    "Data['BeamHardeningCorrection'] = [get_beamhardening(log) for log in Data['LogFile']]\n",
    "Data['Scan date'] = [get_scandate(log) for log in Data['LogFile']]\n",
    "Data['Scan time'] = [get_scantime(log) for log in Data['LogFile']]\n",
    "Data['Stacks'] = [get_stacks(log) for log in Data['LogFile']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data['Scan time total'] = [ st * stk  for st, stk in zip(Data['Scan time'], Data['Stacks'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a text file for each rec-folder, in which we can note what's going on with the fish\n",
    "# Generate filename\n",
    "for c,row in Data.iterrows():\n",
    "    Data.at[c, 'commentsfile'] = os.path.join(os.path.dirname(row.Folder),\n",
    "                                              row.Fish + '.' + row.Scan + '.md')\n",
    "# Create actual file on disk\n",
    "for c,row in Data.iterrows():\n",
    "    # Only do this if the file does not already exist\n",
    "    if not os.path.exists(row.commentsfile):\n",
    "        with open(row.commentsfile, 'w', encoding='utf-8') as f:\n",
    "            f.write('# Fish %s, Scan %s\\n\\n' % (row.Fish, row.Scan))\n",
    "            f.write('This fish was scanned on %s on the %s, with a voxel size of %s μm.\\n\\n'\n",
    "                    % (row['Scan date'], row.Scanner, numpy.round(row.Voxelsize, 2)))\n",
    "            f.write('## Comments')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # https://www.geeksforgeeks.org/iterating-over-rows-and-columns-in-pandas-dataframe/\n",
    "# columns = list(Data)\n",
    "# columns.remove('Folder') \n",
    "# columns.remove('Fish')\n",
    "# columns.remove('LogFile')\n",
    "# columns.remove('Reconstructions')\n",
    "# columns.remove('Number of reconstructions')\n",
    "# columns.remove('Grayvalue')\n",
    "# columns.remove('Scan time')\n",
    "# columns.remove('Scan time total')\n",
    "# columns.remove('Scan date')\n",
    "# print(columns)\n",
    "# for col in columns:\n",
    "#     print(col)\n",
    "#     print(Data[col].unique())\n",
    "#     print(80*'-')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check voxel sizes (*rounded* to two after-comma values)\n",
    "# # If different, spit out which values\n",
    "# roundto = 2\n",
    "# if len(Data['Voxelsize'].round(roundto).unique()) > 1:\n",
    "#     print('We scanned all datasets with %s different voxel sizes' % len(Data['Voxelsize'].round(roundto).unique()))\n",
    "#     for vs in sorted(Data['Voxelsize'].round(roundto).unique()):\n",
    "#         print('-', vs, 'um for ', end='')\n",
    "#         for c, row in Data.iterrows():\n",
    "#             if float(vs) == round(row['Voxelsize'], roundto):\n",
    "#                 print(os.path.join(row['Fish'], row['Scan']), end=', ')\n",
    "#         print('')\n",
    "# else:\n",
    "#     print('We scanned all datasets with equal voxel size, namely %s um.' % float(Data['Voxelsize'].round(roundto).unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if len(Data['Grayvalue'].unique()) > 1:\n",
    "#     print('We reconstructed the datasets with different maximum gray values, namely')\n",
    "#     for gv in Data['Grayvalue'].unique():\n",
    "#         print(gv, 'for Samples ', end='')\n",
    "#         for c, row in Data.iterrows():\n",
    "#             if float(gv) == row['Grayvalue']:\n",
    "#                 print(os.path.join(row['Fish'], row['Scan']), end=', ')\n",
    "#         print('')\n",
    "# else:\n",
    "#     print('We reconstructed all datasets with equal maximum gray value, namely %s.' % Data['Grayvalue'].unique()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data[['Fish', 'Scan',\n",
    "#       'Voxelsize', 'Scanner',\n",
    "#       'Scan date', 'CameraWindow', 'RotationStep', 'Averaging',\n",
    "#       'Scan time', 'Stacks', 'Scan time total']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In total, we scanned for 360 hours and 12 minutes)\n",
      "\t - Of these, we scanned 266 hours and 41 minutes on the SkyScan2214,for 101 scans\n",
      "\t - Of these, we scanned 93 hours and 31 minutes on the SkyScan1272,for 7 scans\n"
     ]
    }
   ],
   "source": [
    "# Get an overview over the total scan time\n",
    "# Nice output based on https://stackoverflow.com/a/8907407/323100\n",
    "total_seconds = int(Data['Scan time total'].sum().total_seconds())\n",
    "hours, remainder = divmod(total_seconds,60*60)\n",
    "minutes, seconds = divmod(remainder,60)\n",
    "print('In total, we scanned for %s hours and %s minutes)' % (hours, minutes))\n",
    "for machine in Data['Scanner'].unique():\n",
    "    total_seconds = int(Data[Data['Scanner'] == machine]['Scan time total'].sum().total_seconds())\n",
    "    hours, remainder = divmod(total_seconds,60*60)\n",
    "    minutes, seconds = divmod(remainder,60)\n",
    "    print('\\t - Of these, we scanned %s hours and %s minutes on the %s,'\n",
    "          'for %s scans' % (hours,\n",
    "                            minutes,\n",
    "                            machine,\n",
    "                            len(Data[Data['Scanner'] == machine])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data[['Fish', 'Scan',\n",
    "      'Voxelsize', 'Scanner',\n",
    "      'Scan date', 'CameraWindow', 'RotationStep', 'Averaging', 'Scan time', 'Stacks' ]].to_excel('Details.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data[['Fish', 'Scan',\n",
    "      'Voxelsize', 'Scanner',\n",
    "      'Scan date', 'CameraWindow',\n",
    "      'RotationStep', 'Averaging', 'Scan time', 'Stacks' ]].to_excel(os.path.join(Root,'Details.xlsx'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading in D:\\Results\\EAWAG\\17.11.2021_CTscanFishList.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Read Mikkis datafile\n",
    "MikkisFile = sorted(glob.glob(os.path.join(Root, 'X_ArchiveFiles', '*CTscanFishList.xlsx')))[0]\n",
    "# Read excel file and use the first column as index\n",
    "print('Reading in %s' % MikkisFile)\n",
    "DataMikki = pandas.read_excel(MikkisFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Fishec</th>\n",
       "      <th>FieldID</th>\n",
       "      <th>OtherID</th>\n",
       "      <th>ReplacementID</th>\n",
       "      <th>Length(cm)</th>\n",
       "      <th>TemporaryJar</th>\n",
       "      <th>Genus</th>\n",
       "      <th>Species</th>\n",
       "      <th>Ecology</th>\n",
       "      <th>Scan date</th>\n",
       "      <th>...</th>\n",
       "      <th>UpperOralJaw</th>\n",
       "      <th>LowerOralJaw</th>\n",
       "      <th>PharyngealJawScan</th>\n",
       "      <th>UpperPharyngealJaw</th>\n",
       "      <th>LowerPharyngealJaw</th>\n",
       "      <th>ScanComments</th>\n",
       "      <th>QualityChecked</th>\n",
       "      <th>Unnamed: 19</th>\n",
       "      <th>SpecimenReturned</th>\n",
       "      <th>Comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>103635</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt; 7</td>\n",
       "      <td>&lt; 7cm</td>\n",
       "      <td>\"Astatotilapia\"</td>\n",
       "      <td>nubila swamp blue</td>\n",
       "      <td>insectivore</td>\n",
       "      <td>2021-02-08T14:24:12</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>yes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2-3 inner rows of tricuspid teeth, low meandep...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>103658</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "      <td>Mark4</td>\n",
       "      <td>\"Astatotilapia\"</td>\n",
       "      <td>nubila swamp red</td>\n",
       "      <td>insectivore</td>\n",
       "      <td>2021-04-23T10:36:23</td>\n",
       "      <td>...</td>\n",
       "      <td>load and enter info</td>\n",
       "      <td>NaN</td>\n",
       "      <td>yes</td>\n",
       "      <td>load and enter info</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2 inner rows of tricuspid teeth</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14</td>\n",
       "      <td>Mark4</td>\n",
       "      <td>\"Astatotilapia\"</td>\n",
       "      <td>nubila rocks</td>\n",
       "      <td>insectivore</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>load and enter info</td>\n",
       "      <td>NaN</td>\n",
       "      <td>yes</td>\n",
       "      <td>load and enter info</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1-2 inner rows of tricuspid teeth</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84713</td>\n",
       "      <td>63</td>\n",
       "      <td>NaN</td>\n",
       "      <td>need replacement?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\"Astatotilapia\"</td>\n",
       "      <td>velifer</td>\n",
       "      <td>insectivore</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>81022</td>\n",
       "      <td>KC-31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LE</td>\n",
       "      <td>\"Harpagochromis\"</td>\n",
       "      <td>squamipinnis</td>\n",
       "      <td>piscivore</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>load and enter info</td>\n",
       "      <td>NaN</td>\n",
       "      <td>yes</td>\n",
       "      <td>load and enter info</td>\n",
       "      <td>NaN</td>\n",
       "      <td>inner rows uni, bi? and tricuspid teeth</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Fishec FieldID OtherID      ReplacementID Length(cm) TemporaryJar  \\\n",
       "0  103635     NaN     NaN                NaN        < 7        < 7cm   \n",
       "1  103658     NaN     NaN                NaN          8        Mark4   \n",
       "2   11500     NaN     NaN                NaN         14        Mark4   \n",
       "3   84713      63     NaN  need replacement?        NaN          NaN   \n",
       "4   81022   KC-31     NaN                NaN        NaN           LE   \n",
       "\n",
       "              Genus            Species      Ecology            Scan date  ...  \\\n",
       "0   \"Astatotilapia\"  nubila swamp blue  insectivore  2021-02-08T14:24:12  ...   \n",
       "1   \"Astatotilapia\"   nubila swamp red  insectivore  2021-04-23T10:36:23  ...   \n",
       "2   \"Astatotilapia\"       nubila rocks  insectivore                  NaN  ...   \n",
       "3   \"Astatotilapia\"            velifer  insectivore                  NaN  ...   \n",
       "4  \"Harpagochromis\"       squamipinnis    piscivore                  NaN  ...   \n",
       "\n",
       "          UpperOralJaw LowerOralJaw PharyngealJawScan   UpperPharyngealJaw  \\\n",
       "0                  NaN          NaN               yes                  NaN   \n",
       "1  load and enter info          NaN               yes  load and enter info   \n",
       "2  load and enter info          NaN               yes  load and enter info   \n",
       "3                  NaN          NaN               NaN                  NaN   \n",
       "4  load and enter info          NaN               yes  load and enter info   \n",
       "\n",
       "  LowerPharyngealJaw                                       ScanComments  \\\n",
       "0                NaN  2-3 inner rows of tricuspid teeth, low meandep...   \n",
       "1                NaN                    2 inner rows of tricuspid teeth   \n",
       "2                NaN                  1-2 inner rows of tricuspid teeth   \n",
       "3                NaN                                                NaN   \n",
       "4                NaN            inner rows uni, bi? and tricuspid teeth   \n",
       "\n",
       "  QualityChecked Unnamed: 19 SpecimenReturned  Comments  \n",
       "0            NaN         NaN              NaN       NaN  \n",
       "1            NaN         NaN              NaN       NaN  \n",
       "2            NaN         NaN              NaN       NaN  \n",
       "3            NaN         NaN              NaN       NaN  \n",
       "4            NaN         NaN              NaN       NaN  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DataMikki.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the fish we look at and display all the info we know about it\n",
    "# Set a substring you're looking for to the variable below\n",
    "# In which jar can we find it?\n",
    "fish = 'MA3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*MA3*: Found on disk in D:\\Results\\EAWAG\\MA31\n",
      "*MA3*: Found on disk in D:\\Results\\EAWAG\\MA38\n"
     ]
    }
   ],
   "source": [
    "# Do we have something from this fish on disk?\n",
    "ondisk = glob.glob(os.path.join(Root, '*%s*' % fish))\n",
    "if len(ondisk):\n",
    "    for found in ondisk:\n",
    "        print('*%s*: Found on disk in %s' % (fish, found))\n",
    "        foundondisk = 1\n",
    "else:\n",
    "    print('*%s*: Nothing found in %s' % (fish, Root))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*MA3*: Sample MA31/rec_rescan was scanned on 2021-08-20T14:28:47\n",
      "*MA3*: Sample MA38/rec was scanned on 2021-09-06T11:35:24\n",
      "*MA3*: Sample MA31/rec_rescan_rereconstruct_OJ was scanned on 2021-08-20T14:28:47\n",
      "*MA3*: Sample MA31/head_rec was scanned on 2021-10-19T11:15:30\n"
     ]
    }
   ],
   "source": [
    "# Did we scan it already?\n",
    "found = 0\n",
    "for c, row in Data.iterrows():\n",
    "    if fish in row.Fish:\n",
    "        print('*%s*: Sample %s/%s was scanned on %s' % (fish, row['Fish'], row['Scan'], row['Scan date']))\n",
    "        found = 1\n",
    "if not found:\n",
    "    if foundondisk:\n",
    "        print('*%s*: We have a folder (%s) for this sample, but nothing in the dataframe, so it probably is all good' % (fish, ondisk[0]))\n",
    "        print('Check the folder to be shure')\n",
    "    else:\n",
    "        print('*%s*: Nothing about this sample is found in our dataframe' % fish)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'*MA3*: Nothing found in D:\\\\Results\\\\EAWAG\\\\FullHeadList.txt'"
      ]
     },
     "execution_count": 433,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Can we find it in FullHeadList.txt?\n",
    "def findinFullHeadList(sample):\n",
    "    ''' Look for the sample in the FullHeadList.txt file'''\n",
    "    fullheadlist = glob.glob(os.path.join(Root, 'FullHeadList.*'))[0]    \n",
    "    found = 0\n",
    "    with open(fullheadlist, 'r') as f:\n",
    "        for line in f:\n",
    "            if str(sample) in line:\n",
    "                print(line.strip())\n",
    "                found = 1\n",
    "    if not found:\n",
    "        return('*%s*: Nothing found in %s' % (sample, fullheadlist))\n",
    "    else:\n",
    "        return(None)\n",
    "findinFullHeadList(fish)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*MA3*: A fish called 14175/MA38/80351/ should be found in jar \"length=10.5 cm\" (Mark1))\n",
      "*MA3*: A fish called 80344/MA31/ should be found in jar \"length=11 cm\" (Mark6))\n"
     ]
    }
   ],
   "source": [
    "# In which jar should it be/go?\n",
    "foundfishes = 0\n",
    "for d, row in DataMikki.iterrows():\n",
    "    if (str(fish).lower() in str(row.Fishec).lower()) or \\\n",
    "    (str(fish).lower() in str(row.FieldID).lower()) or \\\n",
    "    (str(fish).lower() in str(row.OtherID).lower()) or \\\n",
    "    (str(fish).lower() in str(row.ReplacementID).lower()):\n",
    "        foundfishes = (row.Fishec, row.FieldID, row.OtherID, row.ReplacementID)\n",
    "        # remove nan from the list of hits\n",
    "        foundfishes = [str(x).lower() for x in foundfishes if pandas.isnull(x) == False]\n",
    "        print('*%s*: A fish called ' % fish, end='')        \n",
    "        if len(foundfishes) > 1:\n",
    "            for found in foundfishes:\n",
    "                print(found.upper(), end='/')\n",
    "        else:\n",
    "            print(foundfishes[0].upper(), end='')\n",
    "        print(' should be found in jar \"length=%s cm\" (%s))' % (row['Length(cm)'],\n",
    "                                                                row['TemporaryJar']))\n",
    "if not foundfishes:\n",
    "    print('*%s*: Nothing found in %s' % (fish, MikkisFile))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "103    2021-11-29T20:27:08\n",
       "Name: Scan date, dtype: object"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data[Data['Fish']=='IG94']['Scan date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11965 needs to be rescanned\n",
      "\t- We comment: Oral jaw has artifacts in segmentation, can we try re reconstruction on the rescan - 21.09.2021, ML\n",
      "\n",
      "\t- 11965 should be found in jar \"length=13.5 cm\" (Mark2))\n",
      "--------------------------------------------------------------------------------\n",
      "MA31 needs to be rescanned\n",
      "\t- We comment: The rescan shows artifacts on oral jaw teeth which stick them together - QualityCheck png. \n",
      "\t- 80344/MA31 should be found in jar \"length=11 cm\" (Mark6))\n",
      "--------------------------------------------------------------------------------\n",
      "14269 needs to be rescanned\n",
      "\t- We comment: Re-reconstructed, to alleviate artefacts, but since 'very tip (of PJ) is not complete' in scan, we might need to rescan it.\n",
      "\n",
      "\t- 14269 should be found in jar \"length=6.5 cm\" (nan))\n",
      "--------------------------------------------------------------------------------\n",
      "14269 needs to be rescanned\n",
      "\t- We comment: 13.10.2021: I misread PJ as OJ, so re-reconstruction did not help :( We will have to rescan this fish.\n",
      "\t- 14269 should be found in jar \"length=6.5 cm\" (nan))\n",
      "--------------------------------------------------------------------------------\n",
      "TS03 needs to be rescanned\n",
      "\t- We comment: Really blurred oral jaw - reconstruct or best case rescan at lower voxel size if possible\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "12807 needs to be rescanned\n",
      "\t- We comment: The segments towards the pharyngeal jaw thus show considerable motion artefacts and have to be rescanned.\n",
      "\n",
      "\t- 10400/12807 should be found in jar \"length=9.5 cm\" (Mark1))\n",
      "--------------------------------------------------------------------------------\n",
      "10619 needs to be rescanned\n",
      "\t- We comment: He is using this scan - as discussed - to scan the complete head with different binnings and rescans the oral jaw in the same go.\n",
      "\t- 10619 should be found in jar \"length=10 cm\" (Mark3))\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Find fishes to be rescanned, e.g. 'grep' through the comments files.\n",
    "for c, row in Data.iterrows():\n",
    "    with open(row.commentsfile, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            if 'resc' in line and 'rec_rescan' not in line:\n",
    "                print(row.Fish, 'needs to be rescanned')\n",
    "                print('\\t- We comment: %s' % line)\n",
    "                for d, rowmikki in DataMikki.iterrows():\n",
    "                    if (str(row.Fish).lower() in str(rowmikki.Fishec).lower()) or \\\n",
    "                        (str(row.Fish).lower() in str(rowmikki.FieldID).lower()) or \\\n",
    "                        (str(row.Fish).lower() in str(rowmikki.OtherID).lower()) or \\\n",
    "                        (str(row.Fish).lower() in str(rowmikki.ReplacementID).lower()):\n",
    "                        foundfishes = (rowmikki.Fishec, rowmikki.FieldID, rowmikki.OtherID, rowmikki.ReplacementID)\n",
    "                        # remove nan from the list of hits\n",
    "                        foundfishes = [str(x).lower() for x in foundfishes if pandas.isnull(x) == False]\n",
    "                        print('\\t- ', end='')\n",
    "                        if len(foundfishes) > 1:\n",
    "                            for found in foundfishes[:-1]:\n",
    "                                print(found.upper(), end='/')\n",
    "                        print('%s should be found in jar \"length=%s cm\" (%s))' % (foundfishes[-1].upper(),\n",
    "                                                                                  rowmikki['Length(cm)'],\n",
    "                                                                                  rowmikki['TemporaryJar']))\n",
    "                print(80*'-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fish 11965 needs to be re-reconstructed, because in its commentfile we write \"Oral jaw has artifacts in segmentation, can we try re reconstruction on the rescan - 21.09.2021, ML\n",
      "\"\n",
      "Fish IG156 needs to be re-reconstructed, because in its commentfile we write \"26.10.2021 - re reconstruction OJ artifacts - quality check segmentation file uploaded\"\n",
      "Fish 14269 needs to be re-reconstructed, because in its commentfile we write \"Re-reconstructed, to alleviate artefacts, but since 'very tip (of PJ) is not complete' in scan, we might need to rescan it.\n",
      "\"\n",
      "Fish 14269 needs to be re-reconstructed, because in its commentfile we write \"13.10.2021: I misread PJ as OJ, so re-reconstruction did not help :( We will have to rescan this fish.\"\n",
      "Fish TS03 needs to be re-reconstructed, because in its commentfile we write \"Really blurred oral jaw - reconstruct or best case rescan at lower voxel size if possible\n",
      "\"\n",
      "Fish ZU12 needs to be re-reconstructed, because in its commentfile we write \"12.10.2021 - OJ bright artifacts - re reconstruct?\n",
      "\"\n",
      "Fish ZU12 needs to be re-reconstructed, because in its commentfile we write \"13.10.2021: Re-reconstructed with slightly varying overlap and brightness values\"\n",
      "Fish 11807 needs to be re-reconstructed, because in its commentfile we write \"- (22.08.2021, try  re reconstructing to better later segmentation, OJ artifacts, PJ is good)\n",
      "\"\n",
      "Fish 11557 needs to be re-reconstructed, because in its commentfile we write \"artifacts on lower OJ - try re reconstruction\n",
      "\"\n",
      "Fish IG80 needs to be re-reconstructed, because in its commentfile we write \"Oral jaw - slight artifacts, would re reconstruction help? - QualityCheck png available in Oct2021 folder.\"\n",
      "Fish ZU12 needs to be re-reconstructed, because in its commentfile we write \"# Fish ZU12, Scan rec_reconstruct\n",
      "\"\n",
      "Fish 10794 needs to be re-reconstructed, because in its commentfile we write \"# Fish 10794, Scan rec_rescan_rereconstruct_OJ\n",
      "\"\n",
      "Fish MA31 needs to be re-reconstructed, because in its commentfile we write \"# Fish MA31, Scan rec_rescan_rereconstruct_OJ\n",
      "\"\n",
      "Fish 109188 needs to be re-reconstructed, because in its commentfile we write \"2.11.21, DH: The fish was not aligned nicely perpendicular in the sample holder. I re-reconstructed the data *without* a ROI. Parts of the OJ might still be outside of the visible region. @Mikki, can you double-check?\"\n"
     ]
    }
   ],
   "source": [
    "# Find fishes to be rescanned, e.g. 'grep' through the comments files.\n",
    "for c,row in Data.iterrows():\n",
    "    with open(row.commentsfile, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            if 'recons' in line and 'rec_rerecon' not in line:\n",
    "                print('Fish', row.Fish, 'needs to be re-reconstructed, because in its commentfile we write \"%s\"' % line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fish 13420 needs to be looked at in detail, because in its commentfile we write \"DH 2.11.21: This was a naming SNAFU from David's side during scanning. I renamed the files, but forgot to update the relevant part in the log file. There's an encrypted/immutable copy of the log file (*.enc), which I restored, you can see this as `12807~00_recovered.log` in which the originally wrong prefix is shown on line 15. The 'process' is shown in the Screenshots also in this folder. I renamed the files now correctly to a prefix of 13420.\n",
      "\"\n",
      "Fish 312 needs to be looked at in detail, because in its commentfile we write \"We figured this out on 19.10.2021, the fish has a huge '312' label and a much smaller, barely readable label, which Mikki will update in the encompassing Excel sheet. 21.10.2021 DH\"\n",
      "Fish 11807 needs to be looked at in detail, because in its commentfile we write \"- Mikki and David need to discuss this in detail.\"\n",
      "Fish 10619 needs to be looked at in detail, because in its commentfile we write \"21.10.2021: David completely b0rked this scan, the oral jaw is really not present.\n",
      "\"\n",
      "Fish 10619 needs to be looked at in detail, because in its commentfile we write \"He is using this scan - as discussed - to scan the complete head with different binnings and rescans the oral jaw in the same go.\"\n",
      "Fish 109188 needs to be looked at in detail, because in its commentfile we write \"2.11.21, DH: The fish was not aligned nicely perpendicular in the sample holder. I re-reconstructed the data *without* a ROI. Parts of the OJ might still be outside of the visible region. @Mikki, can you double-check?\"\n"
     ]
    }
   ],
   "source": [
    "# Find fishes which need to be discussed in detail\n",
    "for c,row in Data.iterrows():\n",
    "    with open(row.commentsfile, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            if 'Mikki' in line or 'David' in line or 'discus' in line:\n",
    "                print('Fish', row.Fish, 'needs to be looked at in detail, because in its commentfile we write \"%s\"' % line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01/60: 12319 has a head-scan\n",
      "02/60: 12319 has a head-scan\n",
      "03/60: 109188 has a head-scan\n",
      "04/60: IG80 has a head-scan\n",
      "05/60: 10448 has a head-scan\n",
      "06/60: 11601 has a head-scan\n",
      "07/60: IG104 has a head-scan\n",
      "08/60: AN33 has a head-scan\n",
      "09/60: 109320 has a head-scan\n",
      "10/60: 10605 has a head-scan\n",
      "11/60: 10619 has a head-scan\n",
      "12/60: 10619 has a head-scan\n",
      "13/60: 10619 has a head-scan\n",
      "14/60: 103718 has a head-scan\n",
      "15/60: 104671 has a head-scan\n",
      "16/60: 104671 has a head-scan\n",
      "17/60: 103704 has a head-scan\n",
      "18/60: 11639 has a head-scan\n",
      "19/60: 11807 has a head-scan\n",
      "20/60: 10794 has a head-scan\n",
      "21/60: 11729 has a head-scan\n",
      "22/60: 11322 has a head-scan\n",
      "23/60: 13393 has a head-scan\n",
      "24/60: 11500 has a head-scan\n",
      "25/60: 11946 has a head-scan\n",
      "26/60: 11344 has a head-scan\n",
      "27/60: 13492 has a head-scan\n",
      "28/60: 11116 has a head-scan\n",
      "29/60: BH58 has a head-scan\n",
      "30/60: 109209 has a head-scan\n",
      "31/60: 106816 has a head-scan\n",
      "32/60: IG156 has a head-scan\n",
      "33/60: IG142 has a head-scan\n",
      "34/60: MA31 has a head-scan\n",
      "35/60: IG161 has a head-scan\n",
      "36/60: 13069 has a head-scan\n",
      "37/60: 11992 has a head-scan\n",
      "38/60: 11557 has a head-scan\n",
      "39/60: 11053 has a head-scan\n",
      "40/60: 109220 has a head-scan\n",
      "41/60: 10629 has a head-scan\n",
      "42/60: 10619 has a head-scan\n",
      "43/60: 11965 has a head-scan\n"
     ]
    }
   ],
   "source": [
    "d = 1\n",
    "for c, row in Data.iterrows():\n",
    "    if 'head' in row.Scan:\n",
    "        print('%0.2d/60: %s has a head-scan' % (d, row.Fish))\n",
    "        d+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
