{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handle the 'data' of the fishes\n",
    "Wrestle with our data and Mikkis XLS sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "import os\n",
    "import glob\n",
    "import pandas\n",
    "import imageio\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib_scalebar.scalebar import ScaleBar\n",
    "import seaborn\n",
    "import dask\n",
    "import dask_image.imread\n",
    "from dask.distributed import Client, LocalCluster\n",
    "from numcodecs import Blosc\n",
    "import skimage\n",
    "from tqdm import notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dask temporarry files go to D:\\tmp\n"
     ]
    }
   ],
   "source": [
    "# Set dask temporary folder\n",
    "# Do this before creating a client: https://stackoverflow.com/a/62804525/323100\n",
    "import tempfile\n",
    "if 'Linux' in platform.system():\n",
    "    tmp = os.path.join(os.sep, 'media', 'habi', 'Fast_SSD')\n",
    "elif 'Darwin' in platform.system():\n",
    "    tmp = tempfile.gettempdir()\n",
    "else:\n",
    "    if 'anaklin' in platform.node():\n",
    "        tmp = os.path.join('F:\\\\')\n",
    "    else:\n",
    "        tmp = os.path.join('D:\\\\')\n",
    "dask.config.set({'temporary_directory': os.path.join(tmp, 'tmp')})\n",
    "print('Dask temporarry files go to %s' % dask.config.get('temporary_directory'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\haberthu\\Miniconda3\\lib\\site-packages\\distributed\\node.py:160: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 54785 instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Start cluster and client now, after setting tempdir\n",
    "cluster = LocalCluster(n_workers=8)\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You can seee what DASK is doing at \"http://localhost:54785/status\"\n"
     ]
    }
   ],
   "source": [
    "print('You can seee what DASK is doing at \"http://localhost:%s/status\"' % client.scheduler_info()['services']['dashboard'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Ignore warnings in the notebook\n",
    "# import warnings\n",
    "# warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up figure defaults\n",
    "plt.rc('image', cmap='gray', interpolation='nearest')  # Display all images in b&w and with 'nearest' interpolation\n",
    "plt.rcParams['figure.figsize'] = (16, 9)  # Size up figures a bit\n",
    "plt.rcParams['figure.dpi'] = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup scale bar defaults\n",
    "plt.rcParams['scalebar.location'] = 'lower right'\n",
    "plt.rcParams['scalebar.frameon'] = False\n",
    "plt.rcParams['scalebar.color'] = 'white'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display all plots identically\n",
    "lines = 3\n",
    "# And then do something like\n",
    "# plt.subplot(lines, numpy.ceil(len(Data) / float(lines)), c + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are loading all the data from D:\\Results\\EAWAG\n"
     ]
    }
   ],
   "source": [
    "# Different locations if running either on Linux or Windows\n",
    "FastSSD = False\n",
    "overthere = False # Load the data directly from the iee-research_storage drive\n",
    "# to speed things up significantly\n",
    "if 'Linux' in platform.system():\n",
    "    if FastSSD:\n",
    "        BasePath = os.path.join(os.sep, 'media', 'habi', 'Fast_SSD')\n",
    "    else:\n",
    "        BasePath = os.path.join(os.sep, 'home', 'habi', '1272')\n",
    "elif 'Darwin' in platform.system():\n",
    "    FastSSD = False\n",
    "    BasePath = os.path.join('/Users/habi/Dev/EAWAG/Data')\n",
    "elif 'Windows' in platform.system():\n",
    "    if FastSSD:\n",
    "        BasePath = os.path.join('F:\\\\')\n",
    "    else:\n",
    "        if 'anaklin' in platform.node():\n",
    "            BasePath = os.path.join('S:\\\\')\n",
    "        else:\n",
    "            BasePath = os.path.join('D:\\\\Results')\n",
    "Root = os.path.join(BasePath, 'EAWAG')\n",
    "if overthere:\n",
    "        Root = os.path.join('I:\\\\microCTupload')\n",
    "print('We are loading all the data from %s' % Root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pixelsize(logfile):\n",
    "    \"\"\"Get the pixel size from the scan log file\"\"\"\n",
    "    pixelsize=None    \n",
    "    with open(logfile, 'r') as f:\n",
    "        for line in f:\n",
    "            if 'Image Pixel' in line and 'Scaled' not in line:\n",
    "                pixelsize = float(line.split('=')[1])\n",
    "    return(pixelsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_projectionsize(logfile):\n",
    "    \"\"\"How big did we set the camera?\"\"\"\n",
    "    with open(logfile, 'r') as f:\n",
    "        for line in f:\n",
    "            if 'Number Of Rows' in line:\n",
    "                y = int(line.split('=')[1])\n",
    "            if 'Number Of Columns' in line:\n",
    "                x = int(line.split('=')[1])                \n",
    "    return(x*y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filter(logfile):\n",
    "    \"\"\"Get the filter we used whole scanning from the scan log file\"\"\"\n",
    "    with open(logfile, 'r') as f:\n",
    "        for line in f:\n",
    "            if 'Filter=' in line:\n",
    "                whichfilter = line.split('=')[1].strip()\n",
    "    return(whichfilter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_exposuretime(logfile):\n",
    "    \"\"\"Get the exposure time size from the scan log file\"\"\"\n",
    "    with open(logfile, 'r') as f:\n",
    "        for line in f:\n",
    "            if 'Exposure' in line:\n",
    "                exposuretime = int(line.split('=')[1])\n",
    "    return(exposuretime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ringartefact(logfile):\n",
    "    \"\"\"Get the ring artefact correction from the  scan log file\"\"\"\n",
    "    with open(logfile, 'r') as f:\n",
    "        for line in f:\n",
    "            if 'Ring Artifact' in line:\n",
    "                ringartefactcorrection = int(line.split('=')[1])\n",
    "    return(ringartefactcorrection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reconstruction_grayvalue(logfile):\n",
    "    grayvalue = None\n",
    "    \"\"\"How did we map the brightness of the reconstructions?\"\"\"\n",
    "    with open(logfile, 'r') as f:\n",
    "        for line in f:\n",
    "            if 'Maximum for' in line:\n",
    "                grayvalue = float(line.split('=')[1])\n",
    "    return(grayvalue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_beamhardening(logfile):\n",
    "    \"\"\"Get the beamhardening correction from the  scan log file\"\"\"\n",
    "    with open(logfile, 'r') as f:\n",
    "        for line in f:\n",
    "            if 'Hardening' in line:\n",
    "                beamhardeningcorrection = int(line.split('=')[1])\n",
    "    return(beamhardeningcorrection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rotationstep(logfile):\n",
    "    \"\"\"Get the rotation step from the scan log file\"\"\"\n",
    "    with open(logfile, 'r') as f:\n",
    "        for line in f:\n",
    "            if 'Rotation Step' in line:\n",
    "                rotstep = float(line.split('=')[1])\n",
    "    return(rotstep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_frameaveraging(logfile):\n",
    "    \"\"\"Get the frame averaging from the scan log file\"\"\"\n",
    "    with open(logfile, 'r') as f:\n",
    "        for line in f:\n",
    "            if 'Averaging' in line:\n",
    "                avg = line.split('=')[1]\n",
    "    return(avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_machine(logfile):\n",
    "    \"\"\"Get the machine we used to scan\"\"\"\n",
    "    with open(logfile, 'r') as f:\n",
    "        for line in f:\n",
    "            if 'Scanner' in line:\n",
    "                machine = line.split('=')[1].strip()\n",
    "    return(machine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scantime(logfile):\n",
    "    \"\"\"How long did we scan?\"\"\"\n",
    "    with open(logfile, 'r') as f:\n",
    "        for line in f:\n",
    "            if 'Scan duration' in line:\n",
    "                time = line.split('=')[1].strip()\n",
    "    return(pandas.to_timedelta(time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stacks(logfile):\n",
    "    \"\"\"How many stacks/connected scans did we make?\"\"\"\n",
    "    stacks = 1\n",
    "    with open(logfile, 'r') as f:\n",
    "        for line in f:\n",
    "            if 'conn' in line:\n",
    "                stacks = int(line.split('=')[1])\n",
    "    return(stacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scandate(logfile, verbose=False):\n",
    "    \"\"\"When did we scan the fish?\"\"\"\n",
    "    with open(logfile, 'r') as f:\n",
    "        for line in f:\n",
    "            if 'Study Date and Time' in line:\n",
    "                if verbose:\n",
    "                    print('Found \"date\" line: %s' % line.strip())\n",
    "                datestring = line.split('=')[1].strip().replace('  ', ' ')\n",
    "                if verbose:\n",
    "                    print('The date string is: %s' % datestring)\n",
    "                date = pandas.to_datetime(datestring , format='%d %b %Y %Hh:%Mm:%Ss')\n",
    "                if verbose:\n",
    "                    print('Parsed to: %s' % date)\n",
    "                (date)\n",
    "    return(date.isoformat())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_git_hash():\n",
    "    '''\n",
    "    Get the current git hash from the repository.\n",
    "    Based on http://stackoverflow.com/a/949391/323100 and\n",
    "    http://stackoverflow.com/a/18283905/323100\n",
    "    '''\n",
    "    from subprocess import Popen, PIPE\n",
    "    import os\n",
    "    gitprocess = Popen(['git',\n",
    "                        '--git-dir',\n",
    "                        os.path.join(os.getcwd(), '.git'),\n",
    "                        'rev-parse',\n",
    "                        '--short',\n",
    "                        '--verify',\n",
    "                        'HEAD'],\n",
    "                       stdout=PIPE)\n",
    "    (output, _) = gitprocess.communicate()\n",
    "    return output.strip().decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Make directory for output\n",
    "# OutPutDir = os.path.join(os.getcwd(), 'Output', get_git_hash())\n",
    "# print('We are saving all the output to %s' % OutPutDir)\n",
    "# os.makedirs(OutPutDir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make us a dataframe for saving all that we need\n",
    "Data = pandas.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get *all* log files\n",
    "# Sort them by time, not name\n",
    "Data['LogFile'] = [f for f in sorted(glob.glob(os.path.join(Root, '**', '*.log'), recursive=True), key=os.path.getmtime)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all folders\n",
    "Data['Folder'] = [os.path.dirname(f) for f in Data['LogFile']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "- 105005_104015\\proj\\105005_104015~00.log is missing matching reconstructions\n",
      "[]\n",
      "- 105005_104015\\proj\\105005_104015~01.log is missing matching reconstructions\n",
      "[]\n",
      "- 105005_104015\\proj\\105005_104015~02.log is missing matching reconstructions\n",
      "[]\n",
      "- 105005_104015\\proj\\105005_104015~03.log is missing matching reconstructions\n",
      "[]\n",
      "- 105005_104015\\proj\\105005_104015~04.log is missing matching reconstructions\n",
      "[]\n",
      "- 105005_104015\\proj\\105005_104015.log is missing matching reconstructions\n",
      "[]\n",
      "- 104671_156645\\proj\\104671_156645~00.log is missing matching reconstructions\n",
      "[]\n",
      "- 104671_156645\\proj\\104671_156645~01.log is missing matching reconstructions\n",
      "[]\n",
      "- 104671_156645\\proj\\104671_156645~02.log is missing matching reconstructions\n",
      "[]\n",
      "- 104671_156645\\proj\\104671_156645.log is missing matching reconstructions\n",
      "[]\n",
      "- MA41\\proj\\MA41.log is missing matching reconstructions\n",
      "[]\n",
      "- MA41\\proj\\MA41_rectmp.log is missing matching reconstructions\n",
      "[]\n",
      "- MA41\\proj\\MA41~00.log is missing matching reconstructions\n",
      "[]\n",
      "- MA41\\proj\\MA41~01.log is missing matching reconstructions\n",
      "[]\n",
      "- MA41\\proj\\MA41~02.log is missing matching reconstructions\n"
     ]
    }
   ],
   "source": [
    "# Check for samples which are not yet reconstructed\n",
    "for c, row in Data.iterrows():\n",
    "    # Iterate over every 'proj' folder\n",
    "    if 'proj' in row.Folder:\n",
    "        if not 'TScopy' in row.Folder and not 'PR' in row.Folder:\n",
    "            # If there's nothing with 'rec*' on the same level, then tell us        \n",
    "            if not glob.glob(row.Folder.replace('proj', 'rec')):\n",
    "                print(glob.glob(row.Folder.replace('proj', 'rec')))\n",
    "                print('- %s is missing matching reconstructions' % row.LogFile[len(Root)+1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get rid of all non-rec logfiles\n",
    "for c, row in Data.iterrows():\n",
    "    if 'rec' not in row.Folder:\n",
    "        Data.drop([c], inplace=True)\n",
    "    elif 'rectmp.log' in row.LogFile:\n",
    "        Data.drop([c], inplace=True)\n",
    "# Reset dataframe to something that we would get if we only would have loaded the 'rec' files\n",
    "Data = Data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate us some meaningful colums\n",
    "Data['Fish'] = [l[len(Root)+1:].split(os.sep)[0] for l in Data['LogFile']]\n",
    "Data['Scan'] = ['_'.join(l[len(Root)+1:].split(os.sep)[1:-1]) for l in Data['LogFile']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the file names of the reconstructions\n",
    "Data['Reconstructions'] = [sorted(glob.glob(os.path.join(f, '*rec0*.png'))) for f in Data['Folder']]\n",
    "Data['Number of reconstructions'] = [len(r) for r in Data.Reconstructions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/210: Scan 103908\\jaw_rec does not contain any reconstructions and will be removed in the next step\n",
      "2/210: Scan Teeth\\W_rec_al0.25 does not contain any reconstructions and will be removed in the next step\n",
      "3/210: Scan Teeth\\W_rec_nofilter does not contain any reconstructions and will be removed in the next step\n",
      "4/210: Scan Teeth\\P_rec_al0.25 does not contain any reconstructions and will be removed in the next step\n",
      "5/210: Scan Teeth\\P_rec_nofilter does not contain any reconstructions and will be removed in the next step\n",
      "6/210: Scan 104016\\pharynx_rec does not contain any reconstructions and will be removed in the next step\n",
      "7/210: Scan 103375\\rec_stuck does not contain any reconstructions and will be removed in the next step\n",
      "8/210: Scan 103375\\rec does not contain any reconstructions and will be removed in the next step\n",
      "9/210: Scan NY75\\rec does not contain any reconstructions and will be removed in the next step\n",
      "10/210: Scan 161543\\head_30um_rec does not contain any reconstructions and will be removed in the next step\n",
      "11/210: Scan 161543\\rec does not contain any reconstructions and will be removed in the next step\n",
      "12/210: Scan 10628\\head_13um_rec does not contain any reconstructions and will be removed in the next step\n",
      "13/210: Scan 14298\\rec does not contain any reconstructions and will be removed in the next step\n",
      "14/210: Scan 14269\\rec does not contain any reconstructions and will be removed in the next step\n",
      "15/210: Scan 103754\\rec does not contain any reconstructions and will be removed in the next step\n",
      "16/210: Scan 21322\\jaw_rec_mouth_5um does not contain any reconstructions and will be removed in the next step\n",
      "17/210: Scan 131282\\jaw_rec does not contain any reconstructions and will be removed in the next step\n",
      "18/210: Scan 131282\\pharynx_rec does not contain any reconstructions and will be removed in the next step\n",
      "19/210: Scan 10628\\head_18um_rec does not contain any reconstructions and will be removed in the next step\n",
      "20/210: Scan 11045\\rec does not contain any reconstructions and will be removed in the next step\n",
      "21/210: Scan 13115\\rec_13um does not contain any reconstructions and will be removed in the next step\n",
      "22/210: Scan 103571\\rec does not contain any reconstructions and will be removed in the next step\n",
      "23/210: Scan 14128\\rec does not contain any reconstructions and will be removed in the next step\n",
      "24/210: Scan 105105\\rec does not contain any reconstructions and will be removed in the next step\n",
      "25/210: Scan 103767\\rec does not contain any reconstructions and will be removed in the next step\n",
      "26/210: Scan 103908\\stack_rec does not contain any reconstructions and will be removed in the next step\n",
      "27/210: Scan 106985\\rec does not contain any reconstructions and will be removed in the next step\n",
      "28/210: Scan TJ3\\jaw_rec does not contain any reconstructions and will be removed in the next step\n",
      "29/210: Scan KAT13\\jaw_rec does not contain any reconstructions and will be removed in the next step\n",
      "30/210: Scan KAT13\\pharynx_rec does not contain any reconstructions and will be removed in the next step\n",
      "31/210: Scan 104016\\jaw_rec does not contain any reconstructions and will be removed in the next step\n",
      "32/210: Scan KC31\\jaw_rec does not contain any reconstructions and will be removed in the next step\n",
      "33/210: Scan 103778\\jaw_rec does not contain any reconstructions and will be removed in the next step\n",
      "34/210: Scan 103635\\jaw_rec does not contain any reconstructions and will be removed in the next step\n",
      "35/210: Scan KC31\\pharynx_rec does not contain any reconstructions and will be removed in the next step\n",
      "36/210: Scan TJ3\\pharynx_rec_pharyngealjaw does not contain any reconstructions and will be removed in the next step\n",
      "37/210: Scan 103734\\rec does not contain any reconstructions and will be removed in the next step\n",
      "38/210: Scan 103761\\rec does not contain any reconstructions and will be removed in the next step\n",
      "39/210: Scan 10715\\rec does not contain any reconstructions and will be removed in the next step\n",
      "40/210: Scan 11447\\rec does not contain any reconstructions and will be removed in the next step\n",
      "41/210: Scan 105005_104015\\104015_rec does not contain any reconstructions and will be removed in the next step\n",
      "42/210: Scan 105005_104015\\105005_rec does not contain any reconstructions and will be removed in the next step\n",
      "43/210: Scan TJ3\\jaw_v1_rec does not contain any reconstructions and will be removed in the next step\n",
      "44/210: Scan 14295\\rec does not contain any reconstructions and will be removed in the next step\n",
      "45/210: Scan 103635\\pharynx_rec does not contain any reconstructions and will be removed in the next step\n",
      "46/210: Scan 103778\\pharynx_rec does not contain any reconstructions and will be removed in the next step\n",
      "47/210: Scan 104021\\rec does not contain any reconstructions and will be removed in the next step\n",
      "48/210: Scan 104621\\rec does not contain any reconstructions and will be removed in the next step\n",
      "49/210: Scan 11116\\rec does not contain any reconstructions and will be removed in the next step\n",
      "50/210: Scan 11946\\rec does not contain any reconstructions and will be removed in the next step\n",
      "51/210: Scan 11639\\rec does not contain any reconstructions and will be removed in the next step\n",
      "52/210: Scan 13393\\rec does not contain any reconstructions and will be removed in the next step\n",
      "54/210: Scan 11500\\rec does not contain any reconstructions and will be removed in the next step\n",
      "55/210: Scan 10448\\rec does not contain any reconstructions and will be removed in the next step\n",
      "56/210: Scan 11729\\rec does not contain any reconstructions and will be removed in the next step\n",
      "57/210: Scan 11322\\rec does not contain any reconstructions and will be removed in the next step\n",
      "58/210: Scan 11053\\rec does not contain any reconstructions and will be removed in the next step\n",
      "59/210: Scan 109209\\rec does not contain any reconstructions and will be removed in the next step\n",
      "61/210: Scan 161476\\rec does not contain any reconstructions and will be removed in the next step\n",
      "62/210: Scan 104042\\rec does not contain any reconstructions and will be removed in the next step\n",
      "63/210: Scan 103658\\rec does not contain any reconstructions and will be removed in the next step\n",
      "64/210: Scan 10628\\full_188um_rec does not contain any reconstructions and will be removed in the next step\n",
      "65/210: Scan 10791\\rec does not contain any reconstructions and will be removed in the next step\n",
      "66/210: Scan 11313\\rec does not contain any reconstructions and will be removed in the next step\n",
      "67/210: Scan 106816\\rec does not contain any reconstructions and will be removed in the next step\n",
      "68/210: Scan 21322\\whole_rec_full does not contain any reconstructions and will be removed in the next step\n",
      "69/210: Scan 11601\\rec does not contain any reconstructions and will be removed in the next step\n",
      "70/210: Scan IG92\\rec does not contain any reconstructions and will be removed in the next step\n",
      "71/210: Scan IG96\\rec does not contain any reconstructions and will be removed in the next step\n",
      "72/210: Scan 104671_156645\\104671_rec does not contain any reconstructions and will be removed in the next step\n",
      "73/210: Scan 10618\\head_rec does not contain any reconstructions and will be removed in the next step\n",
      "74/210: Scan 10618\\rec does not contain any reconstructions and will be removed in the next step\n",
      "75/210: Scan 104671_156645\\156645_rec does not contain any reconstructions and will be removed in the next step\n",
      "76/210: Scan 109320\\rec does not contain any reconstructions and will be removed in the next step\n",
      "77/210: Scan IG161\\rec does not contain any reconstructions and will be removed in the next step\n",
      "78/210: Scan 10605\\rec does not contain any reconstructions and will be removed in the next step\n",
      "79/210: Scan 10576\\rec does not contain any reconstructions and will be removed in the next step\n",
      "80/210: Scan IG104\\rec does not contain any reconstructions and will be removed in the next step\n",
      "81/210: Scan JU22\\rec does not contain any reconstructions and will be removed in the next step\n",
      "82/210: Scan 104061\\rec does not contain any reconstructions and will be removed in the next step\n",
      "83/210: Scan TNB1\\rec does not contain any reconstructions and will be removed in the next step\n",
      "84/210: Scan 11344\\rec does not contain any reconstructions and will be removed in the next step\n",
      "85/210: Scan 10794\\rec does not contain any reconstructions and will be removed in the next step\n",
      "86/210: Scan 11965\\rec does not contain any reconstructions and will be removed in the next step\n",
      "87/210: Scan 14125\\rec does not contain any reconstructions and will be removed in the next step\n",
      "88/210: Scan BI10\\rec does not contain any reconstructions and will be removed in the next step\n",
      "89/210: Scan 12319\\rec does not contain any reconstructions and will be removed in the next step\n",
      "90/210: Scan 13492\\rec does not contain any reconstructions and will be removed in the next step\n",
      "91/210: Scan BI10\\rec does not contain any reconstructions and will be removed in the next step\n",
      "92/210: Scan TMG15\\rec does not contain any reconstructions and will be removed in the next step\n",
      "93/210: Scan KI30\\rec does not contain any reconstructions and will be removed in the next step\n",
      "94/210: Scan MA31\\rec does not contain any reconstructions and will be removed in the next step\n",
      "95/210: Scan 11344\\rec_rescan does not contain any reconstructions and will be removed in the next step\n",
      "96/210: Scan 10794\\rec_rescan does not contain any reconstructions and will be removed in the next step\n"
     ]
    }
   ],
   "source": [
    "# Get the file names of the reconstructions\n",
    "for c, row in Data[Data['Number of reconstructions'] == 0].iterrows():\n",
    "    print('%s/%s: Scan %s does not contain any reconstructions and '\n",
    "          'will be removed in the next step' % (c+1, len(Data), os.path.join(row.Fish, row.Scan)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 210 folders in total\n",
      "We have 116 folders *with* reconstructions in them\n"
     ]
    }
   ],
   "source": [
    "# Drop samples which have not been reconstructed yet\n",
    "# Based on https://stackoverflow.com/a/13851602\n",
    "# for c,row in Data.iterrows():\n",
    "#     if not row['Number of reconstructions']:\n",
    "#         print('%s contains no PNG files, we might be currently reconstructing it' % row.Folder)\n",
    "print('We have %s folders in total' % (len(Data)))\n",
    "Data = Data[Data['Number of reconstructions'] > 0]\n",
    "Data.reset_index(drop=True, inplace=True)\n",
    "print('We have %s folders *with* reconstructions in them' % (len(Data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get parameters to doublecheck from logfiles\n",
    "Data['Voxelsize'] = [get_pixelsize(log) for log in Data['LogFile']]\n",
    "Data['Filter'] = [get_filter(log) for log in Data['LogFile']]\n",
    "Data['Exposuretime'] = [get_exposuretime(log) for log in Data['LogFile']]\n",
    "Data['Scanner'] = [get_machine(log) for log in Data['LogFile']]\n",
    "Data['Averaging'] = [get_frameaveraging(log) for log in Data['LogFile']]\n",
    "Data['ProjectionSize'] = [get_projectionsize(log) for log in Data['LogFile']]\n",
    "Data['RotationStep'] = [get_rotationstep(log) for log in Data['LogFile']]\n",
    "Data['CameraWindow'] = [round((ps ** 0.5)/100)*100  for ps in Data['ProjectionSize']]\n",
    "Data['Grayvalue'] = [get_reconstruction_grayvalue(log) for log in Data['LogFile']]\n",
    "Data['RingartefactCorrection'] = [get_ringartefact(log) for log in Data['LogFile']]\n",
    "Data['BeamHardeningCorrection'] = [get_beamhardening(log) for log in Data['LogFile']]\n",
    "Data['Scan date'] = [get_scandate(log) for log in Data['LogFile']]\n",
    "Data['Scan time'] = [get_scantime(log) for log in Data['LogFile']]\n",
    "Data['Stacks'] = [get_stacks(log) for log in Data['LogFile']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data['Scan time total'] = [ st * stk  for st, stk in zip(Data['Scan time'], Data['Stacks'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a text file for each rec-folder, in which we can note what's going on with the fish\n",
    "# Generate filename\n",
    "for c,row in Data.iterrows():\n",
    "    Data.at[c, 'commentsfile'] = os.path.join(os.path.dirname(row.Folder),\n",
    "                                              row.Fish + '.' + row.Scan + '.md')\n",
    "# Create actual file on disk\n",
    "for c,row in Data.iterrows():\n",
    "    # Only do this if the file does not already exist\n",
    "    if not os.path.exists(row.commentsfile):\n",
    "        with open(row.commentsfile, 'w', encoding='utf-8') as f:\n",
    "            f.write('# Fish %s, Scan %s\\n\\n' % (row.Fish, row.Scan))\n",
    "            f.write('This fish was scanned on %s on the %s, with a voxel size of %s μm.\\n\\n'\n",
    "                    % (row['Scan date'], row.Scanner, numpy.round(row.Voxelsize, 2)))\n",
    "            f.write('## Comments')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # https://www.geeksforgeeks.org/iterating-over-rows-and-columns-in-pandas-dataframe/\n",
    "# columns = list(Data)\n",
    "# columns.remove('Folder') \n",
    "# columns.remove('Fish')\n",
    "# columns.remove('LogFile')\n",
    "# columns.remove('Reconstructions')\n",
    "# columns.remove('Number of reconstructions')\n",
    "# columns.remove('Grayvalue')\n",
    "# columns.remove('Scan time')\n",
    "# columns.remove('Scan time total')\n",
    "# columns.remove('Scan date')\n",
    "# print(columns)\n",
    "# for col in columns:\n",
    "#     print(col)\n",
    "#     print(Data[col].unique())\n",
    "#     print(80*'-')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check voxel sizes (*rounded* to two after-comma values)\n",
    "# # If different, spit out which values\n",
    "# roundto = 2\n",
    "# if len(Data['Voxelsize'].round(roundto).unique()) > 1:\n",
    "#     print('We scanned all datasets with %s different voxel sizes' % len(Data['Voxelsize'].round(roundto).unique()))\n",
    "#     for vs in sorted(Data['Voxelsize'].round(roundto).unique()):\n",
    "#         print('-', vs, 'um for ', end='')\n",
    "#         for c, row in Data.iterrows():\n",
    "#             if float(vs) == round(row['Voxelsize'], roundto):\n",
    "#                 print(os.path.join(row['Fish'], row['Scan']), end=', ')\n",
    "#         print('')\n",
    "# else:\n",
    "#     print('We scanned all datasets with equal voxel size, namely %s um.' % float(Data['Voxelsize'].round(roundto).unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if len(Data['Grayvalue'].unique()) > 1:\n",
    "#     print('We reconstructed the datasets with different maximum gray values, namely')\n",
    "#     for gv in Data['Grayvalue'].unique():\n",
    "#         print(gv, 'for Samples ', end='')\n",
    "#         for c, row in Data.iterrows():\n",
    "#             if float(gv) == row['Grayvalue']:\n",
    "#                 print(os.path.join(row['Fish'], row['Scan']), end=', ')\n",
    "#         print('')\n",
    "# else:\n",
    "#     print('We reconstructed all datasets with equal maximum gray value, namely %s.' % Data['Grayvalue'].unique()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data[['Fish', 'Scan',\n",
    "#       'Voxelsize', 'Scanner',\n",
    "#       'Scan date', 'CameraWindow', 'RotationStep', 'Averaging',\n",
    "#       'Scan time', 'Stacks', 'Scan time total']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In total, we scanned for 409 hours and 43 minutes)\n",
      "\t - Of these, we scanned 276 hours and 9 minutes on the SkyScan2214,for 106 scans\n",
      "\t - Of these, we scanned 133 hours and 33 minutes on the SkyScan1272,for 10 scans\n"
     ]
    }
   ],
   "source": [
    "# Get an overview over the total scan time\n",
    "# Nice output based on https://stackoverflow.com/a/8907407/323100\n",
    "total_seconds = int(Data['Scan time total'].sum().total_seconds())\n",
    "hours, remainder = divmod(total_seconds,60*60)\n",
    "minutes, seconds = divmod(remainder,60)\n",
    "print('In total, we scanned for %s hours and %s minutes)' % (hours, minutes))\n",
    "for machine in Data['Scanner'].unique():\n",
    "    total_seconds = int(Data[Data['Scanner'] == machine]['Scan time total'].sum().total_seconds())\n",
    "    hours, remainder = divmod(total_seconds,60*60)\n",
    "    minutes, seconds = divmod(remainder,60)\n",
    "    print('\\t - Of these, we scanned %s hours and %s minutes on the %s,'\n",
    "          'for %s scans' % (hours,\n",
    "                            minutes,\n",
    "                            machine,\n",
    "                            len(Data[Data['Scanner'] == machine])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data[['Fish', 'Scan',\n",
    "      'Voxelsize', 'Scanner',\n",
    "      'Scan date', 'CameraWindow', 'RotationStep', 'Averaging', 'Scan time', 'Stacks' ]].to_excel('Details.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data[['Fish', 'Scan',\n",
    "      'Voxelsize', 'Scanner',\n",
    "      'Scan date', 'CameraWindow',\n",
    "      'RotationStep', 'Averaging', 'Scan time', 'Stacks' ]].to_excel(os.path.join(Root,'Details.xlsx'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading in D:\\Results\\EAWAG\\X_ArchiveFiles\\02.07.2021_CTscanFishList.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Read Mikkis datafile\n",
    "MikkisFile = sorted(glob.glob(os.path.join(Root, 'X_ArchiveFiles', '*CTscanFishList.xlsx')))[0]\n",
    "# Read excel file and use the first column as index\n",
    "print('Reading in %s' % MikkisFile)\n",
    "DataMikki = pandas.read_excel(MikkisFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Fishec</th>\n",
       "      <th>FieldID</th>\n",
       "      <th>OtherID</th>\n",
       "      <th>ReplacementID</th>\n",
       "      <th>Length(cm)</th>\n",
       "      <th>TemporaryJar</th>\n",
       "      <th>Genus</th>\n",
       "      <th>Species</th>\n",
       "      <th>Ecology</th>\n",
       "      <th>Scan date</th>\n",
       "      <th>HeadScan</th>\n",
       "      <th>OralJawScan</th>\n",
       "      <th>PharyngealJawScan</th>\n",
       "      <th>OperculumVisible</th>\n",
       "      <th>DataUploaded</th>\n",
       "      <th>QualityChecked</th>\n",
       "      <th>ScanComments</th>\n",
       "      <th>SpecimenReturned</th>\n",
       "      <th>Comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>103635</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt; 7</td>\n",
       "      <td>&lt; 7cm</td>\n",
       "      <td>\"Astatotilapia\"</td>\n",
       "      <td>nubila swamp blue</td>\n",
       "      <td>insectivore</td>\n",
       "      <td>2021-02-08T12:25:19</td>\n",
       "      <td>no 20um headscan</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>no 20um headscan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2-3 inner row of tricuspid teeth</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>103635</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt; 7</td>\n",
       "      <td>&lt; 7cm</td>\n",
       "      <td>\"Astatotilapia\"</td>\n",
       "      <td>nubila swamp blue</td>\n",
       "      <td>insectivore</td>\n",
       "      <td>2021-02-08T14:24:12</td>\n",
       "      <td>no 20um headscan</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>no 20um headscan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2-3 inner row of tricuspid teeth</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>104016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt; 7</td>\n",
       "      <td>&lt; 7cm</td>\n",
       "      <td>Enterochromis I</td>\n",
       "      <td>cinctus (St. E)</td>\n",
       "      <td>detritivore</td>\n",
       "      <td>2021-02-04T11:21:23</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>not complete</td>\n",
       "      <td>no</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>pharyngeal jaw not complete</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>104016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt; 7</td>\n",
       "      <td>&lt; 7cm</td>\n",
       "      <td>Enterochromis I</td>\n",
       "      <td>cinctus (St. E)</td>\n",
       "      <td>detritivore</td>\n",
       "      <td>2021-02-04T13:30:11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14298</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt; 7</td>\n",
       "      <td>&lt; 7cm</td>\n",
       "      <td>Incertae sedis</td>\n",
       "      <td>thick skin</td>\n",
       "      <td>insectivore</td>\n",
       "      <td>NaN</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>no</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>bad segmentation quality</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Fishec FieldID OtherID ReplacementID Length(cm) TemporaryJar  \\\n",
       "0  103635     NaN     NaN           NaN        < 7        < 7cm   \n",
       "1  103635     NaN     NaN           NaN        < 7        < 7cm   \n",
       "2  104016     NaN     NaN           NaN        < 7        < 7cm   \n",
       "3  104016     NaN     NaN           NaN        < 7        < 7cm   \n",
       "4   14298     NaN     NaN           NaN        < 7        < 7cm   \n",
       "\n",
       "             Genus            Species      Ecology            Scan date  \\\n",
       "0  \"Astatotilapia\"  nubila swamp blue  insectivore  2021-02-08T12:25:19   \n",
       "1  \"Astatotilapia\"  nubila swamp blue  insectivore  2021-02-08T14:24:12   \n",
       "2  Enterochromis I    cinctus (St. E)  detritivore  2021-02-04T11:21:23   \n",
       "3  Enterochromis I    cinctus (St. E)  detritivore  2021-02-04T13:30:11   \n",
       "4   Incertae sedis         thick skin  insectivore                  NaN   \n",
       "\n",
       "           HeadScan OralJawScan PharyngealJawScan  OperculumVisible  \\\n",
       "0  no 20um headscan         yes               yes  no 20um headscan   \n",
       "1  no 20um headscan         yes               yes  no 20um headscan   \n",
       "2                no         yes      not complete                no   \n",
       "3               NaN         NaN               NaN               NaN   \n",
       "4                no         yes               NaN                no   \n",
       "\n",
       "  DataUploaded QualityChecked                      ScanComments  \\\n",
       "0          NaN            NaN  2-3 inner row of tricuspid teeth   \n",
       "1          NaN            NaN  2-3 inner row of tricuspid teeth   \n",
       "2          NaN            NaN       pharyngeal jaw not complete   \n",
       "3          NaN            NaN                               NaN   \n",
       "4          NaN            NaN          bad segmentation quality   \n",
       "\n",
       "   SpecimenReturned Comments  \n",
       "0               NaN      NaN  \n",
       "1               NaN      NaN  \n",
       "2               NaN      NaN  \n",
       "3               NaN      NaN  \n",
       "4               NaN      NaN  "
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DataMikki.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the fish we look at and display all the info we know about it\n",
    "# Set a substring you're looking for to the variable below\n",
    "# In which jar can we find it?\n",
    "fish = 'MA31'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*MA31*: Found on disk in D:\\Results\\EAWAG\\MA31\n"
     ]
    }
   ],
   "source": [
    "# Do we have something from this fish on disk?\n",
    "ondisk = glob.glob(os.path.join(Root, '*%s*' % fish))\n",
    "if len(ondisk):\n",
    "    for found in ondisk:\n",
    "        print('*%s*: Found on disk in %s' % (fish, found))\n",
    "        foundondisk = 1\n",
    "else:\n",
    "    print('*%s*: Nothing found in %s' % (fish, Root))\n",
    "    foundondisk = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*MA31*: Sample MA31/rec_rescan was scanned on 2021-08-20T14:28:47\n",
      "*MA31*: Sample MA31/rec_rescan_rereconstruct_OJ was scanned on 2021-08-20T14:28:47\n",
      "*MA31*: Sample MA31/head_rec was scanned on 2021-10-19T11:15:30\n"
     ]
    }
   ],
   "source": [
    "# Did we scan it already?\n",
    "found = 0\n",
    "for c, row in Data.iterrows():\n",
    "    if fish in row.Fish:\n",
    "        print('*%s*: Sample %s/%s was scanned on %s' % (fish, row['Fish'], row['Scan'], row['Scan date']))\n",
    "        found = 1\n",
    "if not found:\n",
    "    if foundondisk:\n",
    "        print('*%s*: We have a folder (%s) for this sample, but nothing in the dataframe, so it probably is all good' % (fish, ondisk[0]))\n",
    "        print('Check the folder to be shure')\n",
    "    else:\n",
    "        print('*%s*: Nothing about this sample is found in our dataframe' % fish)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'*MA31*: Nothing found in D:\\\\Results\\\\EAWAG\\\\FullHeadList.txt'"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Can we find it in FullHeadList.txt?\n",
    "def findinFullHeadList(sample):\n",
    "    ''' Look for the sample in the FullHeadList.txt file'''\n",
    "    fullheadlist = glob.glob(os.path.join(Root, 'FullHeadList.*'))[0]    \n",
    "    found = 0\n",
    "    with open(fullheadlist, 'r') as f:\n",
    "        for line in f:\n",
    "            if str(sample) in line:\n",
    "                print(line.strip())\n",
    "                found = 1\n",
    "    if not found:\n",
    "        return('*%s*: Nothing found in %s' % (sample, fullheadlist))\n",
    "    else:\n",
    "        return(None)\n",
    "findinFullHeadList(fish)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We found these comment files in our dataframe\n",
      "\t- D:\\Results\\EAWAG\\MA31\\MA31.rec_rescan.md\n",
      "\t- D:\\Results\\EAWAG\\MA31\\MA31.rec_rescan_rereconstruct_OJ.md\n",
      "\t- D:\\Results\\EAWAG\\MA31\\head\\MA31.head_rec.md\n",
      "We found these comment files on disk\n",
      "# Fish MA31, Scan rec_rescan\n",
      "BEEEEP!\n",
      "\n",
      "This fish was scanned on 2021-08-20T14:28:47 on the SkyScan2214, with a voxel size of 9.3 μm.\n",
      "\n",
      "## Comments\n",
      "\n",
      "Side note - previous scan was done at 8.59um.\n",
      "The rescan shows artifacts on oral jaw teeth which stick them together - QualityCheck png.\n",
      "BEEEEP!\n",
      "# Fish MA31, Scan rec_rescan_rereconstruct_OJ\n",
      "BEEEEP!\n",
      "\n",
      "This fish was scanned on 2021-08-20T14:28:47 on the SkyScan2214, with a voxel size of 9.3 μm.\n",
      "\n",
      "## Comments\n",
      "# Fish MA31, Scan head_rec\n",
      "\n",
      "This fish was scanned on 2021-10-19T11:15:30 on the SkyScan2214, with a voxel size of 17.0 μm.\n",
      "\n",
      "## Comments\n"
     ]
    }
   ],
   "source": [
    "# Do we need to rescan this fish\n",
    "# Find all relevant comment files\n",
    "commentfiles = glob.glob(os.path.join(Root, '*%s*' % fish, '**', '*.md'), recursive=True)\n",
    "\n",
    "print('We found these comment files in our dataframe')\n",
    "for c, row in Data.iterrows():\n",
    "    if fish in row.Fish:\n",
    "        print('\\t-', row.commentsfile)\n",
    "        found = 1\n",
    "\n",
    "print('We found these comment files on disk')\n",
    "if len(commentfiles):\n",
    "    for commentfile in commentfiles:\n",
    "        with open(commentfile, 'r', encoding='utf-8') as file:\n",
    "            for line in file:\n",
    "                print(line.strip())\n",
    "                if 'rescan' in line:\n",
    "                    print('BEEEEP!')      \n",
    "else:\n",
    "    print('No file for potential comments found, sample %s can be returned to its jar')\n",
    "    print('See below')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*105105*: The fish 105105 should now go in jar \"length=15 cm\" (Mark1))\n"
     ]
    }
   ],
   "source": [
    "# In which jar should it be/go?\n",
    "foundfishes = 0\n",
    "for d, row in DataMikki.iterrows():\n",
    "    if (str(fish).lower() in str(row.Fishec).lower()) or \\\n",
    "    (str(fish).lower() in str(row.FieldID).lower()) or \\\n",
    "    (str(fish).lower() in str(row.OtherID).lower()) or \\\n",
    "    (str(fish).lower() in str(row.ReplacementID).lower()):\n",
    "        foundfishes = (row.Fishec, row.FieldID, row.OtherID, row.ReplacementID)\n",
    "        # remove nan from the list of hits\n",
    "        foundfishes = [str(x).lower() for x in foundfishes if pandas.isnull(x) == False]\n",
    "        print('*%s*: The fish ' % fish, end='')        \n",
    "        if len(foundfishes) > 1:\n",
    "            for found in foundfishes:\n",
    "                print(found.upper(), end='/')\n",
    "        else:\n",
    "            print(foundfishes[0].upper(), end='')\n",
    "        print(' should now go in jar \"length=%s cm\" (%s))' % (row['Length(cm)'],\n",
    "                                                              row['TemporaryJar']))\n",
    "if not foundfishes:\n",
    "    print('*%s*: Nothing found in %s' % (fish, MikkisFile))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "60 of the fishes need complete head scans.\n",
    "Let's try to go through Mikkis/Kassandras list and see how far we progressed through that list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104061 is missing a head-scan on disk, but is found on line 32 of the full head list\n",
      "13115 is missing a head-scan on disk, but is found on line 42 of the full head list\n",
      "10448 is missing a head-scan on disk, but is found on line 48 of the full head list\n",
      "103761 is missing a head-scan on disk, but is found on line 72 of the full head list\n",
      "11447 is missing a head-scan on disk, but is found on line 73 of the full head list\n",
      "105105 is missing a head-scan on disk, but is found on line 74 of the full head list\n"
     ]
    }
   ],
   "source": [
    "# Read in full head list, go through all the scans we alredy did and see what needs to be done\n",
    "fullheadlist = glob.glob(os.path.join(Root, '*Head*.txt'))[0]\n",
    "HeadsToBeScanned = []\n",
    "with open(fullheadlist, 'r', encoding='utf-8') as file:\n",
    "    headdone = False\n",
    "    for ln, line in enumerate(file):\n",
    "        if line.strip():  #skip empty lines\n",
    "            # The firs 'item' on the line should be the fish ID\n",
    "            fish = line.split()[0].replace(',','').replace('asdf','').upper()\n",
    "            # Let's ignore some lines which don't start with a fish ID\n",
    "            # The set-join here removes duplicate characters from the string (e.g. =====, !! and ::)\n",
    "            if len(''.join(set(fish))) > 2:\n",
    "                for c, row in Data[Data.Fish == fish].iterrows():\n",
    "                    if 'head' in row.Scan:\n",
    "                        # print('\\t%s has a head-scan' % row.Fish)\n",
    "                        # print('%s has a head-scan on disk, and is found on line %s of the full head list' % (fish, ln + 1 ))\n",
    "                        headdone = True\n",
    "                    else:\n",
    "                        headdone = False\n",
    "                # At this point we have either found the fish in the list or 'headdone' is false\n",
    "                if not headdone:\n",
    "                    print('%s is missing a head-scan on disk, but is found on line %s of the full head list' % (fish, ln + 1 ))\n",
    "                    HeadsToBeScanned.append(fish)             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fish 10448 can be ignored because we did another scan after the head-scan, so we reset \"headdone\" in the loop above\n",
    "# We could probably do it in a more clever way, but already spent too much time on this part :)\n",
    "try:\n",
    "    HeadsToBeScanned.remove('10448')\n",
    "except ValueError:\n",
    "    # Nothing to see here, pass along\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*104061*: A fish called 13405/104061/ should be found in jar \"length=14.5 cm\" (Mark5))\n",
      "*13115*: A fish called 11317/13115/ should be found in jar \"length=18 cm\" (Mark4))\n",
      "*103761*: A fish called 104626/103761/ should be found in jar \"length=8 cm\" (Mark3))\n",
      "*11447*: A fish called 11447 should be found in jar \"length=7.9 cm\" (Mark2))\n",
      "*105105*: A fish called 105105 should be found in jar \"length=15 cm\" (Mark1))\n"
     ]
    }
   ],
   "source": [
    "for fish in HeadsToBeScanned:\n",
    "    # In which jar should we look for the fishes we still need to scan the head of?\n",
    "    foundfishes = 0\n",
    "    for d, row in DataMikki.iterrows():\n",
    "        if (str(fish).lower() in str(row.Fishec).lower()) or \\\n",
    "        (str(fish).lower() in str(row.FieldID).lower()) or \\\n",
    "        (str(fish).lower() in str(row.OtherID).lower()) or \\\n",
    "        (str(fish).lower() in str(row.ReplacementID).lower()):\n",
    "            foundfishes = (row.Fishec, row.FieldID, row.OtherID, row.ReplacementID)\n",
    "            # remove nan from the list of hits\n",
    "            foundfishes = [str(x).lower() for x in foundfishes if pandas.isnull(x) == False]\n",
    "            print('*%s*: A fish called ' % fish, end='')        \n",
    "            if len(foundfishes) > 1:\n",
    "                for found in foundfishes:\n",
    "                    print(found.upper(), end='/')\n",
    "            else:\n",
    "                print(foundfishes[0].upper(), end='')\n",
    "            print(' should be found in jar \"length=%s cm\" (%s))' % (row['Length(cm)'],\n",
    "                                                                    row['TemporaryJar']))\n",
    "    if not foundfishes:\n",
    "        print('*%s*: Nothing found in %s' % (fish, MikkisFile))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
