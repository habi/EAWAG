{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handle the 'data' of the fishes\n",
    "Wrestle with the data, check parameters and generate some helping files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "import os\n",
    "import glob\n",
    "import pandas\n",
    "import imageio\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib_scalebar.scalebar import ScaleBar\n",
    "import seaborn\n",
    "import dask\n",
    "import dask_image.imread\n",
    "from dask.distributed import Client, LocalCluster\n",
    "import skimage\n",
    "from tqdm import notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dask temporary files go to /media/habi/Fast_SSD/tmp\n"
     ]
    }
   ],
   "source": [
    "# Set dask temporary folder\n",
    "# Do this before creating a client: https://stackoverflow.com/a/62804525/323100\n",
    "import tempfile\n",
    "if 'Linux' in platform.system():\n",
    "    tmp = os.path.join(os.sep, 'media', 'habi', 'Fast_SSD')\n",
    "elif 'Darwin' in platform.system():\n",
    "    tmp = tempfile.gettempdir()\n",
    "else:\n",
    "    if 'anaklin' in platform.node():\n",
    "        tmp = os.path.join('F:\\\\')\n",
    "    else:\n",
    "        tmp = os.path.join('D:\\\\')\n",
    "dask.config.set({'temporary_directory': os.path.join(tmp, 'tmp')})\n",
    "print('Dask temporary files go to %s' % dask.config.get('temporary_directory'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/habi/miniconda3/envs/eawag/lib/python3.8/site-packages/distributed/node.py:151: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 42905 instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Start cluster and client now, after setting tempdir\n",
    "cluster = LocalCluster(n_workers=8)\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You can seee what DASK is doing at \"http://localhost:42905/status\"\n"
     ]
    }
   ],
   "source": [
    "print('You can seee what DASK is doing at \"http://localhost:%s/status\"' % client.scheduler_info()['services']['dashboard'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Ignore warnings in the notebook\n",
    "# import warnings\n",
    "# warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up figure defaults\n",
    "plt.rc('image', cmap='gray', interpolation='nearest')  # Display all images in b&w and with 'nearest' interpolation\n",
    "plt.rcParams['figure.figsize'] = (16, 9)  # Size up figures a bit\n",
    "plt.rcParams['figure.dpi'] = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup scale bar defaults\n",
    "plt.rcParams['scalebar.location'] = 'lower right'\n",
    "plt.rcParams['scalebar.frameon'] = False\n",
    "plt.rcParams['scalebar.color'] = 'white'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display all plots identically\n",
    "lines = 3\n",
    "# And then do something like\n",
    "# plt.subplot(lines, numpy.ceil(len(Data) / float(lines)), c + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are loading all the data from /media/habi/Fast_SSD/EAWAG\n"
     ]
    }
   ],
   "source": [
    "# Different locations if running either on Linux or Windows\n",
    "FastSSD = True\n",
    "overthere = False  # Load the data directly from the iee-research_storage drive\n",
    "# to speed things up significantly\n",
    "if 'Linux' in platform.system():\n",
    "    if FastSSD:\n",
    "        BasePath = os.path.join(os.sep, 'media', 'habi', 'Fast_SSD')\n",
    "    else:\n",
    "        BasePath = os.path.join(os.sep, 'home', 'habi', '1272')\n",
    "elif 'Darwin' in platform.system():\n",
    "    FastSSD = False\n",
    "    BasePath = os.path.join('/Users/habi/Dev/EAWAG/Data')\n",
    "elif 'Windows' in platform.system():\n",
    "    if FastSSD:\n",
    "        BasePath = os.path.join('F:\\\\')\n",
    "    else:\n",
    "        if 'anaklin' in platform.node():\n",
    "            BasePath = os.path.join('S:\\\\')\n",
    "        else:\n",
    "            BasePath = os.path.join('D:\\\\Results')\n",
    "Root = os.path.join(BasePath, 'EAWAG')\n",
    "if overthere:\n",
    "    if 'Linux' in platform.system():\n",
    "        Root = os.path.join(os.sep, 'home', 'habi', 'research-storage-iee')\n",
    "    else:\n",
    "        Root = os.path.join('I:\\\\microCTupload')\n",
    "print('We are loading all the data from %s' % Root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pixelsize(logfile):\n",
    "    \"\"\"Get the pixel size from the scan log file\"\"\"\n",
    "    pixelsize = None\n",
    "    with open(logfile, 'r') as f:\n",
    "        for line in f:\n",
    "            if 'Image Pixel' in line and 'Scaled' not in line:\n",
    "                pixelsize = float(line.split('=')[1])\n",
    "    return(pixelsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_projectionsize(logfile):\n",
    "    \"\"\"How big did we set the camera?\"\"\"\n",
    "    with open(logfile, 'r') as f:\n",
    "        for line in f:\n",
    "            if 'Number Of Rows' in line:\n",
    "                y = int(line.split('=')[1])\n",
    "            if 'Number Of Columns' in line:\n",
    "                x = int(line.split('=')[1])\n",
    "    return(x * y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_voltage(logfile):\n",
    "    \"\"\"Get the x-ray voltage \"\"\"\n",
    "    with open(logfile, 'r') as f:\n",
    "        for line in f:\n",
    "            if '(kV)' in line:\n",
    "                voltage = int(line.split('=')[1])\n",
    "    return(voltage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_current(logfile):\n",
    "    \"\"\"Get the x-ray voltage \"\"\"\n",
    "    with open(logfile, 'r') as f:\n",
    "        for line in f:\n",
    "            if '(uA)' in line:\n",
    "                current = int(line.split('=')[1])\n",
    "    return(current)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filter(logfile):\n",
    "    \"\"\"Get the filter we used whole scanning from the scan log file\"\"\"\n",
    "    with open(logfile, 'r') as f:\n",
    "        for line in f:\n",
    "            if 'Filter=' in line:\n",
    "                whichfilter = line.split('=')[1].strip()\n",
    "    return(whichfilter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_exposuretime(logfile):\n",
    "    \"\"\"Get the exposure time size from the scan log file\"\"\"\n",
    "    with open(logfile, 'r') as f:\n",
    "        for line in f:\n",
    "            if 'Exposure' in line:\n",
    "                exposuretime = int(line.split('=')[1])\n",
    "    return(exposuretime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ringartefact(logfile):\n",
    "    \"\"\"Get the ring artefact correction from the  scan log file\"\"\"\n",
    "    with open(logfile, 'r') as f:\n",
    "        for line in f:\n",
    "            if 'Ring Artifact' in line:\n",
    "                ringartefactcorrection = int(line.split('=')[1])\n",
    "    return(ringartefactcorrection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reconstruction_grayvalue(logfile):\n",
    "    grayvalue = None\n",
    "    \"\"\"How did we map the brightness of the reconstructions?\"\"\"\n",
    "    with open(logfile, 'r') as f:\n",
    "        for line in f:\n",
    "            if 'Maximum for' in line:\n",
    "                grayvalue = float(line.split('=')[1])\n",
    "    return(grayvalue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_beamhardening(logfile):\n",
    "    \"\"\"Get the beamhardening correction from the  scan log file\"\"\"\n",
    "    with open(logfile, 'r') as f:\n",
    "        for line in f:\n",
    "            if 'Hardening' in line:\n",
    "                beamhardeningcorrection = int(line.split('=')[1])\n",
    "    return(beamhardeningcorrection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rotationstep(logfile):\n",
    "    \"\"\"Get the rotation step from the scan log file\"\"\"\n",
    "    with open(logfile, 'r') as f:\n",
    "        for line in f:\n",
    "            if 'Rotation Step' in line:\n",
    "                rotstep = float(line.split('=')[1])\n",
    "    return(rotstep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_frameaveraging(logfile):\n",
    "    \"\"\"Get the frame averaging from the scan log file\"\"\"\n",
    "    with open(logfile, 'r') as f:\n",
    "        for line in f:\n",
    "            if 'Averaging' in line:\n",
    "                avg = line.split('=')[1]\n",
    "    return(avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_machine(logfile):\n",
    "    \"\"\"Get the machine we used to scan\"\"\"\n",
    "    with open(logfile, 'r') as f:\n",
    "        for line in f:\n",
    "            if 'Scanner' in line:\n",
    "                machine = line.split('=')[1].strip()\n",
    "    return(machine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scantime(logfile):\n",
    "    \"\"\"How long did we scan?\"\"\"\n",
    "    with open(logfile, 'r') as f:\n",
    "        for line in f:\n",
    "            if 'Scan duration' in line:\n",
    "                time = line.split('=')[1].strip()\n",
    "    return(pandas.to_timedelta(time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stacks(logfile):\n",
    "    \"\"\"How many stacks/connected scans did we make?\"\"\"\n",
    "    stacks = 1\n",
    "    with open(logfile, 'r') as f:\n",
    "        for line in f:\n",
    "            if 'conn' in line:\n",
    "                stacks = int(line.split('=')[1])\n",
    "    return(stacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scandate(logfile, verbose=False):\n",
    "    \"\"\"When did we scan the fish?\"\"\"\n",
    "    with open(logfile, 'r') as f:\n",
    "        for line in f:\n",
    "            if 'Study Date and Time' in line:\n",
    "                if verbose:\n",
    "                    print('Found \"date\" line: %s' % line.strip())\n",
    "                datestring = line.split('=')[1].strip().replace('  ', ' ')\n",
    "                if verbose:\n",
    "                    print('The date string is: %s' % datestring)\n",
    "                date = pandas.to_datetime(datestring, format='%d %b %Y %Hh:%Mm:%Ss')\n",
    "                if verbose:\n",
    "                    print('Parsed to: %s' % date)\n",
    "                (date)\n",
    "    return(date.isoformat())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_git_hash():\n",
    "    '''\n",
    "    Get the current git hash from the repository.\n",
    "    Based on http://stackoverflow.com/a/949391/323100 and\n",
    "    http://stackoverflow.com/a/18283905/323100\n",
    "    '''\n",
    "    from subprocess import Popen, PIPE\n",
    "    import os\n",
    "    gitprocess = Popen(['git',\n",
    "                        '--git-dir',\n",
    "                        os.path.join(os.getcwd(), '.git'),\n",
    "                        'rev-parse',\n",
    "                        '--short',\n",
    "                        '--verify',\n",
    "                        'HEAD'],\n",
    "                       stdout=PIPE)\n",
    "    (output, _) = gitprocess.communicate()\n",
    "    return output.strip().decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Make directory for output\n",
    "# OutPutDir = os.path.join(os.getcwd(), 'Output', get_git_hash())\n",
    "# print('We are saving all the output to %s' % OutPutDir)\n",
    "# os.makedirs(OutPutDir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make us a dataframe for saving all that we need\n",
    "Data = pandas.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get *all* log files\n",
    "# Sort them by time, not name\n",
    "Data['LogFile'] = [f for f in sorted(glob.glob(os.path.join(Root, '**', '*.log'),\n",
    "                                               recursive=True),\n",
    "                                     key=os.path.getmtime)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not len(Data):\n",
    "    # Our dataframe is empty.\n",
    "    # We might be running on Binder, e.g. load the logfiles from the subfolder in this repository\n",
    "    print(10 * ' -', 'CAVEAT', 10 * ' -')\n",
    "    print('You are most probably running the notebook on binder.')\n",
    "    print('And thus do not have access to the log files on the research storage')\n",
    "    print('We are using a \"local\" copy of the data in the `logfiles` subfolder')\n",
    "    print('This gives correct, but possibly outdated results...')\n",
    "    print(10 * ' -', 'CAVEAT', 10 * ' -')\n",
    "    # Change root folder\n",
    "    Root = 'logfiles'\n",
    "    # Load log files again\n",
    "    Data['LogFile'] = [f for f in sorted(glob.glob(os.path.join(Root, '**', '*.log'),\n",
    "                                                   recursive=True),\n",
    "                                         key=os.path.getmtime)]\n",
    "    running_on_binder = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all folders\n",
    "Data['Folder'] = [os.path.dirname(f) for f in Data['LogFile']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not running_on_binder:\n",
    "    # Check for samples which are not yet reconstructed\n",
    "    for c, row in Data.iterrows():\n",
    "        # Iterate over every 'proj' folder\n",
    "        if 'proj' in row.Folder:\n",
    "            if 'TScopy' not in row.Folder and 'PR' not in row.Folder:\n",
    "                # If there's nothing with 'rec*' on the same level, then tell us\n",
    "                if not glob.glob(row.Folder.replace('proj', 'rec')):\n",
    "                    # print(glob.glob(row.Folder.replace('proj', 'rec')))\n",
    "                    print('- %s is missing matching reconstructions' % row.LogFile[len(Root) + 1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get rid of all non-rec logfiles\n",
    "for c, row in Data.iterrows():\n",
    "    if 'rec' not in row.Folder:\n",
    "        Data.drop([c], inplace=True)\n",
    "    elif 'rectmp.log' in row.LogFile:\n",
    "        Data.drop([c], inplace=True)\n",
    "# Reset dataframe to something that we would get if we only would have loaded the 'rec' files\n",
    "Data = Data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate us some meaningful colums\n",
    "Data['Fish'] = [l[len(Root) + 1:].split(os.sep)[0] for l in Data['LogFile']]\n",
    "Data['Scan'] = ['_'.join(l[len(Root) + 1:].split(os.sep)[1:-1]) for l in Data['LogFile']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['10151', '103375', '103571', '103634', '103635', '103637',\n",
       "       '103641', '103658', '103704', '103718', '103720', '103723',\n",
       "       '103734', '103754', '103761', '103767', '103778', '103908',\n",
       "       '103926', '104016', '104021', '104042', '104061', '10448',\n",
       "       '104621', '104661', '104671', '104671_156645', '104856', '104929',\n",
       "       '105005_104015', '105105', '10576', '10605', '10618'], dtype=object)"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data.Fish.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 28 unique names in our corpus of scan\n",
      "We performed 91 scans in total\n"
     ]
    }
   ],
   "source": [
    "# How many fishes did we scan?\n",
    "# We scanned six 'buckets of fish' and one set of only 'teeth', so subtract those :)\n",
    "print('We have %s unique names in our corpus of scan' % (len(Data.Fish.unique()) - 7))\n",
    "print('We performed %s scans in total' % len(Data.Scan))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the file names of the reconstructions\n",
    "Data['Reconstructions'] = [sorted(glob.glob(os.path.join(f, '*rec0*.png'))) for f in Data['Folder']]\n",
    "Data['Number of reconstructions'] = [len(r) for r in Data.Reconstructions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not running_on_binder:\n",
    "    # Let's see if we're missing some data\n",
    "    for c, row in Data[Data['Number of reconstructions'] == 0].iterrows():\n",
    "        print('%s/%s: Scan %s does not contain any reconstructions and '\n",
    "              'will be removed in the next step' % (c + 1,\n",
    "                                                    len(Data),\n",
    "                                                    os.path.join(row.Fish, row.Scan)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 91 folders in total\n"
     ]
    }
   ],
   "source": [
    "print('We have %s folders in total' % (len(Data)))\n",
    "if not running_on_binder:\n",
    "    # Drop samples which have not been reconstructed yet\n",
    "    # Based on https://stackoverflow.com/a/13851602\n",
    "    # for c, row in Data.iterrows():\n",
    "    #     if not row['Number of reconstructions']:\n",
    "    #         print('%s contains no PNG files, we might be currently reconstructing it' % row.Folder)\n",
    "    Data = Data[Data['Number of reconstructions'] > 0]\n",
    "    Data.reset_index(drop=True, inplace=True)\n",
    "    print('Of which %s folders do contain reconstructions' % (len(Data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get parameters to doublecheck from logfiles\n",
    "Data['Voxelsize'] = [get_pixelsize(log) for log in Data['LogFile']]\n",
    "Data['Voltage'] = [get_voltage(log) for log in Data['LogFile']]\n",
    "Data['Current'] = [get_current(log) for log in Data['LogFile']]\n",
    "Data['Filter'] = [get_filter(log) for log in Data['LogFile']]\n",
    "Data['Exposuretime'] = [get_exposuretime(log) for log in Data['LogFile']]\n",
    "Data['Scanner'] = [get_machine(log) for log in Data['LogFile']]\n",
    "Data['Averaging'] = [get_frameaveraging(log) for log in Data['LogFile']]\n",
    "Data['ProjectionSize'] = [get_projectionsize(log) for log in Data['LogFile']]\n",
    "Data['RotationStep'] = [get_rotationstep(log) for log in Data['LogFile']]\n",
    "Data['CameraWindow'] = [round((ps ** 0.5) / 100) * 100 for ps in Data['ProjectionSize']]\n",
    "Data['Grayvalue'] = [get_reconstruction_grayvalue(log) for log in Data['LogFile']]\n",
    "Data['RingartefactCorrection'] = [get_ringartefact(log) for log in Data['LogFile']]\n",
    "Data['BeamHardeningCorrection'] = [get_beamhardening(log) for log in Data['LogFile']]\n",
    "Data['Scan date'] = [get_scandate(log) for log in Data['LogFile']]\n",
    "Data['Scan time'] = [get_scantime(log) for log in Data['LogFile']]\n",
    "Data['Stacks'] = [get_stacks(log) for log in Data['LogFile']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data['Scan time total'] = [st * stk for st, stk in zip(Data['Scan time'], Data['Stacks'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.499972, 4.000007, 5.000014, 5.000018, 5.12833] [28.000837, 29.000594, 30.000352, 30.001079, 34.999138]\n"
     ]
    }
   ],
   "source": [
    "print(sorted(Data.Voxelsize.unique())[:5],\n",
    "      sorted(Data.Voxelsize.unique())[-5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['No Filter', 'Al 0.25mm', 'Al 0.5mm'], dtype=object)"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data.Filter.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[55, 60, 65, 70, 80]"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(Data.Voltage.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[115, 116, 125, 132, 134, 136, 137, 138, 140, 142, 166]"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(Data.Current.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 7, 13, 14, 19]"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(Data.RingartefactCorrection.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 10]"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(Data.BeamHardeningCorrection.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a text file for each rec-folder, in which we can note what's going on with the fish\n",
    "# Generate filename\n",
    "for c, row in Data.iterrows():\n",
    "    Data.at[c, 'CommentFile'] = os.path.join(os.path.dirname(row.Folder),\n",
    "                                             row.Fish + '.' + row.Scan + '.md')\n",
    "# Create actual file on disk\n",
    "for c, row in Data.iterrows():\n",
    "    # Only do this if the file does not already exist\n",
    "    if not os.path.exists(row.CommentFile):\n",
    "        with open(row.CommentFile, 'w', encoding='utf-8') as f:\n",
    "            f.write('# Fish %s, Scan %s\\n\\n' % (row.Fish, row.Scan))\n",
    "            f.write('This fish was scanned on %s on the %s, with a voxel size of %s μm.\\n\\n'\n",
    "                    % (row['Scan date'], row.Scanner, numpy.round(row.Voxelsize, 2)))\n",
    "            f.write('## Comments')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # https://www.geeksforgeeks.org/iterating-over-rows-and-columns-in-pandas-dataframe/\n",
    "# columns = list(Data)\n",
    "# columns.remove('Folder')\n",
    "# columns.remove('Fish')\n",
    "# columns.remove('LogFile')\n",
    "# columns.remove('Reconstructions')\n",
    "# columns.remove('Number of reconstructions')\n",
    "# columns.remove('Grayvalue')\n",
    "# columns.remove('Scan time')\n",
    "# columns.remove('Scan time total')\n",
    "# columns.remove('Scan date')\n",
    "# print(columns)\n",
    "# for col in columns:\n",
    "#     print(col)\n",
    "#     print(Data[col].unique())\n",
    "#     print(80 * '-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data[['Fish', 'Scan',\n",
    "#       'Voxelsize', 'Scanner',\n",
    "#       'Scan date', 'CameraWindow', 'RotationStep', 'Averaging',\n",
    "#       'Scan time', 'Stacks', 'Scan time total']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In total, we scanned for 326 hours and 19 minutes)\n",
      "\t - Of these, we scanned 183 hours and 51 minutes on the SkyScan2214, for 74 scans\n",
      "\t - Of these, we scanned 142 hours and 27 minutes on the SkyScan1272, for 17 scans\n"
     ]
    }
   ],
   "source": [
    "# Get an overview over the total scan time\n",
    "# Nice output based on https://stackoverflow.com/a/8907407/323100\n",
    "total_seconds = int(Data['Scan time total'].sum().total_seconds())\n",
    "hours, remainder = divmod(total_seconds, 60 * 60)\n",
    "minutes, seconds = divmod(remainder, 60)\n",
    "print('In total, we scanned for %s hours and %s minutes)' % (hours, minutes))\n",
    "for machine in Data['Scanner'].unique():\n",
    "    total_seconds = int(Data[Data['Scanner'] == machine]['Scan time total'].sum().total_seconds())\n",
    "    hours, remainder = divmod(total_seconds, 60 * 60)\n",
    "    minutes, seconds = divmod(remainder, 60)\n",
    "    print('\\t - Of these, we scanned %s hours and %s minutes on the %s,'\n",
    "          ' for %s scans' % (hours,\n",
    "                             minutes,\n",
    "                             machine,\n",
    "                             len(Data[Data['Scanner'] == machine])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We scanned 35 fishes\n"
     ]
    }
   ],
   "source": [
    "print('We scanned %s fishes' % len(Data.Fish.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We did a total of 91 scans\n"
     ]
    }
   ],
   "source": [
    "print('We did a total of %s scans' % len(Data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We perfomed 25 scans with \"head\" in their folder name\n"
     ]
    }
   ],
   "source": [
    "print('We perfomed %s scans with \"head\" in their folder name' % len(Data[Data['Scan'].str.contains('head')]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data[['Fish', 'Scan',\n",
    "      'Voxelsize', 'Scanner',\n",
    "      'Scan date', 'CameraWindow',\n",
    "      'RotationStep', 'Averaging',\n",
    "      'Scan time', 'Stacks']].to_excel('Details.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not running_on_binder:\n",
    "    Data[['Fish', 'Scan',\n",
    "          'Voxelsize', 'Scanner',\n",
    "          'Scan date', 'CameraWindow',\n",
    "          'RotationStep', 'Averaging',\n",
    "          'Scan time', 'Stacks']].to_excel(os.path.join(Root, 'Details.xlsx'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not running_on_binder:\n",
    "    # Read Mikkis datafile\n",
    "    MikkisFile = sorted(glob.glob(os.path.join(Root, 'X_ArchiveFiles', '*CTscanFishList.xlsx')))[0]\n",
    "    # Read excel file and use the first column as index\n",
    "    print('Reading in %s' % MikkisFile)\n",
    "    DataMikki = pandas.read_excel(MikkisFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not running_on_binder:\n",
    "    DataMikki.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the fish we look at and display all the info we know about it\n",
    "# Set a substring you're looking for to the variable below\n",
    "# In which jar can we find it?\n",
    "fish = '104061'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DataMikki' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [184]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# In which jar should it be/go?\u001b[39;00m\n\u001b[1;32m      2\u001b[0m foundfishes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m d, row \u001b[38;5;129;01min\u001b[39;00m \u001b[43mDataMikki\u001b[49m\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mstr\u001b[39m(fish)\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(row\u001b[38;5;241m.\u001b[39mFishec)\u001b[38;5;241m.\u001b[39mlower()) \u001b[38;5;129;01mor\u001b[39;00m \\\n\u001b[1;32m      5\u001b[0m     (\u001b[38;5;28mstr\u001b[39m(fish)\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(row\u001b[38;5;241m.\u001b[39mFieldID)\u001b[38;5;241m.\u001b[39mlower()) \u001b[38;5;129;01mor\u001b[39;00m \\\n\u001b[1;32m      6\u001b[0m     (\u001b[38;5;28mstr\u001b[39m(fish)\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(row\u001b[38;5;241m.\u001b[39mOtherID)\u001b[38;5;241m.\u001b[39mlower()) \u001b[38;5;129;01mor\u001b[39;00m \\\n\u001b[1;32m      7\u001b[0m     (\u001b[38;5;28mstr\u001b[39m(fish)\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(row\u001b[38;5;241m.\u001b[39mReplacementID)\u001b[38;5;241m.\u001b[39mlower()):\n\u001b[1;32m      8\u001b[0m         foundfishes \u001b[38;5;241m=\u001b[39m (row\u001b[38;5;241m.\u001b[39mFishec, row\u001b[38;5;241m.\u001b[39mFieldID, row\u001b[38;5;241m.\u001b[39mOtherID, row\u001b[38;5;241m.\u001b[39mReplacementID)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'DataMikki' is not defined"
     ]
    }
   ],
   "source": [
    "if not running_on_binder:\n",
    "    # In which jar should it be/go?\n",
    "    foundfishes = 0\n",
    "    for d, row in DataMikki.iterrows():\n",
    "        if (str(fish).lower() in str(row.Fishec).lower()) or \\\n",
    "        (str(fish).lower() in str(row.FieldID).lower()) or \\\n",
    "        (str(fish).lower() in str(row.OtherID).lower()) or \\\n",
    "        (str(fish).lower() in str(row.ReplacementID).lower()):\n",
    "            foundfishes = (row.Fishec, row.FieldID, row.OtherID, row.ReplacementID)\n",
    "            # remove nan from the list of hits\n",
    "            foundfishes = [str(x).lower() for x in foundfishes if not pandas.isnull(x)]\n",
    "            print('*%s*: The fish ' % fish, end='')\n",
    "            if len(foundfishes) > 1:\n",
    "                for found in foundfishes:\n",
    "                    print(found.upper(), end='/')\n",
    "            else:\n",
    "                print(foundfishes[0].upper(), end='')\n",
    "            print(' should now go in jar \"length=%s cm\" (%s))' % (row['Length(cm)'],\n",
    "                                                                  row['TemporaryJar']))\n",
    "    if not foundfishes:\n",
    "        print('*%s*: Nothing found in %s' % (fish, MikkisFile))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*104061*: Found on disk in /media/habi/Fast_SSD/EAWAG/104061\n"
     ]
    }
   ],
   "source": [
    "if not running_on_binder:\n",
    "    # Do we have something from this fish on disk?\n",
    "    ondisk = glob.glob(os.path.join(Root, '*%s*' % fish))\n",
    "    if len(ondisk):\n",
    "        for found in ondisk:\n",
    "            print('*%s*: Found on disk in %s' % (fish, found))\n",
    "            foundondisk = 1\n",
    "    else:\n",
    "        print('*%s*: Nothing found in %s' % (fish, Root))\n",
    "        foundondisk = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*104061*: We have a folder (/media/habi/Fast_SSD/EAWAG/104061) for this sample, but nothing in the dataframe, so it probably is all good\n",
      "Check the folder to be shure\n"
     ]
    }
   ],
   "source": [
    "if not running_on_binder:\n",
    "    # Did we scan it already?\n",
    "    found = 0\n",
    "    for c, row in Data.iterrows():\n",
    "        if fish in row.Fish:\n",
    "            print('*%s*: Sample %s/%s was scanned on %s' % (fish, row['Fish'], row['Scan'], row['Scan date']))\n",
    "            found = 1\n",
    "    if not found:\n",
    "        if foundondisk:\n",
    "            print('*%s*: We have a folder (%s) for this sample, but nothing in the dataframe, so it probably is all good' % (fish, ondisk[0]))\n",
    "            print('Check the folder to be shure')\n",
    "        else:\n",
    "            print('*%s*: Nothing about this sample is found in our dataframe' % fish)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can we find it in FullHeadList.txt?\n",
    "def findinFullHeadList(sample):\n",
    "    ''' Look for the sample in the FullHeadList.txt file'''\n",
    "    fullheadlist = glob.glob(os.path.join(Root, 'FullHeadList.*'))[0]\n",
    "    found = 0\n",
    "    with open(fullheadlist, 'r') as f:\n",
    "        for line in f:\n",
    "            if str(sample) in line:\n",
    "                print(line.strip())\n",
    "                found = 1\n",
    "    if not found:\n",
    "        return('*%s*: Nothing found in %s' % (sample, fullheadlist))\n",
    "    else:\n",
    "        return(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "findinFullHeadList(fish)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We found these comment files in our dataframe\n",
      "--------------------------------------------------------------------------------\n",
      "- /media/habi/Fast_SSD/EAWAG/104061/104061.rec.md\n",
      "----------\n",
      "# Fish 104061, Scan rec\n",
      "\n",
      "This fish was scanned on 2021-07-15T15:23:55 on the SkyScan2214, with a voxel size of 8.95 μm.\n",
      "\n",
      "## Comments\n",
      "--------------------------------------------------------------------------------\n",
      "- /media/habi/Fast_SSD/EAWAG/104061/104061.rec_rescan.md\n",
      "----------\n",
      "# Fish 104061, Scan rec_rescan\n",
      "BEEEEP!\n",
      "\n",
      "This fish was scanned on 2021-08-20T11:06:31 on the SkyScan2214, with a voxel size of 10.0 μm.\n",
      "\n",
      "## Comments\n",
      "--------------------------------------------------------------------------------\n",
      "- /media/habi/Fast_SSD/EAWAG/104061/head/104061.head_rec.md\n",
      "----------\n",
      "# Fish 104061, Scan head_rec\n",
      "\n",
      "This fish was scanned on 2022-02-18T10:03:36 on the SkyScan2214, with a voxel size of 29.0 μm.\n",
      "\n",
      "## f\n",
      "--------------------------------------------------------------------------------\n",
      "- /media/habi/Fast_SSD/EAWAG/104061/head/104061.head_rec_pressure_loss.md\n",
      "----------\n",
      "# Fish 104061, Scan head_rec_pressure_loss\n",
      "\n",
      "This fish was scanned on 2022-02-17T11:42:34 on the SkyScan2214, with a voxel size of 30.0 μm.\n",
      "\n",
      "## f\n",
      "--------------------------------------------------------------------------------\n",
      "- /media/habi/Fast_SSD/EAWAG/104061/head/README.md\n",
      "----------\n",
      "---\n",
      "title: Pressure loss\n",
      "author: David Haberthür\n",
      "date: 18.02.2022\n",
      "---\n",
      "\n",
      "The two `pressure_loss*` directories in here are from scans that were b0rked because of pressure loss in the pressurized air circuit in the anatomy (down from 6 bar to 4.5 bar).\n",
      "This lead to the 2214 'loosing the connection to the air bearing' and aborting the scans.\n",
      "The first (OJ) segment of the `pressure_loss` director can be reconstructed and is reconstructed, the second is completely b0rked, but kept for potentially looking into the issue with Bruker.\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "if not running_on_binder:\n",
    "    # Do we need to rescan this fish\n",
    "    # Find all relevant comment files\n",
    "    commentfiles = glob.glob(os.path.join(Root, '*%s*' % fish, '**', '*.md'), recursive=True)\n",
    "    print('We found these comment files in our dataframe')\n",
    "    for c, row in Data.iterrows():\n",
    "        if fish in row.Fish:\n",
    "            print('\\t-', row.CommentFile)\n",
    "            found = 1\n",
    "    print(80 * '-')\n",
    "    if len(commentfiles):\n",
    "        for commentfile in commentfiles:\n",
    "            print('-', commentfile)\n",
    "            print(10 * '-')\n",
    "            with open(commentfile, 'r', encoding='utf-8') as file:\n",
    "                for line in file:\n",
    "                    print(line.strip())\n",
    "                    if 'rescan' in line:\n",
    "                        print('BEEEEP!')\n",
    "            print(80 * '-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "60 of the fishes need complete head scans.\n",
    "Let's try to go through Mikkis/Kassandras list and see how far we progressed through that list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WHERE is missing a head-scan on disk, but is found on line 12 of the full head list\n",
      "WHERE is missing a head-scan on disk, but is found on line 13 of the full head list\n",
      "103754 is missing a head-scan on disk, but is found on line 18 of the full head list\n",
      "103778 is missing a head-scan on disk, but is found on line 19 of the full head list\n",
      "TJ3 is missing a head-scan on disk, but is found on line 21 of the full head list\n",
      "11965 is missing a head-scan on disk, but is found on line 24 of the full head list\n",
      "10618 is missing a head-scan on disk, but is found on line 26 of the full head list\n",
      "11807 is missing a head-scan on disk, but is found on line 27 of the full head list\n",
      "10448 is missing a head-scan on disk, but is found on line 28 of the full head list\n",
      "13115 is missing a head-scan on disk, but is found on line 30 of the full head list\n",
      "103734 is missing a head-scan on disk, but is found on line 32 of the full head list\n",
      "104021 is missing a head-scan on disk, but is found on line 33 of the full head list\n",
      "104061 is missing a head-scan on disk, but is found on line 34 of the full head list\n",
      "104621 is missing a head-scan on disk, but is found on line 35 of the full head list\n",
      "2801 is missing a head-scan on disk, but is found on line 44 of the full head list\n",
      "2800 is missing a head-scan on disk, but is found on line 45 of the full head list\n",
      "N/A is missing a head-scan on disk, but is found on line 46 of the full head list\n",
      "N/A is missing a head-scan on disk, but is found on line 47 of the full head list\n",
      "103761 is missing a head-scan on disk, but is found on line 53 of the full head list\n",
      "11447 is missing a head-scan on disk, but is found on line 54 of the full head list\n",
      "104671 is missing a head-scan on disk, but is found on line 56 of the full head list\n",
      "10605 is missing a head-scan on disk, but is found on line 64 of the full head list\n",
      "10715 is missing a head-scan on disk, but is found on line 65 of the full head list\n",
      "10791 is missing a head-scan on disk, but is found on line 66 of the full head list\n",
      "10794 is missing a head-scan on disk, but is found on line 67 of the full head list\n",
      "11045 is missing a head-scan on disk, but is found on line 69 of the full head list\n",
      "11053 is missing a head-scan on disk, but is found on line 70 of the full head list\n",
      "11322 is missing a head-scan on disk, but is found on line 73 of the full head list\n",
      "11344 is missing a head-scan on disk, but is found on line 74 of the full head list\n",
      "11500 is missing a head-scan on disk, but is found on line 75 of the full head list\n",
      "11557 is missing a head-scan on disk, but is found on line 76 of the full head list\n",
      "11601 is missing a head-scan on disk, but is found on line 77 of the full head list\n",
      "11639 is missing a head-scan on disk, but is found on line 78 of the full head list\n",
      "11729 is missing a head-scan on disk, but is found on line 79 of the full head list\n",
      "11946 is missing a head-scan on disk, but is found on line 80 of the full head list\n",
      "11992 is missing a head-scan on disk, but is found on line 81 of the full head list\n",
      "13069 is missing a head-scan on disk, but is found on line 83 of the full head list\n",
      "13393 is missing a head-scan on disk, but is found on line 84 of the full head list\n",
      "13492 is missing a head-scan on disk, but is found on line 85 of the full head list\n",
      "14128 is missing a head-scan on disk, but is found on line 86 of the full head list\n",
      "103571 is missing a head-scan on disk, but is found on line 88 of the full head list\n",
      "103635 is missing a head-scan on disk, but is found on line 89 of the full head list\n",
      "103658 is missing a head-scan on disk, but is found on line 90 of the full head list\n",
      "104016 is missing a head-scan on disk, but is found on line 92 of the full head list\n",
      "104042 is missing a head-scan on disk, but is found on line 93 of the full head list\n",
      "105105 is missing a head-scan on disk, but is found on line 94 of the full head list\n",
      "106816 is missing a head-scan on disk, but is found on line 95 of the full head list\n",
      "109209 is missing a head-scan on disk, but is found on line 96 of the full head list\n",
      "109220 is missing a head-scan on disk, but is found on line 97 of the full head list\n",
      "109320 is missing a head-scan on disk, but is found on line 98 of the full head list\n",
      "131282 is missing a head-scan on disk, but is found on line 100 of the full head list\n",
      "161476 is missing a head-scan on disk, but is found on line 102 of the full head list\n",
      "161543 is missing a head-scan on disk, but is found on line 103 of the full head list\n",
      "AN33 is missing a head-scan on disk, but is found on line 105 of the full head list\n",
      "BH58 is missing a head-scan on disk, but is found on line 106 of the full head list\n",
      "IG104 is missing a head-scan on disk, but is found on line 107 of the full head list\n",
      "IG142 is missing a head-scan on disk, but is found on line 108 of the full head list\n",
      "IG156 is missing a head-scan on disk, but is found on line 109 of the full head list\n",
      "IG161 is missing a head-scan on disk, but is found on line 110 of the full head list\n",
      "KAT-13 is missing a head-scan on disk, but is found on line 111 of the full head list\n",
      "KC-31 is missing a head-scan on disk, but is found on line 112 of the full head list\n",
      "MA31 is missing a head-scan on disk, but is found on line 113 of the full head list\n",
      "NY75 is missing a head-scan on disk, but is found on line 114 of the full head list\n"
     ]
    }
   ],
   "source": [
    "if not running_on_binder:\n",
    "    # Read in full head list, go through all the scans we alredy did and see what needs to be done\n",
    "    fullheadlist = glob.glob(os.path.join(Root, '*Head*.txt'))[0]\n",
    "    HeadsToBeScanned = []\n",
    "    with open(fullheadlist, 'r', encoding='utf-8') as file:\n",
    "        headdone = False\n",
    "        for ln, line in enumerate(file):\n",
    "            if line.strip():  # skip empty lines\n",
    "                # The first 'item' on the line should be the fish ID\n",
    "                fish = line.strip().split()[0].replace(',', '').upper()\n",
    "                # Let's ignore some lines which don't start with a fish ID\n",
    "                # The set-join here removes duplicate characters from the string (e.g. =====, !! and ::)\n",
    "                if len(''.join(set(fish))) > 2:\n",
    "                    for c, row in Data[Data.Fish == fish].iterrows():\n",
    "                        if 'head' in row.Scan:\n",
    "                            # print('\\t%s has a head-scan' % row.Fish)\n",
    "                            # print('%s has a head-scan on disk, and is found on line %s of the full head list' % (fish, ln + 1))\n",
    "                            headdone = True\n",
    "                        else:\n",
    "                            headdone = False\n",
    "                    # At this point we have either found the fish in the list or 'headdone' is false\n",
    "                    if not headdone:\n",
    "                        print('%s is missing a head-scan on disk, but is found on line %s of the full head list' % (fish, ln + 1))\n",
    "                        HeadsToBeScanned.append(fish)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not running_on_binder:\n",
    "    # Fish 10448 can be ignored because we did another scan after the head-scan, so we reset \"headdone\" in the loop above\n",
    "    # We could probably do it in a more clever way, but already spent too much time on this part :)\n",
    "    try:\n",
    "        HeadsToBeScanned.remove('10448')\n",
    "        # HeadsToBeScanned.remove('105515')\n",
    "    except ValueError:\n",
    "        # Nothing to see here, pass along\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DataMikki' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [68]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m fish \u001b[38;5;129;01min\u001b[39;00m HeadsToBeScanned:\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m# In which jar should we look for the fishes we still need to scan the head of?\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     foundfishes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m----> 4\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m d, row \u001b[38;5;129;01min\u001b[39;00m \u001b[43mDataMikki\u001b[49m\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[1;32m      5\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mstr\u001b[39m(fish)\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(row\u001b[38;5;241m.\u001b[39mFishec)\u001b[38;5;241m.\u001b[39mlower()) \u001b[38;5;129;01mor\u001b[39;00m \\\n\u001b[1;32m      6\u001b[0m         (\u001b[38;5;28mstr\u001b[39m(fish)\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(row\u001b[38;5;241m.\u001b[39mFieldID)\u001b[38;5;241m.\u001b[39mlower()) \u001b[38;5;129;01mor\u001b[39;00m \\\n\u001b[1;32m      7\u001b[0m         (\u001b[38;5;28mstr\u001b[39m(fish)\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(row\u001b[38;5;241m.\u001b[39mOtherID)\u001b[38;5;241m.\u001b[39mlower()) \u001b[38;5;129;01mor\u001b[39;00m \\\n\u001b[1;32m      8\u001b[0m         (\u001b[38;5;28mstr\u001b[39m(fish)\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(row\u001b[38;5;241m.\u001b[39mReplacementID)\u001b[38;5;241m.\u001b[39mlower()):\n\u001b[1;32m      9\u001b[0m             foundfishes \u001b[38;5;241m=\u001b[39m (row\u001b[38;5;241m.\u001b[39mFishec, row\u001b[38;5;241m.\u001b[39mFieldID, row\u001b[38;5;241m.\u001b[39mOtherID, row\u001b[38;5;241m.\u001b[39mReplacementID)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'DataMikki' is not defined"
     ]
    }
   ],
   "source": [
    "if not running_on_binder:\n",
    "    for fish in HeadsToBeScanned:\n",
    "        # In which jar should we look for the fishes we still need to scan the head of?\n",
    "        foundfishes = 0\n",
    "        for d, row in DataMikki.iterrows():\n",
    "            if (str(fish).lower() in str(row.Fishec).lower()) or \\\n",
    "            (str(fish).lower() in str(row.FieldID).lower()) or \\\n",
    "            (str(fish).lower() in str(row.OtherID).lower()) or \\\n",
    "            (str(fish).lower() in str(row.ReplacementID).lower()):\n",
    "                foundfishes = (row.Fishec, row.FieldID, row.OtherID, row.ReplacementID)\n",
    "                # remove nan from the list of hits\n",
    "                foundfishes = [str(x).lower() for x in foundfishes if not pandas.isnull(x)]\n",
    "                print('*%s*: A fish called ' % fish, end='')\n",
    "                if len(foundfishes) > 1:\n",
    "                    for found in foundfishes:\n",
    "                        print(found.upper(), end='/')\n",
    "                else:\n",
    "                    print(foundfishes[0].upper(), end='')\n",
    "                print(' should be found in jar \"length=%s cm\" (%s))' % (row['Length(cm)'],\n",
    "                                                                        row['TemporaryJar']))\n",
    "        if not foundfishes:\n",
    "            print('*%s*: Nothing found in %s' % (fish, MikkisFile))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some of the reconstructions need to be looked at?\n",
    "# Mikki wrote something about this into the files.\n",
    "# Get a list of *all* comment files\n",
    "CommentFiles = glob.glob(os.path.join(Root, '**', '*.md'), recursive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Going through all the 185 comments files we find\n",
      "  0/185: 10151/10151.rec.md: 08.02.2022 - ML - Quality check PNG uploaded to the folder. The artifacts are pretty bad, not sure if re alignment will fix it entirely\n",
      "  1/185: 10151/10151.rec_oj.md: 2022.08.12 ML - OJ has many artifacts, please do an alignment check\n",
      " 27/185: 103754/103754.rec_pj.md: 2022.08.12 ML: this rec_pj scan does not contain a full pharyngeal jaw.\n",
      " 31/185: 103761/103761.rec.md: ML 16.11.2021:  OJ and PJ need rescan at lower voxel where possible.\n",
      " 32/185: 103761/103761.rec_oj_2.md: 2022.08.13 ML - The upper right jaw teeth are cropped. Can this be salvaged from a re reconstruction of rec_oj_2?\n",
      " 37/185: 103767/103767.rec.md: ML 16.11.2021 - Tried re reconstructing, OJ has artifacts connected the jaws. Please rescan just the OJ at lower voxel size if possible.\n",
      " 63/185: 10448/10448.rec.md: - According to Mikki \"try re reconstructing to better later segmentation, OJ artifacts, PJ is good\", so we 'only' optimize on oral jaw.\n",
      " 93/185: 10618/10618.head_rec.md: ML 15.11.2021: Edge of operculum missing in the head scan\n",
      " 97/185: 10619/10619.rec.md: ML 15.11.2021: Needs a full head scan\n",
      " 97/185: 10619/10619.rec.md: ML 2022.08: \trec file is of the whole head at 14μm where I have cropped the OJ,\n",
      "103/185: 10628/10628.rec_head.md: 2022.08.10 ML: There are no files in the head folder\n",
      "108/185: 10628/head_13um/10628.rec.md: Needs to be rescanned, according to XLS sheet from Mikki\n",
      "108/185: 10628/head_13um/10628.rec.md: ML 15.11.2021:  Reconfirming, needs rescanning - for full head, OJ and PJ.\n",
      "109/185: 10628/head_18um/10628.head_18um_rec.md: ML 15.11.2021:  Head not complete - operculum is cut off\n",
      "127/185: 10794/10794.rec_rescan.md: OJ still has artifacts - 21.09.2021, ML\n",
      "130/185: 109188/109188.rec.md: 02.11.21, DH: The fish was not aligned nicely perpendicular in the sample holder. I re-reconstructed the data *without* a ROI. Parts of the OJ might still be outside of the visible region. @Mikki, can you double-check\n",
      "130/185: 109188/109188.rec.md: 01.02.2022, ML: Sorry I didn't see your text. The scan is still cropped on the specimen's OJ, left side. Will need rescanning of the OJ.\n",
      "145/185: 11116/11116.rec.md: ML 15.11.2021: Head scan - top of the skull the 'mohawk bone' (supraoccipital bone) is cropped. So close! but not complete\n",
      "155/185: 11447/11447.rec_oj.md: 2022.08.18 ML - Can the rec_oj be realigned?\n",
      "169/185: 11729/11729.rec.md: ML 15.11.2021 - confirmation segementation is much better.\n",
      "172/185: 11807/11807.rec.md: - Mikki and David need to discuss this in detail\n",
      "173/185: 11807/head/11807.head_rec.md: 15.02.2022 - ML - sub opercle on right hand side is cropped - the left side is intact, so rescanning is not high priority, but would be nice\n",
      "178/185: 11965/11965.rec_rescan.md: Oral jaw has artifacts in segmentation, can we try re reconstruction on the rescan - 21.09.2021, ML\n",
      "180/185: 11992/11992.rec.md: 08.02.2022 - ML - can realignment be done with focus on the PJ?\n",
      "180/185: 11992/11992.rec.md: 15.02.2022 - DH - Tried realignment, but it looks like there is some movemen artefacts in the PJ. Needs rescanning.\n"
     ]
    }
   ],
   "source": [
    "# Read what we want\n",
    "print('Going through all the %s comments files we find' % len(CommentFiles))\n",
    "for c, cf in enumerate(CommentFiles):\n",
    "    with open(cf, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            if 'Mikki' in line:\n",
    "                print('%03s/%s: %s: %s' % (c, len(CommentFiles), cf[len(Root) + 1:], line.strip()))\n",
    "            elif 'ML' in line:\n",
    "                print('%03s/%s: %s: %s' % (c, len(CommentFiles), cf[len(Root) + 1:], line.strip()))\n",
    "            elif 'realign' in line:\n",
    "                print('%03s/%s: %s: %s' % (c, len(CommentFiles), cf[len(Root) + 1:], line.strip()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data.sort_values(['Scan date'], ascending=False, inplace=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
