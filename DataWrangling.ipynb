{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract all the data of all the scans\n",
    "\n",
    "Wrestle with all the log files of all the scans.\n",
    "We double-check all scanning and reconstruction parameters to look for inconsistencies to be corrected.\n",
    "At the end we generate some helping files which we need for collaboration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First set up the notebook with some imports and defaults."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the python modules we need\n",
    "import platform\n",
    "import os\n",
    "import glob\n",
    "import pandas\n",
    "import imageio\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib_scalebar.scalebar import ScaleBar\n",
    "import seaborn\n",
    "import dask\n",
    "import dask_image.imread\n",
    "from dask.distributed import Client, LocalCluster\n",
    "import skimage\n",
    "from tqdm import notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load our own log file parsing code\n",
    "from BrukerSkyScanLogfileRuminator.parsing_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set dask temporary folder\n",
    "# Do this before creating a client: https://stackoverflow.com/a/62804525/323100\n",
    "import tempfile\n",
    "if 'Linux' in platform.system():\n",
    "    # Check if me mounted the FastSSD, otherwise go to standard tmp file\n",
    "    if os.path.exists(os.path.join(os.sep, 'media', 'habi', 'Fast_SSD')):\n",
    "        tmp = os.path.join(os.sep, 'media', 'habi', 'Fast_SSD', 'tmp')\n",
    "    else:\n",
    "        tmp = tempfile.gettempdir()\n",
    "elif 'Darwin' in platform.system():\n",
    "    tmp = tempfile.gettempdir()\n",
    "else:\n",
    "    if 'anaklin' in platform.node():\n",
    "        tmp = os.path.join('F:\\\\tmp')\n",
    "    else:\n",
    "        tmp = os.path.join('D:\\\\tmp')\n",
    "dask.config.set({'temporary_directory': tmp})\n",
    "print('Dask temporary files go to %s' % dask.config.get('temporary_directory'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "client = Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('You can seee what DASK is doing at \"http://localhost:%s/status\"' % client.scheduler_info()['services']['dashboard'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up figure defaults\n",
    "plt.rc('image', cmap='gray', interpolation='nearest')  # Display all images in b&w and with 'nearest' interpolation\n",
    "plt.rcParams['figure.figsize'] = (16, 9)  # Size up figures a bit\n",
    "plt.rcParams['figure.dpi'] = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup scale bar defaults\n",
    "plt.rcParams['scalebar.location'] = 'lower right'\n",
    "plt.rcParams['scalebar.frameon'] = False\n",
    "plt.rcParams['scalebar.color'] = 'white'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the (tomographic) data can reside on different drives we set a folder to use below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different locations if running either on Linux or Windows\n",
    "FastSSD = False\n",
    "nanoct = False  # Load the data directly from the 2214\n",
    "overthere = True  # Load the data directly from the iee-research_storage drive\n",
    "# to speed things up significantly\n",
    "if 'Linux' in platform.system():\n",
    "    if FastSSD:\n",
    "        BasePath = os.path.join(os.sep, 'media', 'habi', 'Fast_SSD')\n",
    "    elif overthere:\n",
    "        BasePath = os.path.join(os.sep, 'home', 'habi', 'research-storage-iee', 'microCT')\n",
    "    elif nanoct:\n",
    "        BasePath = os.path.join(os.path.sep, 'home', 'habi', '2214')\n",
    "    else:\n",
    "        BasePath = os.path.join(os.sep, 'home', 'habi', '1272')\n",
    "elif 'Darwin' in platform.system():\n",
    "    FastSSD = False\n",
    "    BasePath = os.path.join('/Users/habi/Dev/EAWAG/Data')\n",
    "elif 'Windows' in platform.system():\n",
    "    if FastSSD:\n",
    "        BasePath = os.path.join('F:\\\\')\n",
    "    else:\n",
    "        if overthere:\n",
    "            BasePath = os.path.join('\\\\\\\\resstore.unibe.ch', 'iee_aqua', 'microCTupload')\n",
    "        elif nanoct:\n",
    "            BasePath = os.path.join('N:\\\\')\n",
    "        else:\n",
    "            BasePath = os.path.join('D:\\\\Results')\n",
    "if overthere:\n",
    "    Root = BasePath\n",
    "else:\n",
    "    Root = os.path.join(BasePath, 'EAWAG')\n",
    "print('We are loading all the data from %s' % Root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We generate some output in this notebook.\n",
    "To make all the data completely reproducible, save the output to a directory named according to the current `git` hash of the repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_git_hash():\n",
    "    '''\n",
    "    Get the current git hash from the repository.\n",
    "    Based on http://stackoverflow.com/a/949391/323100 and\n",
    "    http://stackoverflow.com/a/18283905/323100\n",
    "    '''\n",
    "    from subprocess import Popen, PIPE\n",
    "    import os\n",
    "    gitprocess = Popen(['git',\n",
    "                        '--git-dir',\n",
    "                        os.path.join(os.getcwd(), '.git'),\n",
    "                        'rev-parse',\n",
    "                        '--short',\n",
    "                        '--verify',\n",
    "                        'HEAD'],\n",
    "                       stdout=PIPE)\n",
    "    (output, _) = gitprocess.communicate()\n",
    "    return output.strip().decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make directory for output\n",
    "OutPutDir = os.path.join(os.getcwd(), 'Output', get_git_hash())\n",
    "print('We are saving all the output to %s' % OutPutDir)\n",
    "os.makedirs(OutPutDir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we are set up, actually start to load/ingest the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make us a dataframe for saving all that we need\n",
    "Data = pandas.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get *all* log files, unsorted but fast\n",
    "Data['LogFile'] = [os.path.join(root, name)\n",
    "                   for root, dirs, files in os.walk(Root)\n",
    "                   for name in files\n",
    "                   if name.endswith((\".log\"))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Let's try to optimize the timing, based on https://stackoverflow.com/a/27565420/323100\n",
    "# n, t = 0, time.time()\n",
    "# LogFiles = [os.path.join(root, name)\n",
    "#             for root, dirs, files in os.walk(Root)\n",
    "#             for name in files\n",
    "#             if name.endswith((\".log\"))]\n",
    "# t = time.time() - t\n",
    "# print(\"os.walk: %.4fs, %d files found\" % (t, len(LogFiles)))\n",
    "\n",
    "# n, t = 0, time.time()\n",
    "# globfiles = [f for f in glob.glob(os.path.join(Root, '**', '*.log'),\n",
    "#                                   recursive=True)]\n",
    "# t = time.time() - t\n",
    "# print(\"glob.glob, unsorted: %.4fs, %d files found\" % (t, len(globfiles)))\n",
    "\n",
    "# n, t = 0, time.time()\n",
    "# globfiles = [f for f in sorted(glob.glob(os.path.join(Root, '**', '*.log'),\n",
    "#                                          recursive=True),\n",
    "#                                key=os.path.getmtime)]\n",
    "# t = time.time() - t\n",
    "# print(\"glob.glob, sorted: %.4fs, %d files found\" % (t, len(globfiles)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On the FastSSD, sorted glob is about half as fast as os.walk, and unsorted glob is still substantially slower than walk!\n",
    "# glob.glob: 0.7773s, 1206 files found\n",
    "# os.walk: 0.3810s, 1206 files found"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The notebook might not be running locally on our machines, but on Binder.\n",
    "There, the user has no access to the log files, so we fail back to a local copy of them.\n",
    "This also means that no reconstructions are available, und we thus cannot count them.\n",
    "We thus set a variable which skips looking for parameters related to the reconstructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not len(Data):\n",
    "    # Our dataframe is empty.\n",
    "    # We might be running on Binder, e.g. load the logfiles from the subfolder in this repository\n",
    "    print(10 * ' -', 'CAVEAT', 10 * ' -')\n",
    "    print('You are most probably running the notebook on binder.')\n",
    "    print('And thus do not have access to the log files on the research storage')\n",
    "    print('We are using a \"local\" copy of the data in the `logfiles` subfolder')\n",
    "    print('This gives correct, but possibly outdated results...')\n",
    "    print(10 * ' -', 'CAVEAT', 10 * ' -')\n",
    "    # Change root folder\n",
    "    Root = 'logfiles'\n",
    "    # Load log files again\n",
    "    Data['LogFile'] = [f for f in sorted(glob.glob(os.path.join(Root, '**', '*.log'),\n",
    "                                                   recursive=True),\n",
    "                                         key=os.path.getmtime)]\n",
    "    running_on_binder = True\n",
    "else:\n",
    "    running_on_binder = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all folders\n",
    "Data['Folder'] = [os.path.dirname(f) for f in Data['LogFile']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not running_on_binder:\n",
    "    # Check for samples which are not yet reconstructed\n",
    "    for c, row in Data.iterrows():\n",
    "        # Iterate over every 'proj' folder\n",
    "        if 'proj' in row.Folder:\n",
    "            if 'TScopy' not in row.Folder and 'PR' not in row.Folder:\n",
    "                # If there's nothing with 'rec*' on the same level, then tell us\n",
    "                if not glob.glob(row.Folder.replace('proj', 'rec')):\n",
    "                    # print(glob.glob(row.Folder.replace('proj', 'rec')))\n",
    "                    print('- %s is missing matching reconstructions' % row.LogFile[len(Root) + 1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get rid of all non-rec logfiles\n",
    "for c, row in Data.iterrows():\n",
    "    if 'rec' not in row.Folder:\n",
    "        Data.drop([c], inplace=True)\n",
    "    elif 'rectmp.log' in row.LogFile:\n",
    "        Data.drop([c], inplace=True)\n",
    "# Reset dataframe index\n",
    "Data = Data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate us some meaningful colums\n",
    "Data['Fish'] = [l[len(Root) + 1:].split(os.sep)[0] for l in Data['LogFile']]\n",
    "Data['Scan'] = ['.'.join(l[len(Root) + 1:].split(os.sep)[1:-1]) for l in Data['LogFile']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Debug output\n",
    "# for log in Data['LogFile']:\n",
    "#     try:\n",
    "#         (pixelsize(log))\n",
    "#     except:\n",
    "#         print(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get parameters related to scan from logfiles\n",
    "Data['Voxelsize'] = [pixelsize(log) for log in Data['LogFile']]\n",
    "Data['Voltage'] = [voltage(log) for log in Data['LogFile']]\n",
    "Data['Current'] = [current(log) for log in Data['LogFile']]\n",
    "Data['Filter'] = [whichfilter(log) for log in Data['LogFile']]\n",
    "Data['Exposuretime'] = [exposuretime(log) for log in Data['LogFile']]\n",
    "Data['Scanner'] = [scanner(log) for log in Data['LogFile']]\n",
    "Data['Averaging'] = [averaging(log) for log in Data['LogFile']]\n",
    "Data['Scan date'] = [scandate(log) for log in Data['LogFile']]\n",
    "Data['ProjectionSize'] = [projection_size(log) for log in Data['LogFile']]\n",
    "Data['RotationStep'] = [rotationstep(log) for log in Data['LogFile']]\n",
    "Data['ThreeSixty'] = [threesixtyscan(log) for log in Data['LogFile']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get parameters related to reconstruction from logfiles\n",
    "Data['ReconstructionSize'] = [reconstruction_size(log) for log in Data['LogFile']]\n",
    "Data['Grayvalue'] = [reconstruction_grayvalue(log) for log in Data['LogFile']]\n",
    "Data['RingartefactCorrection'] = [ringremoval(log) for log in Data['LogFile']]\n",
    "Data['BeamHardeningCorrection'] = [beamhardening(log) for log in Data['LogFile']]\n",
    "Data['ROI'] = [region_of_interest(log) for log in Data['LogFile']]\n",
    "Data['Duration'] = [duration(log) for log in Data['LogFile']]\n",
    "Data['Stacks'] = [stacks(log) for log in Data['LogFile']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The iee research storage folder contains some folders with scans done by Kassandra on a SkyScan1273.\n",
    "# Exclude those, since they are not part of this study, we just looked at them to help her.\n",
    "for c, row in Data.iterrows():\n",
    "    if '1273' in row.Scanner:\n",
    "        print('Dropping %s from our dataframe' % row.LogFile[len(Root)+1:])\n",
    "        Data.drop([c], inplace=True)\n",
    "# Reset dataframe index\n",
    "Data = Data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The iee research storage folder contains folders with scans of only teeth, done as a small pilot study.\n",
    "# Exclude those, since they are not part of this study.\n",
    "for c, row in Data.iterrows():\n",
    "    if 'Teeth' in row.Folder:\n",
    "        print('Dropping %s from our dataframe' % row.LogFile[len(Root)+1:])\n",
    "        Data.drop([c], inplace=True)\n",
    "# Reset dataframe index\n",
    "Data = Data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort dataframe on fishes and scans\n",
    "Data.sort_values(by=['Fish', 'Scan'], inplace=True)\n",
    "# Reset dataframe index\n",
    "Data = Data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many fishes did we scan?\n",
    "# We scanned six 'BucketOfFish' so subtract those :)\n",
    "print('We have %s unique names in our corpus of scans' % (len(Data.Fish.unique()) - 6))\n",
    "print('We performed %s scans in total' % len(Data.Scan))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the file names of the reconstructions\n",
    "Data['Reconstructions'] = [[os.path.join(root, name)\n",
    "                            for root, dirs, files in os.walk(f)\n",
    "                            for name in files\n",
    "                            if 'rec0' in name and name.endswith((\".png\"))] for f in Data['Folder']]\n",
    "# Count how many files we have\n",
    "Data['Number of reconstructions'] = [len(r) for r in Data.Reconstructions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not running_on_binder:\n",
    "    # Let's see if we're missing some data\n",
    "    for c, row in Data[Data['Number of reconstructions'] == 0].iterrows():\n",
    "        print('%s/%s: Folder %s does not contain any reconstructions and '\n",
    "              'will be removed in the next step' % (c + 1,\n",
    "                                                    len(Data),\n",
    "                                                    os.path.join(row.Fish, row.Scan)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 103761/rec_rereconstruct is a folder where we tried to salvage a scan where the sample holder touched the source\n",
    "# MA31/moved_rec/ is a folder where the fish moved during the acquisition\n",
    "# MA31/stuck_rec/ is a folder where we've lost air pressure in the building and the stage got stuck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('We have %s folders in total' % (len(Data)))\n",
    "if not running_on_binder:\n",
    "    # Drop samples which have not been reconstructed yet\n",
    "    # Based on https://stackoverflow.com/a/13851602\n",
    "    # for c, row in Data.iterrows():\n",
    "    #     if not row['Number of reconstructions']:\n",
    "    #         print('%s contains no PNG files, we might be currently reconstructing it' % row.Folder)\n",
    "    Data = Data[Data['Number of reconstructions'] > 0]\n",
    "    Data.reset_index(drop=True, inplace=True)\n",
    "    print('Of which %s folders do contain reconstructions' % (len(Data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data['Total Duration'] = [st * stk for st, stk in zip(Data['Duration'], Data['Stacks'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show five smallest voxelsizes and scans\n",
    "for c, vs in enumerate(sorted(Data.Voxelsize.unique())[:5]):\n",
    "    print('-----vs: %s-----' % vs)\n",
    "    print(Data[Data.Voxelsize == vs][['Fish', 'Scan', 'Voxelsize']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show five largest voxelsizes and scans\n",
    "for c, vs in enumerate(sorted(Data.Voxelsize.unique())[-5:]):\n",
    "    print('-----vs: %s-----' % vs)\n",
    "    print(Data[Data.Voxelsize == vs][['Fish', 'Scan', 'Voxelsize']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data.Filter.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(Data.Voltage.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(Data.Current.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(Data.RingartefactCorrection.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(Data.BeamHardeningCorrection.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a text file in each rec-folder, in which Mikki and I can note what's going on with the fish\n",
    "# Generate filename\n",
    "for c, row in Data.iterrows():\n",
    "    Data.at[c, 'CommentFile'] = os.path.join(os.path.dirname(row.Folder),\n",
    "                                             row.Fish + '.' + row.Scan + '.md')\n",
    "# Create actual file on disk\n",
    "for c, row in Data.iterrows():\n",
    "    # Only do this if the file does not already exist\n",
    "    if not os.path.exists(row.CommentFile):\n",
    "        with open(row.CommentFile, 'w', encoding='utf-8') as f:\n",
    "            f.write('# Fish %s, Scan %s\\n\\n' % (row.Fish, row.Scan))\n",
    "            f.write('This fish was scanned on %s on the %s, with a voxel size of %s μm.\\n\\n'\n",
    "                    % (row['Scan date'], row.Scanner, numpy.round(row.Voxelsize, 2)))\n",
    "            f.write('## Comments')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get an overview of the total scaning time\n",
    "# Nice output based on https://stackoverflow.com/a/8907407/323100\n",
    "total_seconds = int(Data['Total Duration'].sum().total_seconds())\n",
    "hours, remainder = divmod(total_seconds, 60 * 60)\n",
    "minutes, seconds = divmod(remainder, 60)\n",
    "print('In total, we scanned for %s hours and %s minutes)' % (hours, minutes))\n",
    "for machine in Data['Scanner'].unique():\n",
    "    total_seconds = int(Data[Data['Scanner'] == machine]['Scan time total'].sum().total_seconds())\n",
    "    hours, remainder = divmod(total_seconds, 60 * 60)\n",
    "    minutes, seconds = divmod(remainder, 60)\n",
    "    print('\\t - Of these, we scanned %s hours and %s minutes on the %s,'\n",
    "          ' for %s scans' % (hours,\n",
    "                             minutes,\n",
    "                             machine,\n",
    "                             len(Data[Data['Scanner'] == machine])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We scanned six 'buckets of fish', so subtract those :)\n",
    "print('We scanned %0.f fishes' % (len(Data.Fish.unique()) - 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('We did a total of %s scans' % len(Data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('We perfomed %s scans with \"head\" in their folder name' % len(Data[Data['Scan'].str.contains('head')]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data[['Fish', 'Scan', 'LogFile',\n",
    "      'Voxelsize', 'Scanner',\n",
    "      'Scan date', 'ProjectionSize',\n",
    "      'ThreeSixty', 'RotationStep', 'Averaging',\n",
    "      'Duration', 'Stacks', 'Total Duration']].to_excel(os.path.join(OutPutDir, 'Details.xlsx'))\n",
    "print('Saved XLS sheet with some scanning details to', os.path.join(OutPutDir, 'Details.xlsx'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not running_on_binder:\n",
    "    Data[['Fish', 'Scan',\n",
    "          'Voxelsize', 'Scanner',\n",
    "          'Scan date', 'ProjectionSize',\n",
    "          'ThreeSixty', 'RotationStep', 'Averaging',\n",
    "          'Duration', 'Stacks', 'Total Duration']].to_excel(os.path.join(Root, 'Details.xlsx'))\n",
    "print('Saved XLS sheet with some scanning details to', os.path.join(Root, 'Details.xlsx'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save 'data' file for manuscript: github.com/habi/eawag-manuscript\n",
    "# Since the manuscript is in a subfolder, we can simply write the output there\n",
    "if not running_on_binder:\n",
    "    Data[['Fish', 'Scan', 'Scanner', 'Scan date',\n",
    "          'Voxelsize', 'Voltage', 'Current', 'Filter', 'Exposuretime', 'Averaging',\n",
    "          'ThreeSixty', 'RotationStep', 'ProjectionSize', 'Duration', 'Stacks', 'Total Duration',\n",
    "          'RingartefactCorrection', 'BeamHardeningCorrection', 'Grayvalue',\n",
    "          ]].to_csv(os.path.join('manuscript', 'content', 'data', 'ScanningDetails.csv'),\n",
    "               index=False,\n",
    "               header=['Fish', 'Scan', 'Scanner', 'Scan date',\n",
    "                       'Voxelsize [μm]', 'Source voltage [kV]', 'Source current [μA]',\n",
    "                       'Filter', 'Exposure time [ms]', 'Frame averaging', '360° scan', \n",
    "                       'Rotation step [°]', 'Projection size', 'Scan duration [s]', 'Stacked scans', 'Total scan duration [s]',\n",
    "                       'Ring removal correction', 'Beam hardening correction', 'Gray value mapping'])\n",
    "print('Saved CSV file with all relevant scanning and reconstruction parameters to',\n",
    "      os.path.join('manuscript', 'content', 'data', 'ScanningDetails.csv'),\n",
    "      'for using as supplementary material in the manuscript')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not running_on_binder:\n",
    "    # Read Mikkis datafile\n",
    "    MikkisFile = sorted(glob.glob(os.path.join(Root, 'X_ArchiveFiles', '*CTscanFishList.xlsx')))[0]\n",
    "    # Read excel file and use the first column as index\n",
    "    print('Reading in %s' % MikkisFile)\n",
    "    DataMikki = pandas.read_excel(MikkisFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not running_on_binder:\n",
    "    DataMikki.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataMikki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the fish we look at and display all the info we know about it\n",
    "# Set a substring you're looking for to the variable below\n",
    "# In which jar can we find it?\n",
    "fish = '104061'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not running_on_binder:\n",
    "    # In which jar should it be/go?\n",
    "    foundfishes = 0\n",
    "    for d, row in DataMikki.iterrows():\n",
    "        if (str(fish).lower() in str(row.Fishec).lower()) \\\n",
    "        or (str(fish).lower() in str(row.FieldID).lower()) \\\n",
    "        or (str(fish).lower() in str(row.OtherID).lower()) \\\n",
    "        or (str(fish).lower() in str(row.ReplacementID).lower()):\n",
    "            foundfishes = (row.Fishec, row.FieldID, row.OtherID, row.ReplacementID)\n",
    "            # remove nan from the list of hits\n",
    "            foundfishes = [str(x).lower() for x in foundfishes if not pandas.isnull(x)]\n",
    "            print('*%s*: The fish ' % fish, end='')\n",
    "            if len(foundfishes) > 1:\n",
    "                for found in foundfishes:\n",
    "                    print(found.upper(), end='/')\n",
    "            else:\n",
    "                print(foundfishes[0].upper(), end='')\n",
    "            print(' should now go in jar \"length=%s cm\" (%s))' % (row['Length(cm)'],\n",
    "                                                                  row['TemporaryJar']))\n",
    "    if not foundfishes:\n",
    "        print('*%s*: Nothing found in %s' % (fish, MikkisFile))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not running_on_binder:\n",
    "    # Do we have something from this fish on disk?\n",
    "    ondisk = glob.glob(os.path.join(Root, '*%s*' % fish))\n",
    "    if len(ondisk):\n",
    "        for found in ondisk:\n",
    "            print('*%s*: Found on disk in %s' % (fish, found))\n",
    "            foundondisk = 1\n",
    "    else:\n",
    "        print('*%s*: Nothing found in %s' % (fish, Root))\n",
    "        foundondisk = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not running_on_binder:\n",
    "    # Did we scan it already?\n",
    "    found = 0\n",
    "    for c, row in Data.iterrows():\n",
    "        if fish in row.Fish:\n",
    "            print('*%s*: Sample %s/%s was scanned on %s' % (fish, row['Fish'], row['Scan'], row['Scan date']))\n",
    "            found = 1\n",
    "    if not found:\n",
    "        if foundondisk:\n",
    "            print('*%s*: We have a folder (%s) for this sample, but nothing in the dataframe, so it probably is all good' % (fish, ondisk[0]))\n",
    "            print('Check the folder to be shure')\n",
    "        else:\n",
    "            print('*%s*: Nothing about this sample is found in our dataframe' % fish)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can we find it in FullHeadList.txt?\n",
    "def findinFullHeadList(sample):\n",
    "    ''' Look for the sample in the FullHeadList.txt file'''\n",
    "    fullheadlist = glob.glob(os.path.join(Root, 'FullHeadList.*'))[0]\n",
    "    found = 0\n",
    "    with open(fullheadlist, 'r') as f:\n",
    "        for line in f:\n",
    "            if str(sample) in line:\n",
    "                print(line.strip())\n",
    "                found = 1\n",
    "    if not found:\n",
    "        return('*%s*: Nothing found in %s' % (sample, fullheadlist))\n",
    "    else:\n",
    "        return(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not running_on_binder:\n",
    "    findinFullHeadList(fish)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not running_on_binder:\n",
    "    # Do we need to rescan this fish\n",
    "    # Find all relevant comment files\n",
    "    commentfiles = glob.glob(os.path.join(Root, '*%s*' % fish, '**', '*.md'), recursive=True)\n",
    "    print('We found these comment files in our dataframe')\n",
    "    for c, row in Data.iterrows():\n",
    "        if fish in row.Fish:\n",
    "            print('\\t-', row.CommentFile)\n",
    "            found = 1\n",
    "    print(80 * '-')\n",
    "    if len(commentfiles):\n",
    "        for commentfile in commentfiles:\n",
    "            print('-', commentfile)\n",
    "            print(10 * '-')\n",
    "            with open(commentfile, 'r', encoding='utf-8') as file:\n",
    "                for line in file:\n",
    "                    print(line.strip())\n",
    "                    if 'rescan' in line:\n",
    "                        print('BEEEEP!')\n",
    "            print(80 * '-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "60 of the fishes need complete head scans.\n",
    "Let's try to go through Mikkis/Kassandras list and see how far we progressed through that list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not running_on_binder:\n",
    "    # Read in full head list, go through all the scans we alredy did and see what needs to be done\n",
    "    fullheadlist = glob.glob(os.path.join(Root, '*Head*.txt'))[0]\n",
    "    HeadsToBeScanned = []\n",
    "    with open(fullheadlist, 'r', encoding='utf-8') as file:\n",
    "        headdone = False\n",
    "        for ln, line in enumerate(file):\n",
    "            if line.strip():  # skip empty lines\n",
    "                # The first 'item' on the line should be the fish ID\n",
    "                fish = line.strip().split()[0].replace(',', '').upper()\n",
    "                # Let's ignore some lines which don't start with a fish ID\n",
    "                # The set-join here removes duplicate characters from the string (e.g. =====, !! and ::)\n",
    "                if len(''.join(set(fish))) > 2:\n",
    "                    for c, row in Data[Data.Fish == fish].iterrows():\n",
    "                        if 'head' in row.Scan:\n",
    "                            # print('\\t%s has a head-scan' % row.Fish)\n",
    "                            # print('%s has a head-scan on disk, and is found on line %s of the full head list' % (fish, ln + 1))\n",
    "                            headdone = True\n",
    "                        else:\n",
    "                            headdone = False\n",
    "                    # At this point we have either found the fish in the list or 'headdone' is false\n",
    "                    if not headdone:\n",
    "                        print('%s is missing a head-scan on disk, but is found on line %s of the full head list' % (fish, ln + 1))\n",
    "                        HeadsToBeScanned.append(fish)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not running_on_binder:\n",
    "    # Fish 10448 can be ignored because we did another scan after the head-scan, so we reset \"headdone\" in the loop above\n",
    "    # We could probably do it in a more clever way, but already spent too much time on this part :)\n",
    "    try:\n",
    "        HeadsToBeScanned.remove('10448')\n",
    "        # HeadsToBeScanned.remove('105515')\n",
    "    except ValueError:\n",
    "        # Nothing to see here, pass along\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not running_on_binder:\n",
    "    for fish in HeadsToBeScanned:\n",
    "        # In which jar should we look for the fishes we still need to scan the head of?\n",
    "        foundfishes = 0\n",
    "        for d, row in DataMikki.iterrows():\n",
    "            if (str(fish).lower() in str(row.Fishec).lower()) \\\n",
    "            or (str(fish).lower() in str(row.FieldID).lower()) \\\n",
    "            or (str(fish).lower() in str(row.OtherID).lower()) \\\n",
    "            or (str(fish).lower() in str(row.ReplacementID).lower()):\n",
    "                foundfishes = (row.Fishec, row.FieldID, row.OtherID, row.ReplacementID)\n",
    "                # remove nan from the list of hits\n",
    "                foundfishes = [str(x).lower() for x in foundfishes if not pandas.isnull(x)]\n",
    "                print('*%s*: A fish called ' % fish, end='')\n",
    "                if len(foundfishes) > 1:\n",
    "                    for found in foundfishes:\n",
    "                        print(found.upper(), end='/')\n",
    "                else:\n",
    "                    print(foundfishes[0].upper(), end='')\n",
    "                print(' should be found in jar \"length=%s cm\" (%s))' % (row['Length(cm)'],\n",
    "                                                                        row['TemporaryJar']))\n",
    "        if not foundfishes:\n",
    "            print('*%s*: Nothing found in %s' % (fish, MikkisFile))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some of the reconstructions need to be looked at?\n",
    "# Mikki wrote something about this into the files.\n",
    "# Get a list of *all* comment files\n",
    "CommentFiles = glob.glob(os.path.join(Root, '**', '*.md'), recursive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read what we want\n",
    "print('Going through all the %s comments files we find' % len(CommentFiles))\n",
    "for c, cf in enumerate(CommentFiles):\n",
    "    with open(cf, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            if 'Mikki' in line:\n",
    "                print('%03s/%s: %s: %s' % (c, len(CommentFiles), cf[len(Root) + 1:], line.strip()))\n",
    "            elif 'ML' in line:\n",
    "                print('%03s/%s: %s: %s' % (c, len(CommentFiles), cf[len(Root) + 1:], line.strip()))\n",
    "            elif 'realign' in line:\n",
    "                print('%03s/%s: %s: %s' % (c, len(CommentFiles), cf[len(Root) + 1:], line.strip()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data.sort_values(['Scan date'], ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
