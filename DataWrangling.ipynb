{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handle the 'data' of the fishes\n",
    "Wrestle with our data and Mikkis XLS sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "import os\n",
    "import glob\n",
    "import pandas\n",
    "import imageio\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib_scalebar.scalebar import ScaleBar\n",
    "import seaborn\n",
    "import dask\n",
    "import dask_image.imread\n",
    "from dask.distributed import Client, LocalCluster\n",
    "from numcodecs import Blosc\n",
    "import skimage\n",
    "from tqdm import notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set dask temporary folder\n",
    "# Do this before creating a client: https://stackoverflow.com/a/62804525/323100\n",
    "import tempfile\n",
    "if 'Linux' in platform.system():\n",
    "    tmp = os.path.join(os.sep, 'media', 'habi', 'Fast_SSD')\n",
    "elif 'Darwin' in platform.system():\n",
    "    tmp = tempfile.gettempdir()\n",
    "else:\n",
    "    if 'anaklin' in platform.node():\n",
    "        tmp = os.path.join('F:\\\\')\n",
    "    else:\n",
    "        tmp = os.path.join('D:\\\\')\n",
    "dask.config.set({'temporary_directory': os.path.join(tmp, 'tmp')})\n",
    "print('Dask temporarry files go to %s' % dask.config.get('temporary_directory'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start cluster and client now, after setting tempdir\n",
    "cluster = LocalCluster(n_workers=8)\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('You can seee what DASK is doing at \"http://localhost:%s/status\"' % client.scheduler_info()['services']['dashboard'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Ignore warnings in the notebook\n",
    "# import warnings\n",
    "# warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up figure defaults\n",
    "plt.rc('image', cmap='gray', interpolation='nearest')  # Display all images in b&w and with 'nearest' interpolation\n",
    "plt.rcParams['figure.figsize'] = (16, 9)  # Size up figures a bit\n",
    "plt.rcParams['figure.dpi'] = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup scale bar defaults\n",
    "plt.rcParams['scalebar.location'] = 'lower right'\n",
    "plt.rcParams['scalebar.frameon'] = False\n",
    "plt.rcParams['scalebar.color'] = 'white'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display all plots identically\n",
    "lines = 3\n",
    "# And then do something like\n",
    "# plt.subplot(lines, numpy.ceil(len(Data) / float(lines)), c + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different locations if running either on Linux or Windows\n",
    "FastSSD = False\n",
    "# to speed things up significantly\n",
    "if 'Linux' in platform.system():\n",
    "    if FastSSD:\n",
    "        BasePath = os.path.join(os.sep, 'media', 'habi', 'Fast_SSD')\n",
    "    else:\n",
    "        BasePath = os.path.join(os.sep, 'home', 'habi', '1272')\n",
    "elif 'Darwin' in platform.system():\n",
    "    FastSSD = False\n",
    "    BasePath = os.path.join('/Users/habi/Dev/EAWAG/Data')\n",
    "elif 'Windows' in platform.system():\n",
    "    if FastSSD:\n",
    "        BasePath = os.path.join('F:\\\\')\n",
    "    else:\n",
    "        if 'anaklin' in platform.node():\n",
    "            BasePath = os.path.join('S:\\\\')\n",
    "        else:\n",
    "            BasePath = os.path.join('D:\\\\Results')\n",
    "Root = os.path.join(BasePath, 'EAWAG')\n",
    "print('We are loading all the data from %s' % Root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pixelsize(logfile):\n",
    "    \"\"\"Get the pixel size from the scan log file\"\"\"\n",
    "    with open(logfile, 'r') as f:\n",
    "        for line in f:\n",
    "            if 'Image Pixel' in line and 'Scaled' not in line:\n",
    "                pixelsize = float(line.split('=')[1])\n",
    "    return(pixelsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_projectionsize(logfile):\n",
    "    \"\"\"How big did we set the camera?\"\"\"\n",
    "    with open(logfile, 'r') as f:\n",
    "        for line in f:\n",
    "            if 'Number Of Rows' in line:\n",
    "                y = int(line.split('=')[1])\n",
    "            if 'Number Of Columns' in line:\n",
    "                x = int(line.split('=')[1])                \n",
    "    return(x*y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filter(logfile):\n",
    "    \"\"\"Get the filter we used whole scanning from the scan log file\"\"\"\n",
    "    with open(logfile, 'r') as f:\n",
    "        for line in f:\n",
    "            if 'Filter=' in line:\n",
    "                whichfilter = line.split('=')[1].strip()\n",
    "    return(whichfilter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_exposuretime(logfile):\n",
    "    \"\"\"Get the exposure time size from the scan log file\"\"\"\n",
    "    with open(logfile, 'r') as f:\n",
    "        for line in f:\n",
    "            if 'Exposure' in line:\n",
    "                exposuretime = int(line.split('=')[1])\n",
    "    return(exposuretime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ringartefact(logfile):\n",
    "    \"\"\"Get the ring artefact correction from the  scan log file\"\"\"\n",
    "    with open(logfile, 'r') as f:\n",
    "        for line in f:\n",
    "            if 'Ring Artifact' in line:\n",
    "                ringartefactcorrection = int(line.split('=')[1])\n",
    "    return(ringartefactcorrection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reconstruction_grayvalue(logfile):\n",
    "    grayvalue = None\n",
    "    \"\"\"How did we map the brightness of the reconstructions?\"\"\"\n",
    "    with open(logfile, 'r') as f:\n",
    "        for line in f:\n",
    "            if 'Maximum for' in line:\n",
    "                grayvalue = float(line.split('=')[1])\n",
    "    return(grayvalue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_beamhardening(logfile):\n",
    "    \"\"\"Get the beamhardening correction from the  scan log file\"\"\"\n",
    "    with open(logfile, 'r') as f:\n",
    "        for line in f:\n",
    "            if 'Hardening' in line:\n",
    "                beamhardeningcorrection = int(line.split('=')[1])\n",
    "    return(beamhardeningcorrection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rotationstep(logfile):\n",
    "    \"\"\"Get the rotation step from the scan log file\"\"\"\n",
    "    with open(logfile, 'r') as f:\n",
    "        for line in f:\n",
    "            if 'Rotation Step' in line:\n",
    "                rotstep = float(line.split('=')[1])\n",
    "    return(rotstep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_frameaveraging(logfile):\n",
    "    \"\"\"Get the frame averaging from the scan log file\"\"\"\n",
    "    with open(logfile, 'r') as f:\n",
    "        for line in f:\n",
    "            if 'Averaging' in line:\n",
    "                avg = line.split('=')[1]\n",
    "    return(avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_machine(logfile):\n",
    "    \"\"\"Get the machine we used to scan\"\"\"\n",
    "    with open(logfile, 'r') as f:\n",
    "        for line in f:\n",
    "            if 'Scanner' in line:\n",
    "                machine = line.split('=')[1].strip()\n",
    "    return(machine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scantime(logfile):\n",
    "    \"\"\"How long did we scan?\"\"\"\n",
    "    with open(logfile, 'r') as f:\n",
    "        for line in f:\n",
    "            if 'Scan duration' in line:\n",
    "                time = line.split('=')[1].strip()\n",
    "    return(pandas.to_timedelta(time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stacks(logfile):\n",
    "    \"\"\"How many stacks/connected scans did we make?\"\"\"\n",
    "    stacks = 1\n",
    "    with open(logfile, 'r') as f:\n",
    "        for line in f:\n",
    "            if 'conn' in line:\n",
    "                stacks = int(line.split('=')[1])\n",
    "    return(stacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scandate(logfile, verbose=False):\n",
    "    \"\"\"When did we scan the fish?\"\"\"\n",
    "    with open(logfile, 'r') as f:\n",
    "        for line in f:\n",
    "            if 'Study Date and Time' in line:\n",
    "                if verbose:\n",
    "                    print('Found \"date\" line: %s' % line.strip())\n",
    "                datestring = line.split('=')[1].strip().replace('  ', ' ')\n",
    "                if verbose:\n",
    "                    print('The date string is: %s' % datestring)\n",
    "                date = pandas.to_datetime(datestring , format='%d %b %Y %Hh:%Mm:%Ss')\n",
    "                if verbose:\n",
    "                    print('Parsed to: %s' % date)\n",
    "                (date)\n",
    "    return(date.isoformat())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_git_hash():\n",
    "    '''\n",
    "    Get the current git hash from the repository.\n",
    "    Based on http://stackoverflow.com/a/949391/323100 and\n",
    "    http://stackoverflow.com/a/18283905/323100\n",
    "    '''\n",
    "    from subprocess import Popen, PIPE\n",
    "    import os\n",
    "    gitprocess = Popen(['git',\n",
    "                        '--git-dir',\n",
    "                        os.path.join(os.getcwd(), '.git'),\n",
    "                        'rev-parse',\n",
    "                        '--short',\n",
    "                        '--verify',\n",
    "                        'HEAD'],\n",
    "                       stdout=PIPE)\n",
    "    (output, _) = gitprocess.communicate()\n",
    "    return output.strip().decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Make directory for output\n",
    "# OutPutDir = os.path.join(os.getcwd(), 'Output', get_git_hash())\n",
    "# print('We are saving all the output to %s' % OutPutDir)\n",
    "# os.makedirs(OutPutDir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make us a dataframe for saving all that we need\n",
    "Data = pandas.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get *all* log files\n",
    "# Sort them by time, not name\n",
    "Data['LogFile'] = [f for f in sorted(glob.glob(os.path.join(Root, '**', '*.log'), recursive=True), key=os.path.getmtime)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all folders\n",
    "Data['Folder'] = [os.path.dirname(f) for f in Data['LogFile']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for samples which are not yet reconstructed\n",
    "for c, row in Data.iterrows():\n",
    "    # Iterate over every 'proj' folder\n",
    "    if 'proj' in row.Folder:\n",
    "        if not 'TScopy' in row.Folder and not 'PR' in row.Folder:\n",
    "            # If there's nothing with 'rec*' on the same level, then tell us        \n",
    "            if not glob.glob(row.Folder.replace('proj', 'rec')):\n",
    "                print(glob.glob(row.Folder.replace('proj', 'rec')))\n",
    "                print('- %s is missing matching reconstructions' % row.LogFile[len(Root)+1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get rid of all non-rec logfiles\n",
    "for c, row in Data.iterrows():\n",
    "    if 'rec' not in row.Folder:\n",
    "        Data.drop([c], inplace=True)\n",
    "    elif 'rectmp.log' in row.LogFile:\n",
    "        Data.drop([c], inplace=True)\n",
    "# Reset dataframe to something that we would get if we only would have loaded the 'rec' files\n",
    "Data = Data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate us some meaningful colums\n",
    "Data['Fish'] = [l[len(Root)+1:].split(os.sep)[0] for l in Data['LogFile']]\n",
    "Data['Scan'] = ['_'.join(l[len(Root)+1:].split(os.sep)[1:-1]) for l in Data['LogFile']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the file names of the reconstructions\n",
    "Data['Reconstructions'] = [sorted(glob.glob(os.path.join(f, '*rec0*.png'))) for f in Data['Folder']]\n",
    "Data['Number of reconstructions'] = [len(r) for r in Data.Reconstructions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop samples which have not been reconstructed yet\n",
    "# Based on https://stackoverflow.com/a/13851602\n",
    "for c,row in Data.iterrows():\n",
    "    if not row['Number of reconstructions']:\n",
    "        print('%s contains no PNG files, we might be currently reconstructing it' % row.Folder)\n",
    "Data = Data[Data['Number of reconstructions'] > 0]\n",
    "Data.reset_index(drop=True, inplace=True)\n",
    "print('We have %s folders with reconstructions' % (len(Data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get parameters to doublecheck from logfiles\n",
    "Data['Voxelsize'] = [get_pixelsize(log) for log in Data['LogFile']]\n",
    "Data['Filter'] = [get_filter(log) for log in Data['LogFile']]\n",
    "Data['Exposuretime'] = [get_exposuretime(log) for log in Data['LogFile']]\n",
    "Data['Scanner'] = [get_machine(log) for log in Data['LogFile']]\n",
    "Data['Averaging'] = [get_frameaveraging(log) for log in Data['LogFile']]\n",
    "Data['ProjectionSize'] = [get_projectionsize(log) for log in Data['LogFile']]\n",
    "Data['RotationStep'] = [get_rotationstep(log) for log in Data['LogFile']]\n",
    "Data['CameraWindow'] = [round((ps ** 0.5)/100)*100  for ps in Data['ProjectionSize']]\n",
    "Data['Grayvalue'] = [get_reconstruction_grayvalue(log) for log in Data['LogFile']]\n",
    "Data['RingartefactCorrection'] = [get_ringartefact(log) for log in Data['LogFile']]\n",
    "Data['BeamHardeningCorrection'] = [get_beamhardening(log) for log in Data['LogFile']]\n",
    "Data['Scan date'] = [get_scandate(log) for log in Data['LogFile']]\n",
    "Data['Scan time'] = [get_scantime(log) for log in Data['LogFile']]\n",
    "Data['Stacks'] = [get_stacks(log) for log in Data['LogFile']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data['Scan time total'] = [ st * stk  for st, stk in zip(Data['Scan time'], Data['Stacks'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a text file for each rec-folder, in which we can note what's going on with the fish\n",
    "for c,row in Data.iterrows():\n",
    "    outputfile = os.path.join(os.path.dirname(row.Folder),\n",
    "                              row.Fish + '.' + row.Scan + '.md')\n",
    "    # Only do this if the file does not already exist\n",
    "    if not os.path.exists(outputfile):\n",
    "        with open(outputfile, 'w', encoding='utf-8') as f:\n",
    "            f.write('# Fish %s, Scan %s\\n\\n' % (row.Fish, row.Scan))\n",
    "            f.write('This fish was scanned on %s on the %s, with a voxel size of %s μm.\\n\\n'\n",
    "                    % (row['Scan date'], row.Scanner, numpy.round(row.Voxelsize, 2)))\n",
    "            f.write('## Comments')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # https://www.geeksforgeeks.org/iterating-over-rows-and-columns-in-pandas-dataframe/\n",
    "# columns = list(Data)\n",
    "# columns.remove('Folder') \n",
    "# columns.remove('Fish')\n",
    "# columns.remove('LogFile')\n",
    "# columns.remove('Reconstructions')\n",
    "# columns.remove('Number of reconstructions')\n",
    "# columns.remove('Grayvalue')\n",
    "# columns.remove('Scan time')\n",
    "# columns.remove('Scan time total')\n",
    "# columns.remove('Scan date')\n",
    "# print(columns)\n",
    "# for col in columns:\n",
    "#     print(col)\n",
    "#     print(Data[col].unique())\n",
    "#     print(80*'-')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check voxel sizes (*rounded* to two after-comma values)\n",
    "# # If different, spit out which values\n",
    "# roundto = 2\n",
    "# if len(Data['Voxelsize'].round(roundto).unique()) > 1:\n",
    "#     print('We scanned all datasets with %s different voxel sizes' % len(Data['Voxelsize'].round(roundto).unique()))\n",
    "#     for vs in sorted(Data['Voxelsize'].round(roundto).unique()):\n",
    "#         print('-', vs, 'um for ', end='')\n",
    "#         for c, row in Data.iterrows():\n",
    "#             if float(vs) == round(row['Voxelsize'], roundto):\n",
    "#                 print(os.path.join(row['Fish'], row['Scan']), end=', ')\n",
    "#         print('')\n",
    "# else:\n",
    "#     print('We scanned all datasets with equal voxel size, namely %s um.' % float(Data['Voxelsize'].round(roundto).unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if len(Data['Grayvalue'].unique()) > 1:\n",
    "#     print('We reconstructed the datasets with different maximum gray values, namely')\n",
    "#     for gv in Data['Grayvalue'].unique():\n",
    "#         print(gv, 'for Samples ', end='')\n",
    "#         for c, row in Data.iterrows():\n",
    "#             if float(gv) == row['Grayvalue']:\n",
    "#                 print(os.path.join(row['Fish'], row['Scan']), end=', ')\n",
    "#         print('')\n",
    "# else:\n",
    "#     print('We reconstructed all datasets with equal maximum gray value, namely %s.' % Data['Grayvalue'].unique()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data[['Fish', 'Scan',\n",
    "#       'Voxelsize', 'Scanner',\n",
    "#       'Scan date', 'CameraWindow', 'RotationStep', 'Averaging',\n",
    "#       'Scan time', 'Stacks', 'Scan time total']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get an overview over the total scan time\n",
    "# Nice output based on https://stackoverflow.com/a/8907407/323100\n",
    "total_seconds = int(Data['Scan time total'].sum().total_seconds())\n",
    "hours, remainder = divmod(total_seconds,60*60)\n",
    "minutes, seconds = divmod(remainder,60)\n",
    "print('In total, we scanned for %s hours and %s minutes)' % (hours, minutes))\n",
    "for machine in Data['Scanner'].unique():\n",
    "    total_seconds = int(Data[Data['Scanner'] == machine]['Scan time total'].sum().total_seconds())\n",
    "    hours, remainder = divmod(total_seconds,60*60)\n",
    "    minutes, seconds = divmod(remainder,60)\n",
    "    print('\\t - Of these, we scanned %s hours and %s minutes on the %s,'\n",
    "          'for %s scans' % (hours,\n",
    "                            minutes,\n",
    "                            machine,\n",
    "                            len(Data[Data['Scanner'] == machine])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data[['Fish', 'Scan',\n",
    "      'Voxelsize', 'Scanner',\n",
    "      'Scan date', 'CameraWindow', 'RotationStep', 'Averaging', 'Scan time', 'Stacks' ]].to_excel('Details.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data[['Fish', 'Scan',\n",
    "      'Voxelsize', 'Scanner',\n",
    "      'Scan date', 'CameraWindow',\n",
    "      'RotationStep', 'Averaging', 'Scan time', 'Stacks' ]].to_excel(os.path.join(Root,'Details.xlsx'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Mikkis datafile\n",
    "MikkisFile = sorted(glob.glob(os.path.join(Root, 'X_ArchiveFiles', '*CTscanFishList.xlsx')))[0]\n",
    "# Read excel file and use the first column as index\n",
    "print('Reading in %s' % MikkisFile)\n",
    "DataMikki = pandas.read_excel(MikkisFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataMikki.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the fish we need to scan and display the jar\n",
    "fish = 'IG8'\n",
    "\n",
    "# Where is it?    \n",
    "for d, row in DataMikki.iterrows():\n",
    "    if (str(fish).lower() in str(row.Fishec).lower()) or \\\n",
    "    (str(fish).lower() in str(row.FieldID).lower()) or \\\n",
    "    (str(fish).lower() in str(row.OtherID).lower()) or \\\n",
    "    (str(fish).lower() in str(row.ReplacementID).lower()):\n",
    "        foundfishes = (row.Fishec, row.FieldID, row.OtherID, row.ReplacementID)\n",
    "        # remove nan from the list of hits\n",
    "        foundfishes = [str(x).lower() for x in foundfishes if pandas.isnull(x) == False]\n",
    "        print('An sample containing \"%s\" (fish ' % fish, end='')\n",
    "        \n",
    "        # Did we scan it already?\n",
    "        for found in foundfishes:\n",
    "            print(found.upper(), end='/')\n",
    "              \n",
    "              \n",
    "        print(') should be in jar \"length=%s cm\" (%s))' % (row['Length(cm)'],\n",
    "                                                         row['TemporaryJar']))\n",
    "  \n",
    "\n",
    "        try:\n",
    "            print('A sample \"%s\" was scanned on %s' % (found.upper(),\n",
    "                                                       Data[Data['Fish'].str.lower()==found]['Scan date'].values[0]))\n",
    "        except IndexError:\n",
    "            pass\n",
    "            #print('Fish %s was not scanned yet' % found)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data[Data['Fish']=='12849']['Scan date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find fishes to be rescanned, e.g. 'grep' through the comments files.\n",
    "for c,row in Data.iterrows():\n",
    "    commentsfile = os.path.join(os.path.dirname(row.Folder), row.Fish + '.' + os.path.basename(row.Folder) + '.md')\n",
    "    with open(commentsfile, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            if 'resc' in line and 'rec_rescan' not in line:\n",
    "                print('Fish', row.Fish, 'needs to be rescanned, because in its commentfile we write \"%s\"' % line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find fishes to be rescanned, e.g. 'grep' through the comments files.\n",
    "for c,row in Data.iterrows():\n",
    "    commentsfile = os.path.join(os.path.dirname(row.Folder), row.Fish + '.' + os.path.basename(row.Folder) + '.md')\n",
    "    with open(commentsfile, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            if 'recons' in line and 'rec_rerecon' not in line:\n",
    "                print('Fish', row.Fish, 'needs to be re-reconstructed, because in its commentfile we write \"%s\"' % line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find fishes which need to be discussed in detail\n",
    "for c,row in Data.iterrows():\n",
    "    commentsfile = os.path.join(os.path.dirname(row.Folder), row.Fish + '.' + os.path.basename(row.Folder) + '.md')\n",
    "    with open(commentsfile, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            if 'Mikki' in line or 'David' in line or 'discus' in line:\n",
    "                print('Fish', row.Fish, 'needs to be rescanned, because in its commentfile we write \"%s\"' % line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
